{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odHbCocfq5Ps"
   },
   "source": [
    "# Introduction to PyTorch with NLP\n",
    "\n",
    "PyTorch is a popular open-source deep learning framework developed by Facebook’s AI Research lab that provides flexibility and dynamic computational graphs, making it highly favored for research and production purposes. With its intuitive interface, PyTorch allows developers to build and train complex neural networks with ease.\n",
    "\n",
    "A computation graph is a network of nodes where each node represents an operation or variable, and the edges represent data dependencies between operations. Dynamic computation graphs are particularly useful in Natural Language Processing (NLP), where the input data, such as sentences or sequences, can vary in length and structure. PyTorch’s dynamic nature allows the graph to adapt to different input shapes and sizes, making it easier to handle complex tasks like sequence-to-sequence learning, attention mechanisms, and more.\n",
    "\n",
    "- A good tutorial on PyTorch: https://www.youtube.com/watch?v=OIenNRt2bjg\n",
    "- Detailed Documentation of PyTorch: https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzUk0ZRV4_ib"
   },
   "source": [
    "## 1. PyTorch Tensors and Basics\n",
    "\n",
    "Tensors are the core data structure in PyTorch, analogous to arrays and matrices in NumPy but with additional capabilities for autodifferentiation and easier device management. A tensor is a multi-dimensional array that serves as the basic building block of PyTorch.\n",
    "\n",
    "Tensor functions: https://pytorch.org/docs/stable/tensors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "id": "fm_ogLRCs0tD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hfd2h-QLrXIM",
    "outputId": "e2146016-492b-481d-d494-e73a5deed058"
   },
   "outputs": [],
   "source": [
    "a = torch.ones(3,3)\n",
    "print(a)\n",
    "print(type(a)) # checking the data type of a\n",
    "print(a.shape) # checking the shape of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor from a Python list\n",
    "data = [[1, 2], [3, 4]]\n",
    "tensor_from_list = torch.tensor(data)\n",
    "print(\"Tensor from list:\\n\", tensor_from_list)\n",
    "\n",
    "# Create a tensor filled with zeros\n",
    "zeros = torch.zeros((2, 3))\n",
    "print(\"\\nZeros tensor:\\n\", zeros)\n",
    "\n",
    "# Create a tensor filled with ones\n",
    "ones = torch.ones((2, 3))\n",
    "print(\"\\nOnes tensor:\\n\", ones)\n",
    "\n",
    "# Create a tensor with random values\n",
    "random_tensor = torch.rand((2, 3))\n",
    "print(\"\\nRandom tensor:\\n\", random_tensor)\n",
    "\n",
    "# Create a tensor with uninitialized values (values will be whatever is in memory)\n",
    "uninitialized = torch.empty((2, 3))\n",
    "print(\"\\nUninitialized tensor:\\n\", uninitialized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Indexing and Splicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a 3x3 tensor\n",
    "tensor = torch.arange(9).reshape(3, 3)\n",
    "print(\"Original tensor:\\n\", tensor)\n",
    "\n",
    "# Access a single element (row 2, column 3)\n",
    "element = tensor[1, 2]\n",
    "print(\"\\nElement at (1, 2):\", element)\n",
    "\n",
    "# Slice a sub-tensor (first two rows and columns)\n",
    "sub_tensor = tensor[:2, :2]\n",
    "print(\"\\nSliced sub-tensor:\\n\", sub_tensor)\n",
    "\n",
    "# Modify a specific element\n",
    "tensor[0, 0] = 100\n",
    "print(\"\\nModified tensor:\\n\", tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two tensors\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "# Addition\n",
    "add = a + b\n",
    "print(\"Addition:\", add)\n",
    "\n",
    "# Subtraction\n",
    "sub = a - b\n",
    "print(\"Subtraction:\", sub)\n",
    "\n",
    "# Multiplication\n",
    "mul = a * b\n",
    "print(\"Multiplication:\", mul)\n",
    "\n",
    "# Division\n",
    "div = b / a\n",
    "print(\"Division:\", div)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Matrix Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two 2x3 and 3x2 tensors\n",
    "A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "B = torch.tensor([[7, 8], [9, 10], [11, 12]])\n",
    "\n",
    "# Matrix multiplication\n",
    "C = torch.matmul(A, B)\n",
    "print(\"Matrix multiplication result:\\n\", C)\n",
    "\n",
    "# Transpose a tensor\n",
    "A_transposed = A.t()\n",
    "print(\"\\nTransposed A:\\n\", A_transposed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Errors with Matrix Multiplications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication requires that the number of columns in the first matrix matches the number of rows in the second matrix. If these dimensions don't align, PyTorch will raise a RuntimeError.\n",
    "\n",
    "Solution: Ensure that the tensors have compatible shapes. For example, if A has shape (2, 3), then B should have shape (3, N) where N is any integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create two tensors with incompatible shapes for matrix multiplication\n",
    "A = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Shape: (2, 3)\n",
    "B = torch.tensor([[7, 8], [9, 10]])       # Shape: (2, 2)\n",
    "\n",
    "# Attempt matrix multiplication (this will raise an error)\n",
    "try:\n",
    "    C = torch.matmul(A, B)\n",
    "except RuntimeError as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Reshaping Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a 4x4 tensor\n",
    "tensor = torch.arange(16).reshape(4, 4)\n",
    "print(\"Original tensor:\\n\", tensor)\n",
    "\n",
    "# Reshape to 2x8\n",
    "reshaped = tensor.view(2, 8)\n",
    "print(\"\\nReshaped to 2x8:\\n\", reshaped)\n",
    "\n",
    "# Flatten the tensor\n",
    "flattened = tensor.view(-1)\n",
    "print(\"\\nFlattened tensor:\", flattened)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Concatenating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two tensors\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "# Concatenate along rows (dim=0)\n",
    "concat_rows = torch.cat((a, b), dim=0)\n",
    "print(\"Concatenated along rows:\\n\", concat_rows)\n",
    "\n",
    "# Concatenate along columns (dim=1)\n",
    "concat_cols = torch.cat((a, b), dim=1)\n",
    "print(\"\\nConcatenated along columns:\\n\", concat_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Pitfall\n",
    "\n",
    "This is a common pitfall that in-compatible tensors are concatenated. In order to correctly concatenate the tensor, the dimensions other than the one it is concatenated on must be same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two tensors with incompatible shapes for concatenation\n",
    "a = torch.tensor([[1, 2], [3, 4]])  # Shape: (2, 2)\n",
    "b = torch.tensor([[5, 6, 7], [8, 9, 10]])  # Shape: (2, 3)\n",
    "\n",
    "# Attempt concatenation along rows (dim=0)\n",
    "try:\n",
    "    result = torch.cat((a, b), dim=0)\n",
    "except RuntimeError as e:\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "# Attempt concatenation along columns: why will it work?\n",
    "result = torch.cat((a, b), dim=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Transposing and Permuting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2x3 tensor\n",
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"Original tensor:\\n\", tensor)\n",
    "\n",
    "# Transpose (swap dimensions)\n",
    "transposed = tensor.t()\n",
    "print(\"\\nTransposed tensor:\\n\", transposed)\n",
    "\n",
    "# For tensors with more than 2 dimensions, use permute\n",
    "tensor_3d = torch.arange(24).reshape(2, 3, 4)\n",
    "print(\"\\nOriginal 3D tensor shape:\", tensor_3d.shape)\n",
    "\n",
    "# Permute dimensions to (3, 2, 4)\n",
    "permuted = tensor_3d.permute(1, 0, 2)\n",
    "print(\"Permuted tensor shape:\", permuted.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7. Reduction Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Sum all elements\n",
    "total_sum = torch.sum(x)\n",
    "print(\"Total sum:\", total_sum)\n",
    "\n",
    "# Sum along a specific dimension (rows)\n",
    "sum_dim0 = torch.sum(x, dim=0)\n",
    "print(\"\\nSum along dim=0 (columns):\", sum_dim0)\n",
    "\n",
    "# Mean along a specific dimension (columns)\n",
    "mean_dim1 = torch.mean(x.float(), dim=1)\n",
    "print(\"\\nMean along dim=1 (rows):\", mean_dim1)\n",
    "\n",
    "# Maximum value and its index\n",
    "max_val, max_idx = torch.max(x, dim=1)\n",
    "print(\"\\nMaximum values:\", max_val)\n",
    "print(\"Indices of maximum values:\", max_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common pitfall in Reduction Operations\n",
    "\n",
    "The dimension specified should be valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2x3 tensor\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(x.shape)\n",
    "print('Valid dimensions -', 0, 'to', len(x.shape)-1)\n",
    "# Attempt to sum along an invalid dimension\n",
    "try:\n",
    "    result = torch.sum(x, dim=2)\n",
    "except IndexError as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8. Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor with gradient tracking\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Perform operations\n",
    "y = x * 2\n",
    "print(y)\n",
    "z = y.mean()\n",
    "\n",
    "# Compute gradients\n",
    "z.backward()\n",
    "\n",
    "print(\"Gradient of z w.r. to x:\", x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ca9Ei4ML3-2D"
   },
   "source": [
    "### 1.9. Device of tensors\n",
    "\n",
    "In PyTorch, the location of tensors—whether on a CPU or GPU—directly impacts the performance and functionality of your code. Tensors can be stored on different devices, and operations on tensors must be performed on the same device to avoid errors and inefficiencies.\n",
    "\n",
    "Importance -\n",
    "\n",
    "1. Performance Optimization: Moving tensors to the GPU can significantly accelerate computations, as GPUs are designed to handle matrix operations much more efficiently than CPUs. This is especially important for training deep learning models where large amounts of data and complex operations are involved.\n",
    "2. Error Prevention: PyTorch requires all tensors involved in a computation to be on the same device. If operations are attempted between tensors on different devices (e.g., adding a CPU tensor to a GPU tensor), it will result in a runtime error like RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!. Properly managing tensor locations ensures compatibility and prevents such errors.\n",
    "3. Memory Management: GPUs have limited memory compared to CPUs. Properly managing tensor locations helps efficiently utilize GPU memory and avoid issues like out-of-memory errors. Moving tensors to the appropriate device only when needed can help balance memory usage across devices.\n",
    "\n",
    "\n",
    "Below is a code snippet explaining critical aspects of tensor location management in PyTorch, highlighting performance benefits and common pitfalls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlEpgNO3uTzg",
    "outputId": "18d7b120-2244-43b1-c4f4-f0dcf6bb948f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else \"cpu\"))\n",
    "\n",
    "# Create a tensor on the CPU\n",
    "x_cpu = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Create a tensor on the GPU (if available)\n",
    "x_gpu = torch.tensor([4.0, 5.0, 6.0], device=device)\n",
    "\n",
    "# Demonstrating Performance Optimization by moving CPU tensor to GPU\n",
    "# Moving x_cpu to the GPU can accelerate computations\n",
    "x_cpu = x_cpu.to(device)\n",
    "\n",
    "# Performing operations on the same device (GPU in this case)\n",
    "result = x_cpu + x_gpu\n",
    "print(\"Result on GPU:\", result)\n",
    "print('\\n')\n",
    "# Demonstrating Error Prevention\n",
    "# This will cause an error if x_cpu is not moved to the GPU\n",
    "try:\n",
    "    # Attempting an operation between tensors on different devices\n",
    "    x_cpu = torch.tensor([1.0, 2.0, 3.0])  # Back on CPU\n",
    "    error_result = x_cpu + x_gpu\n",
    "except RuntimeError as e:\n",
    "    print(\"Error:\", e)\n",
    "    \n",
    "print('\\n')\n",
    "# Demonstrating Memory Management\n",
    "# Moving the result back to the CPU to free up GPU memory\n",
    "result = result.to(\"cpu\")\n",
    "print(\"Result moved back to CPU:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwIHhppJXXS3"
   },
   "source": [
    "### 1.10. Broadcasting\n",
    "\n",
    "Broadcasting is a method used in PyTorch (and other libraries like NumPy) that allows arithmetic operations on tensors of different shapes by automatically expanding the smaller tensor(s) to match the shape of the larger one. Broadcasting follows a set of rules to make the tensors compatible without explicitly copying data, allowing efficient computation even when the shapes initially do not match.\n",
    "\n",
    "#### How is Broadcasting Done?\n",
    "\n",
    "Broadcasting follows a set of rules to determine how to expand the smaller tensor to match the shape of the larger one:\n",
    "\n",
    "1. Alignment from the Right: Tensors are aligned from the right when comparing their shapes. For instance, to broadcast (4, 3, 2) with (3, 2), align as follows:\n",
    "\n",
    "$$\n",
    "(4, 3, 2) \\\\\n",
    "( , 3, 2)\n",
    "$$\n",
    "\n",
    "2. Dimension Compatibility: Two dimensions are compatible if:\n",
    "- They are equal.\n",
    "- One of them is 1. In this case, the dimension with 1 is stretched to match the other dimension.\n",
    "3. Expansion: Dimensions with 1 are virtually expanded to match the corresponding dimension of the other tensor without physically copying data.\n",
    "\n",
    "Consider two tensors A and B:\n",
    "\n",
    "- Let A be of shape (3, 1):\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Let B be of shape (1, 4):\n",
    "\n",
    "$$\n",
    "B =\n",
    "\\begin{bmatrix}\n",
    "10 & 20 & 30 & 40\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "When performing the operation A + B, broadcasting expands A and B to a compatible shape (3, 4):\n",
    "\n",
    "1. Expand A along columns:\n",
    "\n",
    "$$\n",
    "A_{\\text{expanded}} =\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1 \\\\\n",
    "2 & 2 & 2 & 2 \\\\\n",
    "3 & 3 & 3 & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "2. Expand B along rows:\n",
    "\n",
    "$$\n",
    "B_{\\text{expanded}} =\n",
    "\\begin{bmatrix}\n",
    "10 & 20 & 30 & 40 \\\\\n",
    "10 & 20 & 30 & 40 \\\\\n",
    "10 & 20 & 30 & 40\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, you can perform element-wise addition:\n",
    "\n",
    "$$\n",
    "A + B =\n",
    "\\begin{bmatrix}\n",
    "1+10 & 1+20 & 1+30 & 1+40 \\\\\n",
    "2+10 & 2+20 & 2+30 & 2+40 \\\\\n",
    "3+10 & 3+20 & 3+30 & 3+40\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "11 & 21 & 31 & 41 \\\\\n",
    "12 & 22 & 32 & 42 \\\\\n",
    "13 & 23 & 33 & 43\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0oat928XaSr",
    "outputId": "932ceb56-1576-4132-e4a2-f64f39ac8ed9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 2D tensor (batch of vectors)\n",
    "tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"2D Tensor:\")\n",
    "print(tensor_2d)\n",
    "\n",
    "# 1D tensor (vector)\n",
    "tensor_1d = torch.tensor([10, 20, 30])\n",
    "print(\"1D Tensor:\")\n",
    "print(tensor_1d)\n",
    "\n",
    "# Broadcasting\n",
    "result = tensor_2d + tensor_1d\n",
    "print(\"Result after broadcasting:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_f-cK9DXn0t",
    "outputId": "0647421d-8c1f-4c81-f874-3a555ee9c3df"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Tensor with shape (2, 3)\n",
    "tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"2D Tensor:\")\n",
    "print(tensor_2d)\n",
    "\n",
    "# Tensor with shape (2, 2) - This is incompatible for broadcasting\n",
    "tensor_incompatible = torch.tensor([[10, 20], [30, 40]])\n",
    "print(\"Incompatible Tensor:\")\n",
    "print(tensor_incompatible)\n",
    "\n",
    "try:\n",
    "    result = tensor_2d + tensor_incompatible\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NBTUwy5yBVL"
   },
   "source": [
    "## 2. Multiclass Text Classification\n",
    "\n",
    "We will explore multi-class classification using a real-world dataset of tweets annotated with various emotions. The goal is to build a model that can accurately classify the emotion expressed in a given tweet into one of several categories, such as sadness, enthusiasm, anger, joy, and others.\n",
    "\n",
    "The dataset tweet_emotions.csv contains three columns:\n",
    "\n",
    "1. tweet_id: A unique identifier for each tweet.\n",
    "2. sentiment: The emotion or sentiment expressed in the tweet. This is the target variable with multiple classes, such as empty, sadness, enthusiasm, neutral, and others.\n",
    "3. content: The text of the tweet, representing the input features used for classification.\n",
    "\n",
    "![Stages](stages.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5BddCDiyU9A"
   },
   "source": [
    "### 2.1. Loading the Dataset\n",
    "\n",
    "A DataFrame is a two-dimensional, tabular data structure commonly used in data manipulation and analysis. In Python, the most popular implementation of DataFrames is provided by the Pandas library.\n",
    "\n",
    "In this step, we load the dataset containing tweets and their associated emotions using the pandas library. The dataset is stored in a CSV file, which we read into a DataFrame for easier manipulation and analysis.\n",
    "\n",
    "Download the dataset from this link - https://www.kaggle.com/datasets/pashupatigupta/emotion-detection-from-text/data\n",
    "\n",
    "Store it in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "id": "aR0-2KdJyfQE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'tweet_emotions.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qomchg8uysPY"
   },
   "source": [
    "### 2.2. Understanding the Data\n",
    "\n",
    "In this step, we examine the basic characteristics of the dataset. We'll check the structure of the DataFrame, including the column names, data types, and summary statistics. This helps us understand the kind of data we are working with and identify any preprocessing steps that might be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oxCSiTghys7D",
    "outputId": "38171349-49ad-446f-ed39-4da32a45d794"
   },
   "outputs": [],
   "source": [
    "# Display basic information about the DataFrame\n",
    "print(\"DataFrame Info:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "cb86vyJayvDt",
    "outputId": "e0b41a5e-6c7c-4bf8-a988-7ee9d5d1fa20"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the number of tweets for each sentiment\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sentiment_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of Sentiments in Tweets')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_keep = ['neutral', 'worry', 'happiness']\n",
    "\n",
    "# Filter the DataFrame to keep only the specified labels\n",
    "df = df[df['sentiment'].isin(labels_to_keep)]\n",
    "\n",
    "# Shuffle the DataFrame randomly\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Now df_balanced contains the balanced and shuffled DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of now, the input and output are texts. But PyTorch (and Machine Learning) only understands numbers. So, we need to convert these to numbers.\n",
    "\n",
    "Here, the input is Natural Language and output is categorical labels. So, we will preprocess them differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OI93fMCGzaI3"
   },
   "source": [
    "### 2.3. One-Hot Encoding: processing the categorical labels\n",
    "\n",
    "One-hot encoding is a common technique used to convert categorical data into a numerical format for machine learning algorithms. Unlike label encoding, which assigns an integer to each category, one-hot encoding converts categorical variables into a binary matrix representation.\n",
    "\n",
    "#### What is One-Hot Encoding?\n",
    "\n",
    "In one-hot encoding, each category is represented by a binary vector. The vector has the same length as the number of unique categories, with a `1` in the position corresponding to the category and `0` in all other positions.\n",
    "\n",
    "Given a set of categories:\n",
    "\n",
    "One-hot encoding represents these sentiments as follows (for example):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{{\"empty\"}} &\\rightarrow [1, 0, 0, 0] \\\\\n",
    "\\text{{\"sadness\"}} &\\rightarrow [0, 1, 0, 0] \\\\\n",
    "\\text{{\"enthusiasm\"}} &\\rightarrow [0, 0, 1, 0] \\\\\n",
    "\\text{{\"neutral\"}} &\\rightarrow [0, 0, 0, 1] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### Mathematical Representation\n",
    "\n",
    "If \\( x \\) is a categorical feature with \\( n \\) unique values, one-hot encoding can be defined as:\n",
    "\n",
    "$$\n",
    "\\text{{OHE}}(x_i) = [b_1, b_2, \\ldots, b_n]\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "b_j =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } x_i \\text{ belongs to category } j \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Removes any assumption of ordinal relationships between categories, unlike label encoding.\n",
    "- Works well with algorithms that require input to be in a binary format (e.g., neural networks).\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Increases the dimensionality of the dataset, which can lead to a large number of features if there are many unique categories.\n",
    "- Sparse representation can increase memory usage and computational complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlRykVOOzjcY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_sentiments = one_hot_encoder.fit_transform(df[['sentiment']])\n",
    "encoded_df = pd.DataFrame(encoded_sentiments, columns=one_hot_encoder.categories_[0])\n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnwwV6MHzBNJ"
   },
   "source": [
    "### 2.4. Tokenization\n",
    "\n",
    "Tokenization is a crucial step in Natural Language Processing (NLP) that involves breaking down text into smaller units called tokens. Tokens can be words, characters, or subwords, depending on the type of tokenization applied. This process is essential for converting raw text into a format that machine learning models can process.\n",
    "\n",
    "We will study advanced tokenization techniques in the course, but for simplicity, we will stick to basic word-based tokenization.\n",
    "\n",
    "At its core, tokenization is the process of splitting a string of text into meaningful pieces, such as words or subwords. Each of these pieces is called a **token**. For instance, given the sentence:\n",
    "\n",
    "$$\n",
    "\\text{{\"I love deep learning!\"}}\n",
    "$$\n",
    "\n",
    "Tokenization might break this sentence into the following tokens:\n",
    "\n",
    "$$\n",
    "\\text{{Tokens}} = \\{ \\text{{\"I\"}}, \\text{{\"love\"}}, \\text{{\"deep\"}}, \\text{{\"learning\"}}, \\text{{\"!\"}} \\}\n",
    "$$\n",
    "\n",
    "Consider the following tweet:\n",
    "\n",
    "\"I love deep learning and NLP!\"\n",
    "\n",
    "Word tokenization would break this content into the following tokens:\n",
    "\n",
    "$$\n",
    "\\text{{Tokens}} = \\{ \\text{{\"I\"}}, \\text{{\"love\"}}, \\text{{\"deep\"}}, \\text{{\"learning\"}}, \\text{{\"and\"}}, \\text{{\"NLP\"}}, \\text{{\"!\"}} \\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "id": "Q8ac95eSzCAq"
   },
   "outputs": [],
   "source": [
    "def tokenize_df(df):\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        arr = [word for word in text.split()]\n",
    "        return arr\n",
    "\n",
    "    df['tokenized'] = df['content'].apply(tokenize_text)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tokenize_df(df)\n",
    "\n",
    "print('input -',df.iloc[0]['content'])\n",
    "print('Tokens -',df.iloc[0]['tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head() #check tokenized column on extreme right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have basic pre-processing completed. You can also try more techniques like stemming, lemmatization, stop words removal, etc., for better performance, but we will skip them for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Data-splitting\n",
    "\n",
    "In a typical machine learning workflow, the dataset \\( D \\) is split into three parts:\n",
    "\n",
    "$$\n",
    "D = D_{\\text{train}} \\cup D_{\\text{val}} \\cup D_{\\text{test}}\n",
    "$$\n",
    "\n",
    "- **Training Set (\\( D_{\\text{train}} \\))**: This subset is used to train the model, allowing it to learn the patterns and relationships within the data.\n",
    "- **Test Set (\\( D_{\\text{test}} \\))**: This subset is used to evaluate the model’s performance on unseen data, providing an estimate of how well the model will generalize to new, unseen data.\n",
    "\n",
    "The train-test split is crucial for several reasons:\n",
    "\n",
    "1. **Preventing Overfitting**: By keeping the test set separate from the training data, we can detect if the model is overfitting—i.e., performing well on the training data but poorly on unseen data.\n",
    "2. **Model Evaluation**: The test set serves as a proxy for new data, allowing us to evaluate the model's performance and estimate its accuracy, precision, recall, and other metrics.\n",
    "3. **Generalization**: A proper train-test split ensures that the model generalizes well to new data, rather than just memorizing the training data.\n",
    "\n",
    "Dos:\n",
    "\n",
    "- Ensure Randomness: Always shuffle your data before splitting to avoid any ordering bias. The random_state parameter in train_test_split ensures reproducibility.\n",
    "- Maintain Class Distribution: If your dataset is imbalanced (e.g., more instances of one class than another), use stratified splitting to maintain the class distribution across the train and test sets.\n",
    "\n",
    "Don’ts:\n",
    "\n",
    "- Don’t Peek at Test Data: Never train or tune your model on the test set. This can lead to overfitting and unrealistic performance estimates.\n",
    "- Avoid Multiple Tests on the Test Set: Repeatedly testing the model on the same test set can lead to overfitting to the test data. Use cross-validation for model tuning.\n",
    "- Don’t Ignore Data Leakage: Ensure that no information from the test set leaks into the training process. Data leakage can lead to overly optimistic performance metrics.\n",
    "\n",
    "Docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataframe(df, train_ratio=0.7, test_ratio=0.15, val_ratio=0.15, random_state=None):\n",
    "    assert train_ratio + test_ratio + val_ratio == 1.0, \"Ratios must sum to 1\"\n",
    "\n",
    "    # Split the DataFrame into train and remaining (test + validation)\n",
    "    train_df, remaining_df = train_test_split(df, test_size=(1 - train_ratio), random_state=random_state)\n",
    "\n",
    "    # Calculate the ratio of test to remaining (test + validation)\n",
    "    test_size_ratio = test_ratio / (test_ratio + val_ratio)\n",
    "\n",
    "    # Split the remaining into test and validation sets\n",
    "    test_df, val_df = train_test_split(remaining_df, test_size=test_size_ratio, random_state=random_state)\n",
    "\n",
    "    return train_df, test_df, val_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, val_df = split_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyLVsox38mzD"
   },
   "source": [
    "### 2.6. Vocabulary Map\n",
    "\n",
    "A **Vocabulary Class** is a fundamental component in Natural Language Processing (NLP) that is responsible for mapping words or tokens in a dataset to unique numerical identifiers (indices). This mapping allows the conversion of text data into numerical format, which is essential for feeding it into machine learning models.\n",
    "\n",
    "#### What is a Vocabulary Class?\n",
    "\n",
    "A Vocabulary Class manages the collection of unique tokens (words, subwords, or characters) in a dataset and assigns each token a unique index. The vocabulary typically includes special tokens that serve specific purposes, such as indicating the start or end of a sequence.\n",
    "\n",
    "**Basic functionalities of a Vocabulary Class include:**\n",
    "\n",
    "- **Token-to-Index Mapping**: Converting each token to a unique index.\n",
    "- **Index-to-Token Mapping**: Converting indices back to tokens (useful for decoding model outputs).\n",
    "- **Managing Special Tokens**: Handling tokens like padding, unknown words, start-of-sequence, and end-of-sequence.\n",
    "\n",
    "#### Key Components of a Vocabulary Class\n",
    "\n",
    "1. **Token-to-Index Mapping**:\n",
    "   - The main functionality of the vocabulary is to map each unique token in the dataset to a unique integer index. For instance, if your dataset contains the tokens `[\"I\", \"love\", \"NLP\"]`, the vocabulary might map them as follows:\n",
    "\n",
    "     | Token | Index |\n",
    "     |-------|-------|\n",
    "     | I     | 1     |\n",
    "     | love  | 2     |\n",
    "     | NLP   | 3     |\n",
    "\n",
    "2. **Index-to-Token Mapping**:\n",
    "   - This is the inverse operation, which allows converting indices back to their respective tokens. This is useful during the model inference phase to convert model predictions back into human-readable text.\n",
    "\n",
    "3. **Special Tokens**:\n",
    "   - **\\<PAD\\>**: Used for padding sequences to the same length within a batch. This ensures that all sequences in a batch are of uniform length, making them suitable for batch processing.\n",
    "   - **\\<UNK\\>**: Represents unknown tokens, i.e., tokens that were not in the training vocabulary. This is essential for handling out-of-vocabulary words during inference.\n",
    "   - **<SOS\\>**: Indicates the start of a sequence. This is commonly used in sequence generation tasks like machine translation or text generation.\n",
    "   - **\\<EOS\\>**: Marks the end of a sequence. It helps models know when to stop generating further tokens.\n",
    " \n",
    "\n",
    "#### Why Are Special Tokens Important?\n",
    "\n",
    "Special tokens play a critical role in text preprocessing and model training:\n",
    "\n",
    "- **Padding (<PAD>)**: When batching sequences of varying lengths, shorter sequences are padded with `<PAD>` tokens to ensure all sequences in a batch have the same length. This padding is essential for efficient batch processing in deep learning frameworks like PyTorch or TensorFlow.\n",
    "\n",
    "- **Unknown Token (<UNK>)**: During inference, the model may encounter words that were not seen during training. The `<UNK>` token helps the model handle these situations gracefully, rather than crashing or producing undefined behavior.\n",
    "\n",
    "- **Start-of-Sequence (<SOS>) and End-of-Sequence (<EOS>)**: In tasks like sequence-to-sequence modeling, it’s important for the model to know when a sequence starts and ends. `<SOS>` and `<EOS>` tokens guide the model during generation and ensure coherent outputs.\n",
    "    \n",
    "Dos:\n",
    "\n",
    "- Include Special Tokens: Always include special tokens in your vocabulary, especially if your task involves sequence generation or varying sequence lengths.\n",
    "- Handle Out-of-Vocabulary Words: Ensure your vocabulary can handle out-of-vocabulary words by including an <UNK> token, which prevents the model from encountering undefined behavior during inference.\n",
    "- Only make the vocabulary using the training set: In real-world, you don't have test data. So, to adequately represent that, construct your vocabulary using only training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "id": "rtTo_kS-5Rvz"
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, add_special_tokens=True):\n",
    "        self.token_to_index = {}\n",
    "        self.index_to_token = {}\n",
    "        self.add_special_tokens = add_special_tokens\n",
    "        self.special_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"]\n",
    "        self.build_special_tokens()\n",
    "\n",
    "    def build_special_tokens(self):\n",
    "        if self.add_special_tokens: # add special tokens to vocabulary\n",
    "            for token in self.special_tokens:\n",
    "                self.add_token(token)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.token_to_index: # if token is not present\n",
    "            index = len(self.token_to_index) # get as new index\n",
    "            self.token_to_index[token] = index # assign that index to the token\n",
    "            self.index_to_token[index] = token # perform the inverse operation too\n",
    "\n",
    "    def get_index(self, token):\n",
    "        return self.token_to_index.get(token, self.token_to_index[\"<UNK>\"]) # \n",
    "        # if token is present in vocab, return its index, if not present, return the index of unk token\n",
    "\n",
    "    def get_token(self, index):\n",
    "        return self.index_to_token.get(index, \"<UNK>\")\n",
    "        # if index is present in token, return token, else return UNK token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_index)\n",
    "\n",
    "    def build_vocab(self, token_list):\n",
    "        for token in token_list:\n",
    "            self.add_token(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeyJTde-9GN_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the Vocabulary\n",
    "vocab = Vocabulary(add_special_tokens=True)\n",
    "\n",
    "# Add tokens from each tweet to the vocabulary\n",
    "for tokens in train_df['tokenized']: # use only the training set\n",
    "    vocab.build_vocab(tokens)\n",
    "\n",
    "# Check the vocabulary size\n",
    "print(\"Vocabulary Size:\", len(vocab))\n",
    "\n",
    "# Example: Check the index of a word in the vocabulary\n",
    "print(\"Index of 'headache':\", vocab.get_index('headache'))\n",
    "\n",
    "# Example: Check the token for a specific index\n",
    "print(\"Token for index 5:\", vocab.get_token(5))\n",
    "\n",
    "print(\"Token for index 0:\", vocab.get_token(0))\n",
    "\n",
    "# Example: Handling unknown tokens\n",
    "print(\"Index of 'Verstappen':\", vocab.get_index('Verstappen'))\n",
    "print(\"Token for index 355739485:\", vocab.get_token(355739485))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Understanding `max_len`, Padding, and Truncation in NLP\n",
    "\n",
    "When working with sequential data, such as text, it's common to encounter sequences of varying lengths. In NLP, these sequences might represent sentences, paragraphs, or entire documents, and their lengths can vary significantly. However, most machine learning models, especially those based on deep learning, require input sequences to be of uniform length. This is where concepts like `max_len`, padding, and truncation become essential.\n",
    "\n",
    "#### 1. What is `max_len`?\n",
    "\n",
    "`max_len` is a parameter that defines the maximum length of the sequences that will be fed into the model. Any sequence longer than `max_len` will be truncated, and any sequence shorter than `max_len` will be padded.\n",
    "\n",
    "#### 2. Why is `max_len` Important?\n",
    "\n",
    "- **Uniform Input Size**: Deep learning models networks require input sequences to be of the same length. `max_len` ensures that all sequences in a batch have the same length, allowing for efficient batch processing.\n",
    "\n",
    "- **Memory Management**: Setting an appropriate `max_len` helps in managing memory usage. Sequences that are too long might lead to excessive memory consumption, especially in models like Transformers, where memory usage scales with the square of the sequence length.\n",
    "\n",
    "- **Model Efficiency**: By limiting the sequence length with `max_len`, you ensure that your model processes only the most relevant parts of the sequence, improving training efficiency and potentially leading to faster convergence.\n",
    "\n",
    "#### 3. Padding\n",
    "\n",
    "**Padding** is the process of adding a special token (usually `<PAD>`) to sequences to ensure they reach the specified `max_len`. Padding is typically added to the end of the sequence, but it can also be added at the beginning in some cases.\n",
    "\n",
    "#### Importance of Padding:\n",
    "\n",
    "- **Batch Processing**: In machine learning, it’s common to process data in batches for efficiency. However, when dealing with sequences of varying lengths, directly batching them is not possible. Padding allows all sequences to be of the same length, enabling efficient batch processing.\n",
    "\n",
    "- **No Information Loss**: Padding ensures that no information from shorter sequences is lost, as it only adds neutral tokens that do not contribute to the model’s learning process.\n",
    "\n",
    "**Example:**\n",
    "```text\n",
    "Original sequences:\n",
    "[\"I love NLP\", \"Deep learning is fun\", \"AI is the future\"]\n",
    "\n",
    "After padding (assuming max_len=5):\n",
    "[\"I love NLP <PAD> <PAD>\", \"Deep learning is fun <PAD>\", \"AI is the future <PAD> <PAD>\"]\n",
    "```\n",
    "\n",
    "#### 4. Truncation\n",
    "Truncation is the process of cutting off sequences that are longer than the specified max_len. This is done to ensure that no sequence exceeds the maximum length, which is crucial for maintaining uniformity in input size.\n",
    "\n",
    "Importance of Truncation:\n",
    "\n",
    "Handling Long Sequences: In many NLP tasks, some sequences can be much longer than others. Truncation ensures that these long sequences do not dominate the model’s memory usage or processing time.\n",
    "Focus on Relevant Information: By truncating sequences, you can focus the model’s attention on the most relevant or important parts of the sequence. For instance, in sentiment analysis, the beginning of a review might be more indicative of sentiment than its middle or end.\n",
    "\n",
    "**Example:**\n",
    "```text\n",
    "Original sequence:\n",
    "\"Natural Language Processing is a fascinating field with many applications in modern technology, ranging from chatbots to machine translation.\"\n",
    "\n",
    "After truncation (assuming max_len=10):\n",
    "\"Natural Language Processing is a fascinating field with many\"\n",
    "```\n",
    "\n",
    "#### 5. Balancing max_len, Padding, and Truncation\n",
    "Choosing the right max_len involves balancing the trade-offs between truncation and padding:\n",
    "\n",
    "Too Short max_len: If max_len is too short, many sequences will be truncated, leading to potential loss of important information. This can negatively impact model performance, especially if crucial context is lost.\n",
    "Too Long max_len: If max_len is too long, most sequences will require a lot of padding, leading to inefficiencies in model training. The model might waste resources processing padding tokens, which do not carry any meaningful information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us consider we have the following sequence in a batch -\n",
    "token_lists = [\n",
    "    [\"I\", \"solemnly\", \"swear\", \"that\", \"I\", \"am\", \"up\", \"to\", \"no\", \"good\"],\n",
    "    [\"It\", \"does\", \"not\", \"do\", \"to\", \"dwell\", \"on\", \"dreams\", \"and\", \"forget\", \"to\", \"live\"],\n",
    "    [\"Yer\", \"a\", \"wizard\", \"Harry\"],\n",
    "    [\"After\", \"all\", \"this\", \"time?\", \"Always\"]\n",
    "]\n",
    "\n",
    "# Converting them into token IDs -\n",
    "\n",
    "token_ids_list = [[vocab.get_index(token) for token in tokens] for tokens in token_lists]\n",
    "\n",
    "for i in token_ids_list:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the problem here? \n",
    "\n",
    "Linear algebra works on fixed-size matrices. If we batch it, then size size of matrix will be (4, ?).\n",
    "\n",
    "So, truncate and pad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 8  # Set the maximum length\n",
    "\n",
    "# Truncate and pad the sequences\n",
    "padded_token_ids_list = []\n",
    "\n",
    "for token_ids in token_ids_list:\n",
    "    # Truncate the sequence to max_len if size > max_len\n",
    "    if len(token_ids) > max_len:\n",
    "        token_ids = token_ids[:max_len]\n",
    "    \n",
    "    # Pad the sequence if it's shorter than max_len\n",
    "    if len(token_ids) < max_len:\n",
    "        token_ids += [vocab.get_index(\"<PAD>\")] * (max_len - len(token_ids))\n",
    "    \n",
    "    padded_token_ids_list.append(token_ids)\n",
    "\n",
    "for i in padded_token_ids_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's assume that you are considering that the average index of a document is a feature \n",
    "# of that document on which the output depends.\n",
    "import torch\n",
    "\n",
    "padded_tokens = torch.tensor(padded_token_ids_list, dtype=torch.float)\n",
    "\n",
    "features = torch.mean(padded_tokens, axis=1)\n",
    "print(features)\n",
    "\n",
    "# Is this correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Masking (Create a mask that identifies padded and non-padded tokens, and use it everywhere)\n",
    "mask = padded_tokens!=0\n",
    "\n",
    "print('Mask', mask)\n",
    "mean_ignoring_padding = torch.sum(padded_tokens.float() * mask, axis=1) / torch.sum(mask, axis=1)\n",
    "print(mean_ignoring_padding)\n",
    "\n",
    "# Solution 2: Directly dividing by non-zero elements\n",
    "count_non_zero_elements = torch.count_nonzero(padded_tokens, axis=1)\n",
    "mean_ignoring_padding = torch.sum(padded_tokens.float(), axis=1) / count_non_zero_elements\n",
    "print(mean_ignoring_padding)\n",
    "\n",
    "\n",
    "# Note, you will never get an error if you do this, but your model will suffer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't generally apply mask on the token IDs directly (because it seems redundant. But in a network, we apply mask on the output of layers to nullify any contribution from PAD tokens. We shall see this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should always keep a mask handy from the token_ids itself, \n",
    "# as that is the only place where it's definitely known\n",
    "\n",
    "# A more robust way to extract mask\n",
    "pad_token_id = vocab.get_index('<PAD>')\n",
    "mask = padded_tokens!=pad_token_id\n",
    "\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JG-13WN9lu9"
   },
   "source": [
    "### 2.8. Creating a PyTorch Dataset\n",
    "\n",
    "Implementing a custom dataset class in PyTorch is a crucial step for preparing your data for training machine learning models. The dataset class handles data loading, preprocessing, and batching, ensuring that your data is efficiently fed into the model during training. Below is a detailed guide on how to implement a dataset class, along with explanations on how to handle batching and other important aspects.\n",
    "\n",
    "In PyTorch, the Dataset class is an abstract class that you can inherit from to create your custom dataset. The two primary methods you need to implement are:\n",
    "\n",
    "__len__(self): Returns the total number of samples in the dataset.\n",
    "\n",
    "__getitem__(self, idx): Retrieves a single sample from the dataset at the given index idx.\n",
    "\n",
    "We will create a dataset with padding and truncation like above.\n",
    "\n",
    "Docs: https://pytorch.org/docs/stable/data.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "id": "oHUOyvwv9xMQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, vocab, max_len=20):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): The DataFrame containing tokenized texts and labels.\n",
    "            vocab (Vocabulary): Vocabulary object for token-to-index conversion.\n",
    "            max_len (int): Maximum length of the sequences (for padding).\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the tokenized text and labels from the DataFrame\n",
    "        tokenized_text = self.dataframe.iloc[idx]['tokenized']\n",
    "        \n",
    "        # Get one-hot encoded labels and convert to class index\n",
    "        # CrossEntropyLoss expects class indices (0, 1, 2, ...), not one-hot vectors\n",
    "        one_hot_labels = self.dataframe.iloc[idx][['neutral', 'worry', 'happiness']].values.astype(float)\n",
    "        label = torch.argmax(torch.tensor(one_hot_labels)).item()  # Convert one-hot to class index\n",
    "\n",
    "        # Convert tokenized text to indices\n",
    "        token_indices = [self.vocab.get_index(token) for token in tokenized_text]\n",
    "\n",
    "        # Pad or truncate the sequence to the maximum length\n",
    "        if len(token_indices) < self.max_len:\n",
    "            token_indices += [self.vocab.get_index(\"<PAD>\")] * (self.max_len - len(token_indices))\n",
    "        else:\n",
    "            token_indices = token_indices[:self.max_len]\n",
    "\n",
    "        # Convert to tensors\n",
    "        token_indices = torch.tensor(token_indices)\n",
    "        return token_indices, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "id": "GbXL42nVBpu-"
   },
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_df, vocab, max_len=5)\n",
    "val_dataset = TextDataset(val_df, vocab, max_len=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to create batches now? With the dataset class, this becomes super simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_CND6EDLC8v"
   },
   "source": [
    "### 2.9. Dataloader\n",
    "\n",
    "DataLoader is a class in PyTorch’s torch.utils.data module that helps to load data from a dataset in an efficient manner. It manages batching, shuffling, parallel data loading, and more, which are essential for training deep learning models.\n",
    "\n",
    "- Batching: Splits the dataset into small batches of data, making it easier for the model to process and update weights iteratively.\n",
    "- Shuffling: Allows shuffling of the data to prevent models from learning the order of samples, enhancing model generalization.\n",
    "- Parallel Data Loading: Supports loading data in parallel using multiple CPU workers, which speeds up the data loading process.\n",
    "- Custom Sampling: Works seamlessly with custom samplers like RandomSampler to control how data points are selected.\n",
    "\n",
    "Docs: https://pytorch.org/docs/stable/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nrkOe7BgLvIc",
    "outputId": "fe55dfb5-b8a9-43f2-fc83-ae5ba21d7e67",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch_idx, (data, target) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx+1}:\")\n",
    "    print(\"Data (token indices):\", data)\n",
    "    print(\"Target (labels):\", target)\n",
    "    if batch_idx > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10. Model Architecture\n",
    "\n",
    "The model consists of the following layers:\n",
    "\n",
    "1. **Embedding Layer**: Converts tokens into dense vector representations. (Docs: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
    "3. **Masking and Summing**: Masks padding tokens and sums the embeddings across the sequence.\n",
    "4. **Linear Layer**: Applies another linear transformation to the modified embeddings. (Docs: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "5. **Classification Layer**: Outputs the class scores for each input sequence. (Docs: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "\n",
    "The first dimension of input is always the batch size, and all the neural network layers are equipped to handle this scenario. So, for simple operations like forward passes from network layers, you will never need to handle batching explicitly.\n",
    "\n",
    "In PyTorch, all models are extended from `nn.Module` because it serves as the base class for all neural network modules, providing essential functionalities that are crucial for building and training deep learning models. Here’s why extending from nn.Module is important:\n",
    "\n",
    "1. Parameter Management\n",
    "- `nn.Module` automatically registers all the layers (and their parameters) defined within the model. This allows PyTorch to easily access and manage these parameters for operations like training, saving, and loading the model.\n",
    "2. Forward Propagation\n",
    "- `nn.Module` defines the structure that allows you to override the forward method, which is where the forward pass of the network is defined. When you extend from nn.Module, you define how data flows through your model by implementing the forward method.\n",
    "3. Module Nesting\n",
    "- `nn.Module` allows for the creation of complex models by composing simpler sub-modules. When sub-modules (layers, blocks, etc.) are added as attributes of a custom nn.Module, they are automatically registered and their parameters are included in the parent module’s parameters.\n",
    "4. Utility Functions\n",
    "- `nn.Module` provides a variety of utility functions like .to() for moving the model to different devices (CPU/GPU), .eval() and .train() for setting the model’s mode, and methods for saving/loading model parameters.\n",
    "5. Training and Inference\n",
    "- By extending from `nn.Module`, your custom model can easily integrate with PyTorch’s training loop utilities like optimizers and loss functions. The backward pass and parameter updates are handled smoothly when using an nn.Module.\n",
    "\n",
    "Docs: https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "id": "OvV1vEhoNxKh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNLPModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx=0):\n",
    "        super(SimpleNLPModel, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # First hidden layer for each token's embedding\n",
    "        self.linear = nn.Linear(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Output layer\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Padding index to be ignored\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Step 1: Get embeddings for each token in the batch\n",
    "        embedded = self.embedding(x)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # Step 2: Create a mask for padding tokens\n",
    "        mask = (x != self.pad_idx).unsqueeze(-1).float()  # Shape: (batch_size, seq_len, 1)\n",
    "        \n",
    "        # Apply mask to embedded\n",
    "        masked_embedded = embedded * mask  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # Step 3: Sum Pooling\n",
    "        pooled = torch.sum(masked_embedded, dim=1)\n",
    "        # Shape: (batch_size, embedding_dim)\n",
    "        \n",
    "        # Step 4: Apply linear layer to each token embedding\n",
    "        linear_out = self.linear(pooled)  # Shape: (batch_size, hidden_dim)\n",
    "        # Apply non-linearity\n",
    "        linear_out = F.relu(linear_out)\n",
    "        \n",
    "        # Step 5: Apply final classification layer\n",
    "        output = self.classifier(linear_out)  # Shape: (batch_size, output_dim)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "model = SimpleNLPModel(vocab_size, 128, 32, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11. Forward Pass\n",
    "\n",
    "Taking a batch input and passing it through the model to get the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = torch.tensor([0] * 10).reshape(2,5)\n",
    "model = SimpleNLPModel(vocab_size, 128, 32, 8)\n",
    "\n",
    "#Executing the below code will run one forward pass on the model\n",
    "out = model(batch_input)\n",
    "\n",
    "print(out)\n",
    "print(out.shape)\n",
    "# Notice that the first dimension is already two, accounting for the batch. The second dimension is output dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Pitfalls\n",
    "\n",
    "The embedding layer's vocab size should be correctly defined, and all token indices should be between 0 to n-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you define your vocab size incorectly, and give an index that's < 0 or > len(vocab), you will get an error\n",
    "batch_input = torch.tensor([len(vocab)+1] * 10).reshape(2,5)\n",
    "print('Input - ', batch_input)\n",
    "\n",
    "try:\n",
    "    out = model(batch_input)\n",
    "except IndexError as e:\n",
    "    print('Error -',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Pitfalls\n",
    "\n",
    "Data and model should be on the same device to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = batch_input.to(device=device)\n",
    "print('Device of batch -', batch_input.device)\n",
    "print('Device of model -', next(model.parameters()).device)\n",
    "try:\n",
    "    out = model(batch_input)\n",
    "except RuntimeError as e:\n",
    "    print('Error -',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12. Softmax \n",
    "Softmax is a mathematical function used in machine learning, particularly in classification tasks. It converts raw scores (logits) from a model into probabilities by normalizing them. The output is a probability distribution where each value represents the likelihood of each class, and all probabilities sum up to 1.\n",
    "\n",
    "\n",
    "Formula:\n",
    "For a vector $ z = [z_1, z_2, \\ldots, z_n] $, the softmax function is given by:\n",
    "$$\n",
    " \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "$$\n",
    "\n",
    "where  e  is the base of the natural logarithm, $ z_i $ is the raw score for class  i , and the denominator sums over all classes to normalize the scores.\n",
    "\n",
    "Softmax is commonly used in the final layer of a classification model to output probabilities for each class.\n",
    "\n",
    "While working with softmax in pytorch, the dim parameter dictates across which dimension the softmax operations is done. Basically, the softmax operation will transform your input into a probability distribution i.e. the sum of all elements will be 1. The below example shows the difference between using dim=0 or dim=1 for a 2D input tensor (supposing the first dimension for the batch size, and the second for the number of classes).\n",
    "\n",
    "Docs: https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input tensor of dimensions B x C, B = number of batches, C = number of classes.\n",
    "inputs = torch.rand(size=(4, 4), dtype=torch.float32)\n",
    "soft_dim0 = torch.softmax(inputs, dim=0)\n",
    "soft_dim1 = torch.softmax(inputs, dim=1)\n",
    "print('**** INPUTS ****')\n",
    "print(inputs)\n",
    "print('**** SOFTMAX DIM=0 ****')\n",
    "print(soft_dim0)\n",
    "print('**** SOFTMAX DIM=1 ****')\n",
    "print(soft_dim1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.13. Backward Pass\n",
    "\n",
    "Calculating the gradients and updating them. After the forward pass, first, loss will be calculated and then gradients will be calculated.\n",
    "\n",
    "\n",
    "\n",
    "#### Cross-Entropy Loss\n",
    "\n",
    "Cross-entropy is a loss function used to measure the difference between two probability distributions. In classification tasks, it measures how well the predicted probability distribution (obtained from softmax) matches the true distribution (actual class labels).\n",
    "\n",
    "Formula:\n",
    "For a single data point, cross-entropy loss  L  is given by:\n",
    "$$\n",
    " L = -\\sum_{i} y_i \\log(p_i)\n",
    "$$\n",
    "where:\n",
    "\n",
    "- $y_i$ is the true probability (usually 1 for the correct class and 0 for others),\n",
    "- $p_i$ is the predicted probability for class  i .\n",
    "\n",
    "Usage: Cross-entropy is used to train classification models by minimizing the difference between the predicted probabilities and the actual labels. It is particularly effective in guiding the model to produce accurate probability distributions.\n",
    "\n",
    "Docs: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Pitfalls -\n",
    "1. Both model output (predictions) and labels should be on the same device, of the same shape\n",
    "2. Both should be float values\n",
    "3. Softmax is already implemened implicitly in the CrossEntropyLoss. No need to do it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_input = torch.tensor([1,2,3,4,5,6,7,8,9,10]).reshape(2,5)\n",
    "model = SimpleNLPModel(vocab_size, 128, 32, 8)\n",
    "dummy_labels_wrong=torch.tensor([1,2,3,4,5,6,7,8,9,10]).reshape(2,5)\n",
    "dummy_labels = torch.tensor([1,2,3,4,5,6,7,8,9,10,11,13,12,14,15,16]).reshape(2,8)\n",
    "#Executing the below code will run one forward pass on the model\n",
    "out = model(batch_input)\n",
    "\n",
    "try:\n",
    "    loss=criterion(out, dummy_labels_wrong)\n",
    "except RuntimeError as e:\n",
    "    print('Error -', e)\n",
    "print('--------------------')\n",
    "\n",
    "try:\n",
    "    loss = criterion(out, dummy_labels)\n",
    "except RuntimeError as e:\n",
    "    print('Error -', e)\n",
    "print('--------------------')\n",
    "\n",
    "dummy_labels = dummy_labels.to(torch.float32)\n",
    "loss = criterion(out, dummy_labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each parameter in the model to check values and gradients\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Parameter: {name}\")\n",
    "        print(f'Shape: ',param.data.shape)\n",
    "        print(f\"Values: {param.data}\")  # Prints the parameter values\n",
    "        print(f\"Gradients: {param.grad}\")  # Prints the gradients of the parameters\n",
    "        print(\"-------\")\n",
    "        break # (comment to check all layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward() # calculates gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the gradient and values again\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Parameter: {name}\")\n",
    "        print(f\"Values: {param.data}\")  # Prints the parameter values\n",
    "        print(f\"Gradients: {param.grad}\")  # Prints the gradients of the parameters\n",
    "        print(\"-------\")\n",
    "        break # (comment to check all layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the values of parameters still didn't change. It is because how they is determined through an algorithm, called the Optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.14. Optimizers\n",
    "Optimizers are algorithms or methods used to adjust the weights of neural networks to minimize the loss function during training.\n",
    "They update the model parameters based on the computed gradients from backpropagation, guiding the model towards better performance. Different optimizers, such as SGD (Stochastic Gradient Descent), RMSprop, Adam, etc., have unique ways of adjusting the learning rates and handling gradients, which can impact the speed and stability of training.\n",
    "\n",
    "The Adam optimizer is chosen because it is efficient, works well with large datasets and models, and often provides faster and more stable convergence than traditional optimizers like SGD.\n",
    "\n",
    "Docs: https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001) \n",
    "# expect model parameters and other hyperparameters like lr, beta, etc., based on the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step() # update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's check the gradient and values again\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Parameter: {name}\")\n",
    "        print(f\"Values: {param.data}\")  # Prints the parameter values\n",
    "        print(f\"Gradients: {param.grad}\")  # Prints the gradients of the parameters\n",
    "        print(\"-------\")\n",
    "        break # (comment to check all layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common pitfall with backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see, the weights adjusted, but the model still has gradient values stored. If we repeat this step, gradients of the next batch will be added. This will not throw an error, but internally, it will add the gradients of the next step with current step, and so on. This is not desired in ideal scenario. So, we do an additional step called optimizer.zero_grad() to zero out these accumulated gradients. This helps to start fresh in next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_2 = torch.tensor([1,2,3,4,5,6,7,8,9,10]).reshape(2,5)\n",
    "dummy_labels_2 = torch.tensor([1,2,3,4,5,6,7,8,9,10,11,13,12,14,15,16]).reshape(2,8)\n",
    "dummy_labels_2 = dummy_labels_2.to(torch.float32)\n",
    "out_2 = model(batch_input_2)\n",
    "loss = criterion(out_2, dummy_labels_2)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the gradient and values again\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Parameter: {name}\")\n",
    "        print(f\"Values: {param.data}\")  # Prints the parameter values\n",
    "        print(f\"Gradients: {param.grad}\")  # Prints the gradients of the parameters\n",
    "        print(\"-------\")\n",
    "        break # (comment to check all layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, the gradients are cleared\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Parameter: {name}\")\n",
    "        print(f\"Values: {param.data}\")  # Prints the parameter values\n",
    "        print(f\"Gradients: {param.grad}\")  # Prints the gradients of the parameters\n",
    "        print(\"-------\")\n",
    "        break # (comment to check all layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_2 = torch.tensor([1,2,3,4,5,6,7,8,9,10]).reshape(2,5)\n",
    "# CrossEntropyLoss expects 1D tensor of class indices, not 2D one-hot encoded\n",
    "dummy_labels_2 = torch.tensor([1, 2])  # Class indices for batch of 2 samples\n",
    "out_2 = model(batch_input_2)\n",
    "loss = criterion(out_2, dummy_labels_2)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, the gradients are cleared\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Parameter: {name}\")\n",
    "        print(f\"Values: {param.data}\")  # Prints the parameter values\n",
    "        print(f\"Gradients: {param.grad}\")  # Prints the gradients of the parameters\n",
    "        print(\"-------\")\n",
    "        break # (comment to check all layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't handle them properly, your program won't throw any errors, but not allow your model to train properly. So, please make sure you handle these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.15. Saving Model Weights\n",
    "\n",
    "When training deep learning models, it's essential to save your model's state so you can resume training later, evaluate the model, or deploy it to production. PyTorch provides mechanisms to save and load different parts of your model, such as:\n",
    "\n",
    "Model Weights (state_dict): This saves the model's learned parameters.\n",
    "\n",
    "Entire Model: Saving the entire model includes the model architecture and weights.\n",
    "\n",
    "Optimizer State: This saves the state of the optimizer, which is important for resuming training with the same learning rate schedule and momentum.\n",
    "\n",
    "Epoch and Loss Information: Saving the current epoch and loss allows you to resume training from a specific point.\n",
    "\n",
    "Each of these components is essential depending on your use case. For example, if you only need the model for inference, saving the state_dict might be sufficient. However, if you want to continue training, you'll need to save the optimizer state and epoch information as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, file_path='checkpoint.pth'):\n",
    "    \"\"\"\n",
    "    Saves the model state, optimizer state, current epoch, and loss to a checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model whose state needs to be saved.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer whose state needs to be saved.\n",
    "        epoch (int): The current epoch number.\n",
    "        loss (float): The current loss value.\n",
    "        file_path (str): The path to the file where the checkpoint will be saved.\n",
    "    \"\"\"\n",
    "    checkpoint = { # create a dictionary with all the state information\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, file_path)\n",
    "    print(f\"Checkpoint saved to {file_path}\")\n",
    "\n",
    "# Example usage:\n",
    "# save_checkpoint(model, optimizer, epoch, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNLPModel(vocab_size, 128, 32, 5)\n",
    "batch_input_2 = torch.tensor([1,2,3,4,5,6,7,8,9,10]).reshape(2,5)\n",
    "# CrossEntropyLoss expects 1D tensor of class indices, not 2D one-hot encoded\n",
    "dummy_labels_2 = torch.tensor([1, 2])  # Class indices for batch of 2 samples\n",
    "out = model(batch_input_2)\n",
    "loss = criterion(out, dummy_labels_2)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.0005)\n",
    "\n",
    "SAVE_DIR = './pytorch_checkpoints'\n",
    "\n",
    "import os\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "save_checkpoint(model, optimizer, 10, loss, './pytorch_checkpoints/demo_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(file_path, model, optimizer=None, map_location=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    Loads the model state, optimizer state, current epoch, and loss from a checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the checkpoint file.\n",
    "        model (torch.nn.Module): The model into which the state will be loaded.\n",
    "        optimizer (torch.optim.Optimizer, optional): The optimizer into which the state will be loaded. Defaults to None.\n",
    "        map_location (torch.device, optional): Device to map the storage to when loading. Defaults to CPU.\n",
    "\n",
    "    Returns:\n",
    "        int: The epoch at which the training was saved.\n",
    "        float: The loss value at the time of saving.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(file_path, map_location=map_location) # load the checkpoint, ensure correct device\n",
    "    model.load_state_dict(checkpoint['model_state_dict']) # load the model with model state dict\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict']) # it is important to initialize\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    \n",
    "    print(f\"Checkpoint loaded from {file_path}\")\n",
    "    return epoch, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Pitfall\n",
    "\n",
    "The architecture of the model/optimizer should be the same as the ones for which weights are saved, otherwise, they are not compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNLPModel(vocab_size, 128, 32, 8) # notice the change in output layer shape\n",
    "try:\n",
    "    load_checkpoint('./pytorch_checkpoints/demo_1.pth', model, optimizer)\n",
    "except RuntimeError as e:\n",
    "    print('Error -', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNLPModel(vocab_size, 128, 32, 5) # these will load randomly initialized weights\n",
    "# to resume training, you should first load the trained weights of the last epoch\n",
    "model = model.to(device)\n",
    "try:\n",
    "    load_checkpoint('./pytorch_checkpoints/demo_1.pth', model, optimizer)\n",
    "except RuntimeError as e:\n",
    "    print('Error -', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dos and Don'ts\n",
    "\n",
    "- Do save your model’s state_dict regularly during training, especially when working on long-running experiments.\n",
    "- Do save the optimizer state if you plan to resume training. Sometimes you dynamically change your learning rate during training, and therefore, re-loading the correct learning rate for that epoch is crucial to resume training.\n",
    "- Don’t use torch.save() to save the entire model unless you are sure the model class will not change. Saving the state_dict is more flexible and safer.\n",
    "- Do use map_location when loading a model trained on a different device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gzJsRJXNjVS"
   },
   "source": [
    "### 2.16. Early Stopping\n",
    "\n",
    "Early Stopping is a popular regularization technique used in machine learning to prevent overfitting during the training of models, particularly neural networks. It works by monitoring the performance of the model on a validation set and halting the training process when the model’s performance starts to degrade, indicating that further training would lead to overfitting rather than improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "id": "8aFVm_faNlZu"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None  # Initialize with None to correctly handle the first update\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_score is None:  # First time, set the best score\n",
    "            self.best_score = val_loss\n",
    "        elif val_loss < self.best_score - self.min_delta:  # If the current loss improves over the best score\n",
    "            self.best_score = val_loss  # Update the best score\n",
    "            self.counter = 0  # Reset the counter since we have an improvement\n",
    "        else:\n",
    "            self.counter += 1  # No improvement\n",
    "            if self.counter >= self.patience:  # Check if we've hit the patience limit\n",
    "                self.early_stop = True  # Trigger early stopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCujBJvyhEsf"
   },
   "source": [
    "### 2.17. Training Loop\n",
    "\n",
    "The training loop updates the model's parameters by processing batches of data, calculating loss, and backpropagating gradients to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2.17. Addressing Overfitting with Regularization Techniques\n",
    "\n",
    "From your loss plot, it's clear that the model is overfitting - the training loss continues to decrease while validation loss starts increasing after epoch 2-3. Here are several strategies to combat overfitting:\n",
    "\n",
    "### Key Strategies:\n",
    "1. **Reduce Learning Rate**: Lower learning rates help the model converge more slowly and avoid overfitting\n",
    "2. **Add Dropout**: Randomly sets some neurons to zero during training to prevent over-reliance on specific features\n",
    "3. **Reduce Model Complexity**: Smaller hidden dimensions can prevent the model from memorizing the training data\n",
    "4. **Early Stopping**: Stop training when validation loss starts increasing\n",
    "5. **Weight Decay (L2 Regularization)**: Penalizes large weights to encourage simpler models\n",
    "6. **Reduce Number of Epochs**: Train for fewer epochs to prevent overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated model with dropout for regularization\n",
    "class RegularizedNLPModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx=0, dropout_rate=0.3):\n",
    "        super(RegularizedNLPModel, self).__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # Add dropout after embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Linear layers with dropout\n",
    "        self.linear = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.hidden_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Classification layer\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        embedded = self.embedding_dropout(embedded)  # Apply dropout to embeddings\n",
    "        \n",
    "        # Create mask for padding tokens\n",
    "        mask = (x != self.pad_idx).unsqueeze(-1).float()  # Shape: (batch_size, seq_len, 1)\n",
    "        \n",
    "        # Apply mask to embeddings\n",
    "        masked_embedded = embedded * mask  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # Average pooling (ignoring padded positions)\n",
    "        pooled = masked_embedded.sum(dim=1) / mask.sum(dim=1)  # Shape: (batch_size, embedding_dim)\n",
    "        \n",
    "        # Linear transformation with dropout\n",
    "        linear_out = self.linear(pooled)  # Shape: (batch_size, hidden_dim)\n",
    "        linear_out = torch.relu(linear_out)\n",
    "        linear_out = self.hidden_dropout(linear_out)  # Apply dropout\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(linear_out)  # Shape: (batch_size, output_dim)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated hyperparameters to reduce overfitting\n",
    "# Original values that were causing overfitting:\n",
    "# batch_size = 32\n",
    "# learning_rate = 0.001\n",
    "# num_epochs = 20\n",
    "# embedding_dim = 128\n",
    "# hidden_dim = 64\n",
    "\n",
    "# New regularized hyperparameters\n",
    "batch_size = 16  # Smaller batch size for more noisy gradients\n",
    "embedding_dim = 64  # Reduced from 128 to decrease model capacity\n",
    "hidden_dim = 32  # Reduced from 64 to decrease model capacity\n",
    "learning_rate = 0.0005  # Reduced from 0.001 for slower, more stable learning\n",
    "num_epochs = 10  # Reduced from 20 to prevent overfitting\n",
    "dropout_rate = 0.3  # Add dropout for regularization\n",
    "weight_decay = 1e-4  # L2 regularization\n",
    "\n",
    "print(\"Updated Hyperparameters for Overfitting Prevention:\")\n",
    "print(f\"Batch Size: {batch_size} (reduced from 32)\")\n",
    "print(f\"Learning Rate: {learning_rate} (reduced from 0.001)\")\n",
    "print(f\"Epochs: {num_epochs} (reduced from 20)\")\n",
    "print(f\"Embedding Dim: {embedding_dim} (reduced from 128)\")\n",
    "print(f\"Hidden Dim: {hidden_dim} (reduced from 64)\")\n",
    "print(f\"Dropout Rate: {dropout_rate} (new)\")\n",
    "print(f\"Weight Decay: {weight_decay} (new)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regularized model and optimizer with weight decay\n",
    "vocab_size = len(vocab)\n",
    "regularized_model = RegularizedNLPModel(vocab_size, embedding_dim, hidden_dim, 3, dropout_rate=dropout_rate)\n",
    "regularized_model = regularized_model.to(device)\n",
    "\n",
    "# Optimizer with weight decay (L2 regularization)\n",
    "regularized_optimizer = optim.Adam(regularized_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in regularized_model.parameters())} parameters\")\n",
    "print(f\"Regularized model vs Original model parameter reduction: {sum(p.numel() for p in regularized_model.parameters())} vs ~{128*64 + 64*3 + 128*len(vocab)}\")\n",
    "print(\"Regularization techniques applied:\")\n",
    "print(\"- Dropout layers added\")\n",
    "print(\"- Model capacity reduced\")\n",
    "print(\"- Weight decay added to optimizer\")\n",
    "print(\"- Learning rate reduced\")\n",
    "print(\"- Training epochs reduced\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new data loaders with smaller batch size\n",
    "train_loader_reg = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader_reg = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Data loaders created with batch size: {batch_size}\")\n",
    "print(f\"Training batches: {len(train_loader_reg)}\")\n",
    "print(f\"Validation batches: {len(val_loader_reg)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced training function with early stopping\n",
    "def train_model_with_early_stopping(model, optimizer, criterion, train_loader, val_loader, num_epochs, device, \n",
    "                                   save_dir, patience=3, min_delta=0.001):\n",
    "    \"\"\"\n",
    "    Trains the model with early stopping to prevent overfitting.\n",
    "    \n",
    "    Args:\n",
    "        patience (int): Number of epochs to wait for improvement before stopping\n",
    "        min_delta (float): Minimum change in validation loss to qualify as improvement\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()  # Set model to training mode (enables dropout)\n",
    "        running_train_loss = 0.0\n",
    "        \n",
    "        with tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as tepoch:\n",
    "            for batch_idx, (inputs, targets) in enumerate(tepoch):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_train_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=running_train_loss / (batch_idx + 1))\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluation mode (disables dropout)\n",
    "        running_val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                running_val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if avg_val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, os.path.join(save_dir, 'best_regularized_model.pth'))\n",
    "            print(f'New best model saved at epoch {epoch+1}')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'No improvement for {patience_counter} epochs')\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            print(f'Best validation loss: {best_val_loss:.4f} at epoch {best_epoch+1}')\n",
    "            break\n",
    "    \n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the regularized model\n",
    "print(\"Training regularized model with early stopping...\")\n",
    "reg_train_losses, reg_val_losses = train_model_with_early_stopping(\n",
    "    regularized_model, \n",
    "    regularized_optimizer, \n",
    "    criterion, \n",
    "    train_loader_reg, \n",
    "    val_loader_reg, \n",
    "    num_epochs, \n",
    "    device,\n",
    "    './pytorch_checkpoints/regularized_training/',\n",
    "    patience=3,\n",
    "    min_delta=0.001\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison between original and regularized training\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(reg_train_losses, label='Regularized Train Loss', color='blue')\n",
    "plt.plot(reg_val_losses, label='Regularized Val Loss', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Regularized Model: Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot comparison if you have original losses (you can uncomment and modify as needed)\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(original_train_losses, label='Original Train Loss', linestyle='--', alpha=0.7)\n",
    "# plt.plot(original_val_losses, label='Original Val Loss', linestyle='--', alpha=0.7)\n",
    "# plt.plot(reg_train_losses, label='Regularized Train Loss')\n",
    "# plt.plot(reg_val_losses, label='Regularized Val Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Original vs Regularized Model Comparison')\n",
    "# plt.legend()\n",
    "# plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRegularization Results:\")\n",
    "if len(reg_val_losses) > 1:\n",
    "    min_val_loss_epoch = reg_val_losses.index(min(reg_val_losses)) + 1\n",
    "    print(f\"Best validation loss: {min(reg_val_losses):.4f} at epoch {min_val_loss_epoch}\")\n",
    "    print(f\"Final validation loss: {reg_val_losses[-1]:.4f}\")\n",
    "    \n",
    "    # Check if overfitting is reduced\n",
    "    if len(reg_val_losses) >= 3:\n",
    "        early_val_loss = reg_val_losses[1]  # 2nd epoch\n",
    "        late_val_loss = reg_val_losses[-1]  # last epoch\n",
    "        if late_val_loss <= early_val_loss:\n",
    "            print(\"✅ Overfitting appears to be reduced - validation loss didn't increase significantly\")\n",
    "        else:\n",
    "            print(\"⚠️  Some overfitting may still be present - consider further regularization\")\n",
    "else:\n",
    "    print(\"Training completed in 1 epoch or less\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Additional Hyperparameter Tuning Suggestions\n",
    "\n",
    "If the regularized model above still shows signs of overfitting, you can try these additional techniques:\n",
    "\n",
    "**Further Regularization:**\n",
    "- Increase dropout rate to 0.4-0.5\n",
    "- Increase weight decay to 1e-3\n",
    "- Reduce learning rate further to 0.0001\n",
    "\n",
    "**Data Augmentation (for NLP):**\n",
    "- Add synonym replacement\n",
    "- Random word deletion\n",
    "- Sentence shuffling\n",
    "\n",
    "**Model Architecture Changes:**\n",
    "- Use even smaller embedding dimensions (32 or 16)\n",
    "- Add batch normalization\n",
    "- Use different optimizers (SGD with momentum)\n",
    "\n",
    "**Training Strategy:**\n",
    "- Use learning rate scheduling (reduce on plateau)\n",
    "- Implement gradient clipping\n",
    "- Use different loss functions (Label smoothing)\n",
    "\n",
    "**Example of further reduced hyperparameters:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of even more aggressive regularization if needed\n",
    "aggressive_hyperparams = {\n",
    "    'batch_size': 8,           # Even smaller batches\n",
    "    'embedding_dim': 32,       # Much smaller embeddings  \n",
    "    'hidden_dim': 16,          # Much smaller hidden layer\n",
    "    'learning_rate': 0.0001,   # Much slower learning\n",
    "    'num_epochs': 5,           # Fewer epochs\n",
    "    'dropout_rate': 0.5,       # Higher dropout\n",
    "    'weight_decay': 1e-3,      # Stronger L2 regularization\n",
    "    'patience': 2              # Earlier stopping\n",
    "}\n",
    "\n",
    "print(\"Aggressive regularization hyperparameters (use if overfitting persists):\")\n",
    "for param, value in aggressive_hyperparams.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\nKey takeaways for preventing overfitting:\")\n",
    "print(\"1. Start with aggressive regularization and gradually relax if underfitting\")\n",
    "print(\"2. Monitor the gap between training and validation loss\")\n",
    "print(\"3. Use early stopping to prevent training too long\")\n",
    "print(\"4. Smaller models often generalize better than larger ones\")\n",
    "print(\"5. Dropout is one of the most effective regularization techniques\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demos -\n",
    "\n",
    "1. Show result with learning rate = 0.001\n",
    "2. In high learning rate, show results by varying early stopping criteria\n",
    "3. Show result with a very low batch size (4)\n",
    "4. Show result with learning rate = 0.00001\n",
    "\n",
    "#### Take Home Analysis -\n",
    "1. What happens if the embedding dimensions are very high?\n",
    "2. What happens if embedding dimensions are very low?\n",
    "3. Same analysis for hidden dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 300\n",
    "batch_size = 32\n",
    "embedding_dim = 256\n",
    "hidden_dim = 64\n",
    "learning_rate = 0.00003\n",
    "patience = 3\n",
    "min_delta = 0.001\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNLPModel(vocab_size, embedding_dim, hidden_dim, 3)\n",
    "model=model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Early stopping instance\n",
    "early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "\n",
    "train_dataset = TextDataset(train_df, vocab, max_len=max_len)\n",
    "val_dataset = TextDataset(val_df, vocab, max_len=max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def train_model(model, optimizer, criterion, train_loader, val_loader, num_epochs, device, save_dir,\n",
    "                early_stopping=None):\n",
    "    \"\"\"\n",
    "    Trains the model and saves the final model after all epochs are completed.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer for training.\n",
    "        criterion (torch.nn.Module): The loss function.\n",
    "        train_loader (torch.utils.data.DataLoader): DataLoader for the training data.\n",
    "        val_loader (torch.utils.data.DataLoader): DataLoader for the validation data.\n",
    "        num_epochs (int): Number of epochs to train for.\n",
    "        device (torch.device): The device (CPU or GPU) to run the model on.\n",
    "        early_stopping (optional): An early stopping object, if early stopping is desired. Defaults to None.\n",
    "        save_path (str): The file path to save the final model. Defaults to 'final_model.pth'.\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):  # Defines number of epochs\n",
    "        model.train()  # Set model to training mode\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        # Training phase\n",
    "        with tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as tepoch:\n",
    "            for inputs, labels in tepoch:\n",
    "                inputs, labels = inputs.to(torch.int32), labels.to(torch.int64)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "                outputs = model(inputs)  # Forward pass\n",
    "\n",
    "                loss = criterion(outputs, labels)  # Compute the loss\n",
    "                loss.backward()  # Backpropagation\n",
    "                optimizer.step()  # Optimization step\n",
    "\n",
    "                running_train_loss += loss.item()\n",
    "                \n",
    "                # Update tqdm with the current running loss\n",
    "                tepoch.set_postfix(loss=running_train_loss / (tepoch.n + 1))\n",
    "\n",
    "        # Calculate average training loss for the epoch\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        running_val_loss = 0.0\n",
    "\n",
    "        # Validation phase\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(torch.int32), labels.to(torch.int64)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "        # Calculate average validation loss for the epoch\n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "        os.makedirs(save_dir, exist_ok=True) # create directory if doesn't exist\n",
    "        save_path = os.path.join(save_dir, f'checkpoint_{epoch}.pth')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_save_path = os.path.join(save_dir, 'checkpoint_best.pt')\n",
    "            save_checkpoint(model, optimizer, epoch, avg_val_loss, file_path=best_save_path)\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stopping:\n",
    "            early_stopping(avg_val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered\")\n",
    "                save_checkpoint(model, optimizer, epoch, avg_val_loss, file_path=save_path)\n",
    "                break\n",
    "\n",
    "        save_checkpoint(model, optimizer, epoch, avg_val_loss, file_path=save_path)\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses = train_model(model, optimizer, criterion, train_loader, val_loader, num_epochs, 'cuda',\n",
    "                                      './pytorch_checkpoints/model_training', early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "id": "4Yrj4UeQfic0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.18. Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Tests the model on the test dataset and applies a threshold to the outputs for classification.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model to test.\n",
    "        test_loader (torch.utils.data.DataLoader): DataLoader for the test data.\n",
    "        device (torch.device): The device (CPU or GPU) to run the model on.\n",
    "\n",
    "    Returns:\n",
    "        float: The overall accuracy of the model on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    true_labels_all = []\n",
    "    predicted_labels_all = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(torch.int32), labels.to(torch.int64)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            outputs = F.softmax(outputs, dim=1) # is doing softmax here necessary? Why? Why not?\n",
    "            _, predicted = torch.max(outputs, 1)  # Choose the class with the highest score\n",
    "\n",
    "            # Since labels are now class indices (int64), we don't need torch.max\n",
    "            # labels are already in the correct format: [0, 1, 2, 1, ...]\n",
    "            true_labels = labels\n",
    "\n",
    "            correct += (predicted == true_labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            true_labels_all.extend(true_labels.cpu().numpy())\n",
    "            predicted_labels_all.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Test Accuracy(%): {accuracy:.4f}\")\n",
    "\n",
    "    return true_labels_all, predicted_labels_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "id": "78nc_epWmn9S"
   },
   "outputs": [],
   "source": [
    "model = SimpleNLPModel(vocab_size, embedding_dim, hidden_dim, 3)\n",
    "model=model.to(device)\n",
    "test_dataset = TextDataset(test_df, vocab, max_len=max_len)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, predicted_labels = test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 classes, 33% accuracy - random chance\n",
    "\n",
    "But we trained the model right? What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch, loss = load_checkpoint('./pytorch_checkpoints/model_training/checkpoint_best.pt', model, None, 'cuda')\n",
    "print('Last epoch=', epoch, 'Last loss=', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, predicted_labels = test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating a multi-class classification model, it's essential to understand a variety of metrics to get a comprehensive view of your model's performance. Below are some key metrics:\n",
    "\n",
    "#### 1. Confusion Matrix\n",
    "\n",
    "A **Confusion Matrix** is a table that is often used to describe the performance of a classification model. Each row of the matrix represents the instances of the true class, while each column represents the instances of the predicted class. In a multi-class classification scenario, the confusion matrix can provide detailed insight into how well the model is performing for each class.\n",
    "\n",
    "- **True Positives (TP)**: The number of instances correctly predicted as belonging to a class.\n",
    "- **False Positives (FP)**: The number of instances incorrectly predicted as belonging to a class.\n",
    "- **True Negatives (TN)**: The number of instances correctly predicted as not belonging to a class.\n",
    "- **False Negatives (FN)**: The number of instances incorrectly predicted as not belonging to a class.\n",
    "\n",
    "The confusion matrix allows you to see not only the accuracy of the predictions but also where the model is getting confused between classes.\n",
    "\n",
    "#### 2. Precision\n",
    "\n",
    "**Precision** is a metric that tells us how many of the instances predicted as positive (or as a specific class in multi-class classification) are actually positive (or belong to that class). It is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "$$\n",
    "\n",
    "High precision indicates that the model has a low false positive rate. In a multi-class context, precision is typically averaged across all classes to give a single precision score, often using the **weighted average** to account for class imbalance.\n",
    "\n",
    "#### 3. Recall\n",
    "\n",
    "**Recall** (also known as sensitivity or true positive rate) tells us how many of the actual positive instances (or a specific class) were correctly identified by the model. It is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "$$\n",
    "\n",
    "High recall indicates that the model has a low false negative rate. Like precision, recall can also be averaged across all classes.\n",
    "\n",
    "#### 4. F1 Score\n",
    "\n",
    "The **F1 Score** is the harmonic mean of precision and recall. It provides a single metric that balances both precision and recall:\n",
    "\n",
    "$$\n",
    "\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "The F1 score is especially useful when you need to find a balance between precision and recall, and there is an uneven class distribution (one class might be more frequent than others).\n",
    "\n",
    "#### 5. Accuracy\n",
    "\n",
    "**Accuracy** is the simplest and most commonly used metric, which measures the proportion of correctly predicted instances (both true positives and true negatives) out of the total number of instances:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "$$\n",
    "\n",
    "While accuracy gives a good general sense of the model’s performance, it might not be the best metric in cases of class imbalance. In such cases, precision, recall, and F1 score provide a more detailed view.\n",
    "\n",
    "#### How These Metrics are Computed\n",
    "\n",
    "During model evaluation:\n",
    "- The **confusion matrix** provides a summary of the predictions made by the model compared to the actual labels.\n",
    "- **Precision** and **recall** focus on the performance of the model for each class, especially in distinguishing between correct and incorrect predictions.\n",
    "- The **F1 score** offers a balance between precision and recall, particularly useful when you want to ensure that both false positives and false negatives are minimized.\n",
    "- **Accuracy** gives an overall measure of how often the model's predictions are correct, but should be interpreted carefully in the presence of class imbalance.\n",
    "\n",
    "These metrics are essential for understanding the strengths and weaknesses of your classification model, guiding you toward potential improvements or areas needing further tuning.\n",
    "\n",
    "Docs:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(len(classes)))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def calculate_metrics(all_labels, all_preds):\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(true_labels, predicted_labels, classes=['neutral', 'worry', 'happiness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1, precision, recall = calculate_metrics(true_labels, predicted_labels)\n",
    "\n",
    "# Print each metric on a new line\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.19. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: 'neutral',\n",
    "    1: 'worry',\n",
    "    2: 'happiness'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For recap, this was our tokenizer\n",
    "def tokenize_text(text):\n",
    "    arr = [word for word in text.split()]\n",
    "    return arr\n",
    "\n",
    "def predict(text, model):\n",
    "    tokens = tokenize_text(text)\n",
    "    ids = []\n",
    "    for token in tokens:\n",
    "        id = vocab.get_index(token)\n",
    "        ids.append(id)\n",
    "    batch = torch.tensor(ids).unsqueeze(0)\n",
    "    batch = batch.to(next(model.parameters()).device)\n",
    "    outputs = model(batch)\n",
    "    outputs = F.softmax(outputs, dim=1) # is doing softmax here necessary? Why? Why not?\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    index = predicted.cpu().numpy()\n",
    "    index = index[0]\n",
    "    label = id2label[index]\n",
    "    return label\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNLPModel(vocab_size, embedding_dim, hidden_dim, 3)\n",
    "model=model.to(device)\n",
    "epoch, loss = load_checkpoint('./pytorch_checkpoints/model_training/checkpoint_best.pt', model, None, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I am very worried'\n",
    "predict(text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I will definitely fail NLP with bad marks and delay my graduation'\n",
    "predict(text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Good morning, what a great day'\n",
    "predict(text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I am sleepy'\n",
    "predict(text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes, things are not just captured by embedding of the texts. Tone is important.\n",
    "text = 'I will pass NLP with good grade' # could be happy, could be neutral, depends\n",
    "predict(text, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Reasons of Inaccuracies -\n",
    "\n",
    "1. Our preprocessing was very basic and we used word-tokenization. Typically, NLP systems use more complex preprocessing to clean the data, like removal of stop words, removal of less frequent words, stemming, etc., and use sub-word tokenization. They also handle punctuations differently.\n",
    "2. We used a very simple feedforward network. This is not very suited for complex NLP tasks, as they don't capture the semantic relationships well. Using RNNs/LSTMs/Transformers will yeild better results.\n",
    "3. Class imbalance - We can see that the first two classes have around 3500 more data instances than the third class (happiness). This also contributes to inaccuracies, as the training data doesn't represent that. There are ways to avoid this class imbalance problem like over/under sampling, generating synthetic data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.20 Naive, Regex-based Classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll create a naive, regex-based classifier as a baseline to compare against our neural network. While we expect this approach to be weaker, it has the advantage of being highly interpretable and can help us understand what patterns might be driving sentiment classification.\n",
    "\n",
    "Online tools such as [regex101.com](https://regex101.com) is useful for fixing errors in your regex expressions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build naive classifier function which uses hardcoded regex patterns \n",
    "# to find the most common bigrams for each sentiment in the test set. \n",
    "import re\n",
    "\n",
    "class RegexSentimentClassifier:\n",
    "    \"\"\"\n",
    "    A naive regex-based sentiment classifier that looks for hardcoded patterns\n",
    "    associated with each sentiment class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define regex patterns for each sentiment based on our analysis\n",
    "        # These patterns are hardcoded based on common sentiment indicators\n",
    "        self.patterns = {\n",
    "            'happiness': [\n",
    "                r'\\b(happy|joy|excited|great|awesome|wonderful)\\b',\n",
    "                r'[!]{2,}',  # Multiple exclamation marks\n",
    "                r':\\)|:D|:-\\)',  # Happy emoticons\n",
    "            ],\n",
    "            'worry': [\n",
    "                r'\\b(worry|worried|anxious|stress|stressed)\\b',\n",
    "                r'\\b(problem|issue|trouble)\\b',\n",
    "                r':\\(|:-\\(',  # Sad emoticons\n",
    "            ],\n",
    "            'neutral': [\n",
    "                r'\\b(okay|ok|fine|normal|regular|usual|typical)\\b',\n",
    "                r'\\b(just|only|simply)\\b'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Add the regex patterns from the n-gram analysis\n",
    "        # self.patterns.update(ngram_regex_patterns)\n",
    "        \n",
    "        # Compile regex patterns for efficiency\n",
    "        self.compiled_patterns = {}\n",
    "        for sentiment, pattern_list in self.patterns.items():\n",
    "            self.compiled_patterns[sentiment] = [re.compile(pattern, re.IGNORECASE) for pattern in pattern_list]\n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"\n",
    "        Predict sentiment based on regex pattern matching.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text to classify\n",
    "            \n",
    "        Returns:\n",
    "            str: Predicted sentiment ('happiness', 'worry', or 'neutral')\n",
    "        \"\"\"\n",
    "        # Clean the text\n",
    "        clean_text = text.lower().strip()\n",
    "        \n",
    "        # Count matches for each sentiment\n",
    "        sentiment_scores = {}\n",
    "        \n",
    "        for sentiment, patterns in self.compiled_patterns.items():\n",
    "            score = 0\n",
    "            for pattern in patterns:\n",
    "                matches = pattern.findall(clean_text)\n",
    "                score += len(matches)\n",
    "            sentiment_scores[sentiment] = score\n",
    "        \n",
    "        # Return sentiment with highest score, default to neutral if tie\n",
    "        if max(sentiment_scores.values()) == 0:\n",
    "            return 'neutral'  # Default when no patterns match\n",
    "        \n",
    "        return max(sentiment_scores, key=sentiment_scores.get)\n",
    "    \n",
    "    def predict_batch(self, texts):\n",
    "        \"\"\"\n",
    "        Predict sentiments for a batch of texts.\n",
    "        \n",
    "        Args:\n",
    "            texts (list): List of texts to classify\n",
    "            \n",
    "        Returns:\n",
    "            list: List of predicted sentiments\n",
    "        \"\"\"\n",
    "        return [self.predict(text) for text in texts]\n",
    "    \n",
    "    def explain_prediction(self, text):\n",
    "        \"\"\"\n",
    "        Explain why a particular prediction was made by showing matched patterns.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text to analyze\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary showing matched patterns for each sentiment\n",
    "        \"\"\"\n",
    "        clean_text = text.lower().strip()\n",
    "        explanations = {}\n",
    "        \n",
    "        for sentiment, patterns in self.compiled_patterns.items():\n",
    "            matches = []\n",
    "            for i, pattern in enumerate(patterns):\n",
    "                found_matches = pattern.findall(clean_text)\n",
    "                if found_matches:\n",
    "                    matches.extend([(self.patterns[sentiment][i], found_matches)])\n",
    "            explanations[sentiment] = matches\n",
    "        \n",
    "        return explanations\n",
    "\n",
    "# Create and test the regex classifier\n",
    "regex_classifier = RegexSentimentClassifier()\n",
    "\n",
    "# Test on some examples\n",
    "# Test on the actual test split of the dataset\n",
    "test_texts = test_df['content'].tolist()\n",
    "\n",
    "\n",
    "print(\"=== REGEX CLASSIFIER PREDICTIONS ===\")\n",
    "for text in test_texts:\n",
    "    prediction = regex_classifier.predict(text)\n",
    "    explanation = regex_classifier.explain_prediction(text)\n",
    "    \n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(\"Matched patterns:\")\n",
    "    for sentiment, matches in explanation.items():\n",
    "        if matches:\n",
    "            print(f\"  {sentiment}: {matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentiment labels to match our regex classifier output\n",
    "label_mapping = {0: 'neutral', 1: 'worry', 2: 'happiness'}\n",
    "\n",
    "# Get predictions from regex classifier\n",
    "test_texts = test_df['content'].tolist()\n",
    "regex_predictions = regex_classifier.predict_batch(test_texts)\n",
    "\n",
    "# Convert neural network predictions to string labels for comparison\n",
    "neural_predictions = [label_mapping[pred] for pred in predicted_labels]\n",
    "true_labels_str = [label_mapping[label] for label in true_labels]\n",
    "\n",
    "# Calculate metrics for regex classifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "regex_accuracy = accuracy_score(true_labels_str, regex_predictions)\n",
    "neural_accuracy = accuracy_score(true_labels_str, neural_predictions)\n",
    "\n",
    "print(\"=== CLASSIFIER COMPARISON ===\")\n",
    "print(f\"Regex Classifier Accuracy: {regex_accuracy:.4f}\")\n",
    "print(f\"Neural Network Accuracy: {neural_accuracy:.4f}\")\n",
    "print(f\"Improvement from Neural Network: {neural_accuracy - regex_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n=== REGEX CLASSIFIER DETAILED REPORT ===\")\n",
    "print(classification_report(true_labels_str, regex_predictions))\n",
    "\n",
    "# Show confusion matrix for regex classifier\n",
    "print(\"\\n=== REGEX CLASSIFIER CONFUSION MATRIX ===\")\n",
    "plot_confusion_matrix(\n",
    "    [{'neutral': 0, 'worry': 1, 'happiness': 2}[label] for label in true_labels_str],\n",
    "    [{'neutral': 0, 'worry': 1, 'happiness': 2}[label] for label in regex_predictions],\n",
    "    classes=['neutral', 'worry', 'happiness']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_classifiers(texts, true_labels, neural_preds, regex_preds, n_examples=10):\n",
    "    \"\"\"\n",
    "    Compare predictions from both classifiers on sample texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): Input texts\n",
    "        true_labels (list): True sentiment labels\n",
    "        neural_preds (list): Neural network predictions\n",
    "        regex_preds (list): Regex classifier predictions\n",
    "        n_examples (int): Number of examples to show\n",
    "    \"\"\"\n",
    "    print(\"=== CLASSIFIER COMPARISON ON SAMPLE TEXTS ===\")\n",
    "    print(\"Format: Text | True | Neural | Regex | Match?\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i in range(min(n_examples, len(texts))):\n",
    "        text = texts[i][:60] + \"...\" if len(texts[i]) > 60 else texts[i]\n",
    "        true_label = true_labels[i]\n",
    "        neural_pred = neural_preds[i]\n",
    "        regex_pred = regex_preds[i]\n",
    "        \n",
    "        # Check if predictions match\n",
    "        neural_correct = \"✓\" if neural_pred == true_label else \"✗\"\n",
    "        regex_correct = \"✓\" if regex_pred == true_label else \"✗\"\n",
    "        \n",
    "        print(f\"{text}\")\n",
    "        print(f\"  True: {true_label} | Neural: {neural_pred} {neural_correct} | Regex: {regex_pred} {regex_correct}\")\n",
    "        \n",
    "        # Show regex explanation for interesting cases\n",
    "        if regex_pred != neural_pred:\n",
    "            explanation = regex_classifier.explain_prediction(texts[i])\n",
    "            if any(explanation.values()):\n",
    "                print(f\"  Regex reasoning: {explanation}\")\n",
    "        print()\n",
    "\n",
    "# Compare classifiers on sample texts\n",
    "compare_classifiers(\n",
    "    test_texts[:15], \n",
    "    true_labels_str[:15], \n",
    "    neural_predictions[:15], \n",
    "    regex_predictions[:15]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Learnings\n",
    "\n",
    "This tutorial provided a comprehensive introduction to PyTorch with a focus on natural language processing (NLP). We covered fundamental PyTorch concepts such as tensors, operations, and device management, then applied these concepts to a real-world NLP task: multi-class text classification using a dataset of tweets annotated with various emotions. Here's a brief summary of what we covered:\n",
    "\n",
    "1. **Introduction to PyTorch and Tensors:**\n",
    "   - We explored PyTorch's dynamic computation graph, which allows for flexible and efficient model building, particularly useful in NLP tasks where input data can vary in length and structure.\n",
    "   - Tensors, the core data structure in PyTorch, were introduced along with basic operations such as indexing, slicing, and arithmetic.\n",
    "\n",
    "2. **Data Preparation and Preprocessing:**\n",
    "   - We loaded and explored the tweet dataset, performed one-hot encoding for categorical labels, and implemented basic tokenization.\n",
    "   - We discussed the importance of padding, truncation, and handling different sequence lengths in NLP, ensuring all input data is of uniform size for model processing, and how to do these correctly.\n",
    "\n",
    "3. **Building a Vocabulary and DataLoader:**\n",
    "   - We created a custom Vocabulary class to map tokens to unique indices, including handling special tokens like `<PAD>`, `<UNK>`, `<SOS>`, and `<EOS>`.\n",
    "   - A custom PyTorch Dataset class was implemented to handle tokenized data, padding, and truncation, and a DataLoader was used to efficiently batch and shuffle the data.\n",
    "\n",
    "4. **Model Implementation:**\n",
    "   - We implemented a simple feedforward neural network for text classification, highlighting the importance of embedding layers, masking, and the forward pass.\n",
    "   - The model was trained using a well-defined training loop, with early stopping to prevent overfitting, and checkpoints were saved to preserve the model's state during training.\n",
    "\n",
    "5. **Model Evaluation and Inference:**\n",
    "   - We evaluated the model using a test dataset and calculated common classification metrics like accuracy, precision, recall, and F1 score.\n",
    "   - A confusion matrix was plotted to visualize model performance across different classes.\n",
    "   - We explored model inference, using the trained model to predict sentiments for new text inputs.\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "- **PyTorch Flexibility:** PyTorch's dynamic computational graph and tensor operations make it a powerful tool for developing deep learning models, particularly in the NLP domain, where input data can be highly variable.\n",
    "  \n",
    "- **Data Preprocessing:** Proper data preprocessing, including tokenization, padding, and truncation, is crucial in NLP tasks to ensure that models can effectively learn from and generalize to diverse text data.\n",
    "\n",
    "- **Model Building and Training:** Building a neural network in PyTorch involves careful design of layers, loss functions, and optimizers. Early stopping and checkpointing are essential practices to avoid overfitting and ensure the ability to resume training.\n",
    "\n",
    "- **Evaluation Metrics:** Understanding and applying appropriate evaluation metrics, such as accuracy, precision, recall, and F1 score, is vital for assessing model performance, especially in multi-class classification tasks.\n",
    "\n",
    "- **Model Inference:** Model inference in NLP requires careful consideration of tokenization and the handling of out-of-vocabulary words. The use of embeddings and proper softmax application are key to making accurate predictions.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This tutorial provided a solid foundation for using PyTorch in NLP, covering the entire workflow from data preprocessing to model evaluation and inference. While we used a simple feedforward network for demonstration, more advanced models like RNNs, LSTMs, or Transformers would likely yield better performance for complex NLP tasks. The key takeaway is that PyTorch offers the flexibility and tools necessary to build, train, and evaluate deep learning models effectively, making it an excellent choice for NLP and other machine learning tasks.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
