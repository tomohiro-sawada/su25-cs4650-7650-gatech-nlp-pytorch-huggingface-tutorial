{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ffbb14288fe2c51",
   "metadata": {},
   "source": [
    "# CS 7650 - Natural Language - HW - 1 \n",
    "Georgia Tech, Fall 2024 (Instructor: Kartik Goyal)\n",
    "\n",
    "Welcome to the first full programming assignment for CS 7650! \n",
    "\n",
    "In this assignment, you will be implementing different deep learning models for text classification using the [20 Newsgroups](http://qwone.com/~jason/20Newsgroups/). It is esentially classifying news articles into different topics. This assignment will start with with data preprocessing techniques, implementing a baseline, and building up from there to more advanced model. It will cover basics of attention mechanism, something very central to modern NLP systems, and present you an opportunity to analyse different aspects of your training model.\n",
    "\n",
    "This assignment will help you diver deeper into the world of Neural Networks and how to implement them for one application in Natural Language Processing. You are expected to have a good understanding of NumPy and PyTorch before starting this assignment.\n",
    "\n",
    "- NumPy Quickstarter Guide: https://numpy.org/doc/stable/user/quickstart.html\n",
    "- A good tutorial on PyTorch: https://www.youtube.com/watch?v=OIenNRt2bjg\n",
    "- Detailed Documentation of PyTorch: https://pytorch.org/docs/stable/index.html\n",
    "- Lecture Material on PyTorch and HuggingFace: https://github.com/neelabhsinha/cs7650-gatech-nlp-pytorch-huggingface-tutorial\n",
    "\n",
    "DO NOT CHANGE the names of any of the files and contents outside the cells where you have to write code.\n",
    "\n",
    "NOTE: DO NOT USE ANY OTHER EXTERNAL LIBRARIES FOR THIS ASSIGNMENT\n",
    "\n",
    "<font color='red'> DEADLINE: September 30, 2024, 11:59 PM  </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SiFjTT4jFEIW",
   "metadata": {
    "id": "SiFjTT4jFEIW"
   },
   "source": [
    "The assignment is broken down into 6 Sections. The sections are as follows:\n",
    "\n",
    "| Section | Part                                      | Points |\n",
    "|---------|-------------------------------------------|--------|\n",
    "| 1       | Loading and Preprocessing Data            | 7      |\n",
    "| 2       | Neural Bag of Words (NBOW)                | 3      |\n",
    "| 3       | Model Training (utilities for all models) | 15     |  \n",
    "| 4       | Deep Averaging Networks (DANs)            | 8      |\n",
    "| 5       | Attention-based Models                    | 30     |\n",
    "| 6       | Perceptron and Hinge Losses               | 16     |\n",
    "| 7       | Analysis                                  | 21     |\n",
    "| 8       | Bonus: Improving Attention Models         | 10     |\n",
    "| -       | Total                                     | 100 + 10 = 110     |\n",
    "\n",
    "\n",
    "All the best and happy coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e26764d",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Check what version of Python is running\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f28858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute only if you are working in Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3b94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d87790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path = '/content/drive/My Drive/path/to/folder/HW0'\n",
    "# the above is what folder path should look like the folder path if you execute in colab\n",
    "folder_path = '.'\n",
    "\n",
    "# Files in the folder -\n",
    "os.listdir(folder_path)\n",
    "os.chdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32e55cf463e82c55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T23:33:10.407318Z",
     "start_time": "2024-09-05T23:33:05.512054Z"
    }
   },
   "outputs": [],
   "source": [
    "# export\n",
    "# Importing required libraries\n",
    "# Do not change the libraries already imported or import additional libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import html\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, RandomSampler, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf8106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# SOME UTILITY FUNCTIONS - DO NOT CHANGE\n",
    "def save_checkpoint(model, model_name, loss_fn='ce'):\n",
    "    file_path = os.path.join(os.getcwd(), 'model_weights', f'checkpoint_{model_name}_{loss_fn}.pt')\n",
    "    os.makedirs(os.path.join(os.getcwd(), 'model_weights'), exist_ok=True)\n",
    "    checkpoint = { # create a dictionary with all the state information\n",
    "        'model_state_dict': model.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, file_path)\n",
    "    print(f\"Checkpoint saved to {file_path}\")\n",
    "\n",
    "def load_checkpoint(model, model_name, loss_fn='ce', map_location='cpu'):\n",
    "    file_path = os.path.join(os.getcwd(), 'model_weights', f'checkpoint_{model_name}_{loss_fn}.pt')\n",
    "    checkpoint = torch.load(file_path, map_location=map_location) # load the checkpoint, ensure correct device\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c24e5a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOME UTILITY FUNCTIONS - DO NOT CHANGE\n",
    "def plot_loss(train_loss_over_time, val_loss_over_time, model_name):\n",
    "    epochs = range(1, len(train_loss_over_time) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_loss_over_time, color='red', label='Train Loss')\n",
    "    plt.plot(epochs, val_loss_over_time, color='blue', label='Val Loss')\n",
    "    \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training and Validation Loss for {model_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e52d8b77a802b9b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T23:33:10.448870Z",
     "start_time": "2024-09-05T23:33:10.417558Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# Defining global constants - DO NOT CHANGE THESE VALUES\n",
    "RANDOM_SEED = 42\n",
    "PADDING_VALUE = 0\n",
    "UNK_VALUE     = 1\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else('mps' if torch.backends.mps.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb7e17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T23:33:13.222334Z",
     "start_time": "2024-09-05T23:33:13.219647Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is how we select a GPU if it's available on your computer or in the Colab environment.\n",
    "print('Device of execution - ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83360462",
   "metadata": {},
   "source": [
    "## 1. Preprocessing [7 points - Programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0Fulh0MZ8y8b",
   "metadata": {
    "id": "0Fulh0MZ8y8b"
   },
   "source": [
    "### 1.1. Data Cleaning Methods [0 points]\n",
    "The following cell defines some methods to clean the dataset. Do not edit it, but feel free to take a look at some of the operations it's doing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ctNnE1Ui8oKw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T23:33:16.511579Z",
     "start_time": "2024-09-05T23:33:16.505921Z"
    },
    "id": "ctNnE1Ui8oKw"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# example code taken from fast-bert\n",
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "def spec_add_spaces(t: str) -> str:\n",
    "    \"Add spaces around / and # in `t`. \\n\"\n",
    "    return re.sub(r\"([/#\\n])\", r\" \\1 \", t)\n",
    "\n",
    "def rm_useless_spaces(t: str) -> str:\n",
    "    \"Remove multiple spaces in `t`.\"\n",
    "    return re.sub(\" {2,}\", \" \", t)\n",
    "\n",
    "def replace_multi_newline(t: str) -> str:\n",
    "    return re.sub(r\"(\\n(\\s)*){2,}\", \"\\n\", t)\n",
    "\n",
    "def fix_html(x: str) -> str:\n",
    "    \"List of replacements from html strings in `x`.\"\n",
    "    re1 = re.compile(r\"  +\")\n",
    "    x = (\n",
    "        x.replace(\"#39;\", \"'\")\n",
    "        .replace(\"amp;\", \"&\")\n",
    "        .replace(\"#146;\", \"'\")\n",
    "        .replace(\"nbsp;\", \" \")\n",
    "        .replace(\"#36;\", \"$\")\n",
    "        .replace(\"\\\\n\", \"\\n\")\n",
    "        .replace(\"quot;\", \"'\")\n",
    "        .replace(\"<br />\", \"\\n\")\n",
    "        .replace('\\\\\"', '\"')\n",
    "        .replace(\" @.@ \", \".\")\n",
    "        .replace(\" @-@ \", \"-\")\n",
    "        .replace(\" @,@ \", \",\")\n",
    "        .replace(\"\\\\\", \" \\\\ \")\n",
    "    )\n",
    "    return re1.sub(\" \", html.unescape(x))\n",
    "\n",
    "def clean_text(input_text):\n",
    "    text = fix_html(input_text)\n",
    "    text = replace_multi_newline(text)\n",
    "    text = spec_add_spaces(text)\n",
    "    text = rm_useless_spaces(text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MiUlTSBB9Wx6",
   "metadata": {
    "id": "MiUlTSBB9Wx6"
   },
   "source": [
    "### 1.2. Data Cleaning and Tokenizing [0 points]\n",
    "\n",
    "Clean the data using the methods above and tokenize it using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vqtdrhF8FEIZ",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:39:54.123042Z",
     "start_time": "2024-09-06T14:39:33.007172Z"
    },
    "id": "vqtdrhF8FEIZ"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Downloading the NLTK tokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "# Tokenizing the text\n",
    "df = pd.read_csv(\"vocab.csv\")\n",
    "df[\"tokenized\"] = df[\"data\"].apply(lambda x: nltk.word_tokenize(clean_text(x.lower())))\n",
    "func = lambda x: int(x) if x.isdigit() else x\n",
    "df['target'] = df['target'].apply(func)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0b94263",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T23:33:43.837726Z",
     "start_time": "2024-09-05T23:33:43.827897Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE VALUES\n",
    "# Divide the dataset into training and validation sets\n",
    "# The following two lines are used to load the indices of the training and validation sets\n",
    "train_indices = np.load(\"train_indices.npy\")\n",
    "val_indices = np.load(\"valid_indices.npy\")\n",
    "# The following two lines are used to select the training and validation sets from the dataframe based on the indices loaded above\n",
    "train_data = df.iloc[train_indices].reset_index(drop=True)\n",
    "val_data = df.iloc[val_indices].reset_index(drop=True)\n",
    "func = lambda x: int(x) if str(x).isdigit() else x\n",
    "val_data['target'] = val_data['target'].apply(func)\n",
    "val_data = val_data.iloc[1:, :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qBBdVOYxFEIa",
   "metadata": {
    "id": "qBBdVOYxFEIa"
   },
   "source": [
    "Here's what the dataset looks like. You can index into specific rows with pandas, and try to guess some of these yourself :). If you're unfamiliar with pandas, it's a extremely useful and popular library for data analysis and manipulation. You can find their documentation [here](https://pandas.pydata.org/docs/).\n",
    "\n",
    "Pandas primary data structure is a DataFrame. The following cell will print out the basic information of this structure, including the labeled axes (both columns and rows) as well as show you what the first n (default=5) rows look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sJjScqV3FEIb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T23:33:47.458775Z",
     "start_time": "2024-09-05T23:33:47.448945Z"
    },
    "id": "sJjScqV3FEIb"
   },
   "outputs": [],
   "source": [
    "# Print training and validation set heads\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532ebf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "id2label = dict(zip(df['target'], df['target_names']))\n",
    "id2label = {k: id2label[k] for k in id2label if isinstance(k, int)}\n",
    "id2label = {k: id2label[k] for k in sorted(id2label)}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f698ac9f",
   "metadata": {},
   "source": [
    "This is a dictionary which maps ids to label names. It will be handy in the later part of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TQVT6HUA9htQ",
   "metadata": {
    "id": "TQVT6HUA9htQ"
   },
   "source": [
    "### 1.3. Vocabulary Building [2 points - Programming]\n",
    "\n",
    "Generate a vocabulary map for all the words in your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GDI72x8XFEIc",
   "metadata": {
    "id": "GDI72x8XFEIc"
   },
   "source": [
    "Now that we've loaded this dataset, we need to create a vocab map for words, which will map tokens to numbers. This will be useful later, since torch PyTorch use tensors of sequences of numbers as inputs. **Go to the following cell, and fill out generate_vocab_map.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "zeo9kX6i9pbH",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T23:33:53.394479Z",
     "start_time": "2024-09-05T23:33:53.388382Z"
    },
    "id": "zeo9kX6i9pbH"
   },
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def generate_vocab_map(df, cutoff=2):\n",
    "    \"\"\"\n",
    "    This method takes a dataframe and builds a vocabulary to unique number map.\n",
    "    It uses the cutoff argument to remove rare words occurring <= cutoff times.\n",
    "    \"\" and \"UNK\" are reserved tokens in our vocab that will be useful later. \n",
    "    You'll also find the Counter imported for you to be useful as well.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame) : The entire dataset this mapping is built from\n",
    "        cutoff (int) : We exclude words from the vocab that appear less than or equal to cutoff\n",
    "             \n",
    "    Returns:\n",
    "        vocab (dict[str] = int) : In vocab, each str is a unique token, and each dict[str] is a\n",
    "            unique integer ID. Only elements that appear > cutoff times appear in vocab. \n",
    "        reversed_vocab (dict[int] = str) : A reversed version of vocab, which allows us to retrieve\n",
    "            words given their unique integer ID. This map will allow us to \"decode\" integer \n",
    "            sequences we'll encode using vocab!\n",
    "    \"\"\"\n",
    "    vocab          = {\"\": PADDING_VALUE, \"UNK\": UNK_VALUE}\n",
    "    reversed_vocab = None\n",
    "\n",
    "    ## YOUR CODE STARTS HERE ##\n",
    "    # hint: start by iterating over df[\"tokenized\"]\n",
    "    \n",
    "    # Make a dictionary of words and their counts\n",
    "    word_counts = Counter([word for row in df[\"tokenized\"].to_list() for word in row])\n",
    "    \n",
    "    # Start iterator for unique word IDs\n",
    "    WORD_IDX = max(vocab.values()) + 1\n",
    "    \n",
    "    # Loop through the dictionary\n",
    "    for word, count in word_counts.items():\n",
    "        \n",
    "        # If the word appears more than cutoff times, add it to the vocab\n",
    "        if count > cutoff:\n",
    "            vocab[word] = WORD_IDX\n",
    "            WORD_IDX += 1\n",
    "        \n",
    "    # Create the reversed vocab\n",
    "    reversed_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    ## YOUR CODE ENDS HERE ##\n",
    "\n",
    "    return vocab, reversed_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w9LEk83hRFgT",
   "metadata": {
    "id": "w9LEk83hRFgT"
   },
   "source": [
    "With the methods you have implemented above, you can now generate your dictionaries mapping from word tokens to IDs (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rcmX931OFEId",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T23:34:00.216100Z",
     "start_time": "2024-09-05T23:33:59.772028Z"
    },
    "id": "rcmX931OFEId"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "train_vocab, reverse_vocab = generate_vocab_map(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57e50b95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T23:34:01.828924Z",
     "start_time": "2024-09-05T23:34:01.827202Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check Vocabulary Size - DO NOT CHANGE THIS VALUE\n",
    "assert len(train_vocab) == 60233, f\"Vocabulary is of incorrect size: {len(train_vocab)}\"\n",
    "\n",
    "# No error means you've passed the test!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fCFfEHv1hnI",
   "metadata": {
    "id": "5fCFfEHv1hnI"
   },
   "source": [
    "### 1.4. Building a Dataset Class [2 points - Programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8-qTQQa2FEIe",
   "metadata": {
    "id": "8-qTQQa2FEIe"
   },
   "source": [
    "PyTorch has custom Dataset Classes that have very useful extensions, we want to turn our current pandas DataFrame into a subclass of Dataset so that we can iterate and sample through it for minibatch updates. **In the following cell, fill out the HeadlineDataset class.** Refer to PyTorch documentation on [Dataset Classes](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tqt9q92J1QKK",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:44:02.560883Z",
     "start_time": "2024-09-06T13:44:02.556557Z"
    },
    "id": "tqt9q92J1QKK"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class HeadlineDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This class takes a Pandas DataFrame and wraps in a Torch Dataset.\n",
    "    Read more about Torch Datasets here:\n",
    "    https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab, df, max_length=200):\n",
    "        \"\"\"\n",
    "        Initialize the class with appropriate instance variables. In this method, we \n",
    "        STRONGLY recommend storing the dataframe itself as an instance variable, and \n",
    "        keeping this method very simple. Leave processing to __getitem__.\n",
    "        \n",
    "        Args:\n",
    "            vocab (dict[str] = int) : In vocab, each str is a unique token, and each dict[str] is a\n",
    "                unique integer ID. Only elements that appear > cutoff times appear in vocab. \n",
    "            df (pandas.DataFrame) : The entire dataset this mapping is built from\n",
    "            max_length (int) : The max length of a headline we'll allow in our dataset.\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        ## YOUR CODE STARTS HERE - initialize parameters ##\n",
    "\n",
    "        self.vocab, self.df, self.max_length = vocab, df, max_length\n",
    "        \n",
    "        ## YOUR CODE ENDS HERE ##\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        This method returns the length of the underlying dataframe,\n",
    "        Args:\n",
    "            None\n",
    "        Returns:\n",
    "            df_len (int) : The length of the underlying dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        df_len = None\n",
    "        \n",
    "        ## YOUR CODE STARTS HERE ##\n",
    "        \n",
    "        df_len = len(self.df)\n",
    "\n",
    "        ## YOUR CODE ENDS HERE ##\n",
    "        \n",
    "        return df_len\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        This method converts a dataframe row (row[\"tokenized\"]) to an encoded torch LongTensor,\n",
    "        using our vocab map created using generate_vocab_map. Restricts the encoded headline\n",
    "        length to max_length.\n",
    "        \n",
    "        The purpose of this method is to convert the row - a list of words - into a corresponding\n",
    "        list of numbers.\n",
    "        \n",
    "        i.e. using a map of {\"hi\": 2, \"hello\": 3, \"UNK\": 0}\n",
    "        this list [\"hi\", \"hello\", \"NOT_IN_DICT\"] will turn into [2, 3, 0]\n",
    "        \n",
    "        Args:\n",
    "            index (int) : The index of the dataframe we want to retrieve.\n",
    "            \n",
    "        Returns:\n",
    "            tokenized_word_tensor (torch.LongTensor) : A 1D tensor of type Long, that has each\n",
    "                token in the dataframe mapped to a number. These numbers are retrieved from the\n",
    "                vocab_map we created in generate_vocab_map.\n",
    "                \n",
    "                IMPORTANT: If we filtered out the word because it's infrequent (and it doesn't\n",
    "                exist in the vocab) we need to replace it w/ the UNK token\n",
    "                \n",
    "            curr_label (int) : Label index of the class between 0 to len(num_classes) - 1 representing which \n",
    "            class label does this data instance belong to\n",
    "        \"\"\"\n",
    "        \n",
    "        tokenized_word_tensor = None\n",
    "        curr_label            = None\n",
    "        \n",
    "        ## YOUR CODE STARTS HERE ##\n",
    "        \n",
    "        # Get the row from the dataframe\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        # Get the label\n",
    "        curr_label = row[\"target\"]\n",
    "        \n",
    "        # Get the tokenized headline\n",
    "        tokenized_headline = row[\"tokenized\"]\n",
    "        \n",
    "        # Map the tokens to their IDs\n",
    "        tokenized_word_tensor = torch.tensor([self.vocab.get(word, UNK_VALUE) for word in tokenized_headline], dtype=torch.long)\n",
    "        \n",
    "        # Truncate the headline if it's too long\n",
    "        if len(tokenized_word_tensor) > self.max_length:\n",
    "            tokenized_word_tensor = tokenized_word_tensor[:self.max_length]\n",
    "\n",
    "        ## YOUR CODE ENDS HERE ##\n",
    "        \n",
    "        return tokenized_word_tensor, curr_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "KuLtIOAZFEIe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:44:06.793866Z",
     "start_time": "2024-09-06T13:44:06.790572Z"
    },
    "id": "KuLtIOAZFEIe"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "train_dataset = HeadlineDataset(train_vocab, train_data)\n",
    "val_dataset   = HeadlineDataset(train_vocab, val_data)\n",
    "\n",
    "# Now that we're wrapping our dataframes in PyTorch datsets, we can make use of PyTorch Random Samplers, they'll define how our DataLoaders sample elements from the HeadlineDatasets\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "val_sampler   = RandomSampler(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4aa6f12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:44:07.819048Z",
     "start_time": "2024-09-06T13:44:07.816085Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check Dataset Lengths - DO NOT CHANGE THESE VALUES\n",
    "assert len(train_dataset) == 15076, f\"Training Dataset is of incorrect size {len(train_dataset)}\"\n",
    "assert len(val_dataset)   ==  1885, f\"Validation Dataset is of incorrect size {len(val_dataset)}\"\n",
    "\n",
    "# No error means you've passed the test!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n9iBiSKF1yXA",
   "metadata": {
    "id": "n9iBiSKF1yXA"
   },
   "source": [
    "### 1.5. Finalizing our DataLoader [3 points - Programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lfXSbxoFFEIe",
   "metadata": {
    "id": "lfXSbxoFFEIe"
   },
   "source": [
    "We can now use PyTorch DataLoaders to batch our data for us. **In the following cell fill out collate_fn.** Refer to PyTorch documentation on [DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for help. Apply padding and other post-processing required here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "Zp1aQAvn1_mz",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:44:09.644199Z",
     "start_time": "2024-09-06T13:44:09.639812Z"
    },
    "id": "Zp1aQAvn1_mz"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def collate_fn(batch, padding_value=PADDING_VALUE):\n",
    "    \"\"\"\n",
    "    This function is passed as a parameter to Torch DataSampler. collate_fn collects\n",
    "    batched rows, in the form of tuples, from a DataLoader and applies some final\n",
    "    pre-processing.\n",
    "    \n",
    "    Objective:\n",
    "    In our case, we need to take the batched input array of 1D tokenized_word_tensors,\n",
    "    and create a 2D tensor that's padded to be the max length from all our tokenized_word_tensors\n",
    "    in a batch. We're moving from a Python array of tuples, to a padded 2D tensor.\n",
    "    \n",
    "    *HINT*: you're allowed to use torch.nn.utils.rnn.pad_sequence (ALREADY IMPORTED)\n",
    "    \n",
    "    Finally, you can read more about collate_fn here: https://pytorch.org/docs/stable/data.html\n",
    "    \n",
    "    :param batch: PythonArray[tuple(tokenized_word_tensor: 1D Torch.LongTensor, curr_label: int)] of length BATCH_SIZE\n",
    "    :param padding_value: int\n",
    "    \n",
    "    :return padded_tokens: 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
    "    :return y_labels: 1D FloatTensor of shape (BATCH_SIZE)\n",
    "    \"\"\"\n",
    "    \n",
    "    padded_tokens, y_labels = None, None\n",
    "    \n",
    "    ## YOUR CODE STARTS HERE - take the input and target from batch, pad the tokens, convert batches to tensor ##\n",
    "    \n",
    "    # Get the tokenized word tensors and labels\n",
    "    tokenized_word_tensors, labels = zip(*batch)\n",
    "    \n",
    "    # Pad the tokenized word tensors\n",
    "    padded_tokens = pad_sequence(tokenized_word_tensors, batch_first=True, padding_value=padding_value)\n",
    "    \n",
    "    # Convert the labels to a tensor\n",
    "    y_labels = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "    ## YOUR CODE ENDS HERE ##\n",
    "    \n",
    "    return padded_tokens, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "OayoJRTeFEIf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:44:12.546122Z",
     "start_time": "2024-09-06T13:44:12.542206Z"
    },
    "id": "OayoJRTeFEIf"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
    "val_iterator   = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pidbg12AFEIf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:44:13.424618Z",
     "start_time": "2024-09-06T13:44:13.409237Z"
    },
    "id": "pidbg12AFEIf"
   },
   "outputs": [],
   "source": [
    "# Use this to test your collate_fn implementation.\n",
    "# You can look at the shapes of x and y or put print statements in collate_fn while running this snippet\n",
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "for x, y in train_iterator:\n",
    "    print(f'x: {x.shape}')\n",
    "    print(f'y: {y.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BWLK7T1uFEIg",
   "metadata": {
    "id": "BWLK7T1uFEIg"
   },
   "source": [
    "## 2. Neural Bag of Words (NBOW) [3 pts - Programming]\n",
    "Let's move to modeling, now that we have dataset iterators that batch our data for us. The first model is a simple model called NBOW-RAND.\n",
    "\n",
    "In the following code block, you'll build a feed-forward neural network implementing a neural bag-of-words baseline, NBOW-RAND, described in section 2.1 of [this paper](https://www.aclweb.org/anthology/P15-1162.pdf). You'll find [this](https://pytorch.org/docs/stable/nn.html) page useful for understanding the different layers and [this](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) page useful for how to put them into action.\n",
    "\n",
    "The core idea behind this baseline is that after we embed each word for a document, we average the embeddings to produce a single vector that hopefully captures some general information spread across the sequence of embeddings. This means we first turn each document of length *n* into a matrix of *nxd*, where *d* is the dimension of the embedding. Then we average this matrix to produce a vector of length *d*, summarizing the contents of the document and proceed with the rest of the network.\n",
    "\n",
    "While you're working through this implementation, keep in mind how the dimensions change and what each axes represents, as documents will be passed in as minibatches requiring careful selection of which axes you apply certain operations too. Stick to only the architecture described in the instructions below, do not add additional layers, this will impact the validity of local checks.\n",
    "\n",
    "Refer to the following equation on how to define NBOW -\n",
    "\n",
    "\n",
    "$$ h_{avg} = \\frac{1}{n} \\sum_t emb(x_t) $$\n",
    "\n",
    "The probability of a data instance belonging to class $y_i$ is given by:\n",
    "\n",
    "$$ p(y|x) = softmax(w^T h_{avg}) $$\n",
    "\n",
    "where $w \\in R^d$ is a parameter vector.\n",
    "\n",
    "*HINT*: In the forward step, the BATCH_SIZE is the first dimension.\n",
    "\n",
    "*Hint*: Make sure to handle the case where the input contains pad tokens. We don't want to consider them in our average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pZDPs0Sf-H3V",
   "metadata": {
    "id": "pZDPs0Sf-H3V"
   },
   "source": [
    "#### 2.1. Define the NBOW model class [3 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "jzGx2q0jLqyU",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:44:18.987942Z",
     "start_time": "2024-09-06T13:44:18.980032Z"
    },
    "id": "jzGx2q0jLqyU"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class NBOW(nn.Module):\n",
    "    # Instantiate layers for your model-\n",
    "    #\n",
    "    # Your model architecture will be a feed-forward neural network.\n",
    "    #\n",
    "    # You'll need 3 nn.Modules:\n",
    "    # 1. An embeddings layer (see nn.Embedding)\n",
    "    # 2. A linear layer (see nn.Linear)\n",
    "    #\n",
    "    # HINT: In the forward step, the BATCH_SIZE is the first dimension.\n",
    "    #\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes=20):\n",
    "        super().__init__()\n",
    "        ## YOUR CODE STARTS HERE ##\n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.linear = nn.Linear(in_features=embedding_dim, out_features=num_classes, bias=False)\n",
    "        ## YOUR CODE ENDS HERE ##\n",
    "\n",
    "    # Complete the forward pass of the model.\n",
    "    #\n",
    "    # Use the output of the embedding layer to create\n",
    "    # the average vector, which will be input into the\n",
    "    # linear layer.\n",
    "    #\n",
    "    # args:\n",
    "    # x - 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
    "    #     This is the same output that comes out of the collate_fn function you completed\n",
    "    def forward(self, x):\n",
    "        ## Hint: Make sure to handle the case where x contains pad tokens. We don't want to consider them in our average.\n",
    "        ## YOUR CODE STARTS HERE ##\n",
    "        h_avg = self.get_h_avg(x)\n",
    "        x = self.linear(h_avg)\n",
    "        return x\n",
    "        ## YOUR CODE ENDS HERE ##\n",
    "    \n",
    "    def get_embeddings(self, x):\n",
    "        '''\n",
    "        This function returns the embeddings of the input x\n",
    "        '''\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        embed = self.embed(x)\n",
    "        return embed\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "    \n",
    "    def set_embedding_weight(self, weight):\n",
    "        '''\n",
    "        This function sets the embedding weights to the input weight. Ensure you aren't recording gradients for this.\n",
    "        Hint: Refer to nn.Parameter to do this.\n",
    "        Args:\n",
    "            weight: torch.tensor of shape (vocab_size, embedding_dim)\n",
    "        '''\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        with torch.no_grad():\n",
    "            self.embed.weight = nn.Parameter(weight)\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "    def get_h_avg(self, x):\n",
    "        '''\n",
    "        This function returns the average of the embeddings of the input x\n",
    "        '''\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        embedded = self.get_embeddings(x)\n",
    "        embedded = embedded * (x != 0).unsqueeze(-1).float()\n",
    "        h_avg = torch.sum(embedded, dim=1) / torch.sum((x!=0), dim=-1, keepdim=True)\n",
    "        \n",
    "        return h_avg\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6141a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:44:22.181835Z",
     "start_time": "2024-09-06T13:44:22.172804Z"
    }
   },
   "outputs": [],
   "source": [
    "# local test for sanity:\n",
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "def nbow_test_local():\n",
    "    embedding_dim = 10\n",
    "    vocab_size = 5\n",
    "    model = NBOW(vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
    "    for _, module in model.named_parameters():\n",
    "        if hasattr(module, \"data\"):\n",
    "            nn.init.constant_(module, 0.1)\n",
    "    input = torch.arange(12).reshape(2,6) % vocab_size\n",
    "    expected_result = torch.tensor(\n",
    "        [[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
    "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
    "         0.1000, 0.1000],\n",
    "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
    "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
    "         0.1000, 0.1000]]\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        local_result = model(input)\n",
    "    if torch.allclose(expected_result, local_result, rtol=0.001):\n",
    "        print(\"Passed local check\")\n",
    "    else:\n",
    "        print(f\"Test failed, expected value was\\n{expected_result}\\nbut you got:\\n{local_result}\")\n",
    "\n",
    "def nbow_test_local_embeddings():\n",
    "    embedding_dim = 3\n",
    "    vocab_size = 5\n",
    "    model = NBOW(embedding_dim=embedding_dim, vocab_size=vocab_size)\n",
    "    model.set_embedding_weight(torch.arange(15).reshape(5,3) / 50)\n",
    "    embeddings = model.get_embeddings(torch.tensor([[1,2,3,4,1],[1,2,3,0,0]]))\n",
    "    correct_embeddings = torch.tensor([[[0.0600, 0.0800, 0.1000],\n",
    "                                        [0.1200, 0.1400, 0.1600],\n",
    "                                        [0.1800, 0.2000, 0.2200],\n",
    "                                        [0.2400, 0.2600, 0.2800],\n",
    "                                        [0.0600, 0.0800, 0.1000]],\n",
    "\n",
    "                                        [[0.0600, 0.0800, 0.1000],\n",
    "                                        [0.1200, 0.1400, 0.1600],\n",
    "                                        [0.1800, 0.2000, 0.2200],\n",
    "                                        [0.0000, 0.0200, 0.0400],\n",
    "                                        [0.0000, 0.0200, 0.0400]]])\n",
    "    if torch.allclose(embeddings, correct_embeddings, rtol=0.001):\n",
    "        print(\"Passed local embedding test\")\n",
    "    else:\n",
    "        print(f\"Embedding Test failed, expected value was\\n{correct_embeddings}\\nbut you got:\\n{embeddings}\")\n",
    "def nbow_test_local_h_avg():\n",
    "    embedding_dim = 3\n",
    "    vocab_size = 5\n",
    "    model = NBOW(embedding_dim=embedding_dim, vocab_size=vocab_size)\n",
    "    model.set_embedding_weight(torch.arange(15).reshape(5,3) / 50)\n",
    "    h_avg = model.get_h_avg(torch.tensor([[1,2,3,4,1],[1,2,3,0,0]]))\n",
    "    correct_h_avg = torch.tensor([[0.1320, 0.1520, 0.1720],\n",
    "                                  [0.1200, 0.1400, 0.1600]])\n",
    "    if torch.allclose(h_avg, correct_h_avg, rtol=0.001):\n",
    "        print(\"Passed local h_avg test\")\n",
    "    else:\n",
    "        print(f\"h_avg Test failed, expected value was\\n{correct_h_avg}\\nbut you got:\\n{h_avg}\")\n",
    "nbow_test_local()\n",
    "nbow_test_local_embeddings()\n",
    "nbow_test_local_h_avg()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d61c4",
   "metadata": {},
   "source": [
    "## 3. Model Training [12 points - Programming + 3 points - Non-programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad2a8d9",
   "metadata": {},
   "source": [
    "Training a PyTorch model involves several key components:\n",
    "\n",
    "- **Training Loop**: This is the process where the model learns from the training data. In each iteration, the model processes the input data, makes predictions, calculates the loss, and updates its weights using backpropagation.\n",
    "- **Validation Loop**: Performed after the training loop, this evaluates the model on a separate dataset (validation data) to check its performance. It helps in detecting overfitting.\n",
    "- **Optimizer**: This is an algorithm that updates the model's weights during training. Common optimizers include SGD, Adam, etc.\n",
    "- **Criterion (Loss Function)**: This measures how well the model is performing. It calculates the difference between the model's predictions and the actual data. Common loss functions include Mean Squared Error for regression tasks and Cross Entropy Loss for classification.\n",
    "\n",
    "During training, the optimizer uses the gradient of the loss function to adjust the model's parameters. The model's performance is evaluated periodically on the validation set to monitor its generalization capability. This process continues for a specified number of epochs or until the model achieves a desired level of accuracy.\n",
    "\n",
    "*Note - In the following code cells (of this section), the above components will be defined. These functions/objects will be used to train and evaluate all your models in this assignment. So, make sure to implement these in a generic way, so that they can be used for all the models.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d6e836",
   "metadata": {},
   "source": [
    "### 3.0. Evaluation Metrics [0 points]\n",
    "\n",
    "Accuracy is a measure used to evaluate classification models, representing the ratio of correctly predicted observations to the total observations. It's simple and intuitive but may not be suitable for imbalanced datasets, as it can be misleading if the class distribution is skewed.\n",
    "\n",
    "The F1-score, on the other hand, combines precision and recall into a single number. It is particularly useful when dealing with imbalanced datasets or when the cost of false positives and false negatives varies. F1-score provides a better measure of the incorrectly classified cases than the Accuracy Metric. It's calculated as the harmonic mean of precision and recall, thus balancing the two aspects of model performance.\n",
    "\n",
    "You can read about the terms mentioned above here: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "For this assignment, we are already defining the above metrics for you to use in your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "542f53a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:44:26.127388Z",
     "start_time": "2024-09-06T13:44:26.123867Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# DO NOT CHANGE THIS CELL\n",
    "def get_accuracy_and_f1_score(y_true, y_predicted):\n",
    "    \"\"\"\n",
    "    This function takes in two numpy arrays and computes the accuracy and F1 score\n",
    "    between them. You can use the imported sklearn functions to do this.\n",
    "    \n",
    "    Args:\n",
    "        y_true (list) : A 1D numpy array of ground truth labels\n",
    "        y_predicted (list) : A 1D numpy array of predicted labels\n",
    "        \n",
    "    Returns:\n",
    "        accuracy (float) : The accuracy of the predictions\n",
    "        f1_score (float) : The F1 score of the predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the accuracy\n",
    "    accuracy = accuracy_score(y_true, y_predicted)\n",
    "    \n",
    "    # Get the F1 score\n",
    "    f1 = f1_score(y_true, y_predicted, average='macro')\n",
    "    \n",
    "    return accuracy, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd6b6170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(len(classes)))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01467d21",
   "metadata": {},
   "source": [
    "### 3.1. Criterion [2 points - Programming]\n",
    "\n",
    "Criterion in PyTorch, refers to the loss function used to evaluate the model's performance. It quantifies how far off the model's predictions are from the actual target values\n",
    "\n",
    "In PyTorch, **nn.CrossEntropyLoss()** is used for classification tasks. It first does a softmax on the scores, and then calculates the negative log likelihood. In the cell below, implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de7cd432",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:35:55.706533Z",
     "start_time": "2024-09-06T13:35:55.702832Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def get_criterion(loss_type='ce'):\n",
    "    criterion = None\n",
    "    \n",
    "    ## YOUR CODE STARTS HERE ##\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    ## YOUR CODE ENDS HERE ##\n",
    "    \n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d02cdd",
   "metadata": {},
   "source": [
    "### 3.2. Optimizer [2 points - Programming]\n",
    "\n",
    "In PyTorch, an optimizer is a tool that updates the weights of the neural network to minimize the loss. Among these, Adam (Adaptive Moment Estimation) is a widely-used optimizer. Adam combines the best properties of the AdaGrad and RMSProp algorithms to handle sparse gradients on noisy problems. It's known for its effectiveness in deep learning models, especially where large datasets and high-dimensional spaces are involved. Adam adjusts the learning rate during training, making it efficient and effective across a wide range of tasks and model architectures.\n",
    "\n",
    "In the cell below, define your optimizer. We recommend using Adam, but you are free to experiment with other optimizers as well.\n",
    "\n",
    "The following function takes a model and learning rate value as input, and defines an optimizer for that model's parameters with that learning rate.\n",
    "\n",
    "*HINT: model.parameters() can give you all the parameters of a PyTorch model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a19ba82c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:35:59.120826Z",
     "start_time": "2024-09-06T13:35:59.117307Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_optimizer(model, learning_rate):\n",
    "    \"\"\"\n",
    "    This function takes a model and a learning rate, and returns an optimizer.\n",
    "    Feel free to experiment with different optimizers.\n",
    "    \"\"\"\n",
    "    optimizer = None\n",
    "    \n",
    "    ## YOUR CODE STARTS HERE ##\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    ## YOUR CODE ENDS HERE ##\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05074dc0",
   "metadata": {},
   "source": [
    "### 3.3. Training Loop [3 points - Programming]\n",
    "\n",
    "The training loop function in PyTorch is a critical component where the actual learning from data occurs. It typically involves iterating over the training dataset, feeding the data to the model, computing the loss (difference between the predictions and true values), and updating the model's weights.\n",
    "\n",
    "Creating a training loop involves several steps:\n",
    "\n",
    "1. Iterate Over Dataset: Loop through the training dataset, often in mini-batches.\n",
    "2. Forward Pass: Feed the input data to the model to get predictions.\n",
    "3. Compute Loss: Calculate the loss using a loss function.\n",
    "4. Backward Pass: Perform backpropagation by calling loss.backward(), which computes the gradient of the loss function with respect to each weight.\n",
    "5. Update Weights: Use an optimizer (like SGD or Adam) to adjust the weights based on the gradients calculated.\n",
    "6. Zero the Gradients: Reset the gradients to zero after each mini-batch to prevent accumulation of gradients from multiple passes.\n",
    "\n",
    "This loop repeats for a specified number of epochs or until a certain level of accuracy or loss is achieved.\n",
    "\n",
    "In the end, return the mean loss over all samples for this particular iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cd922fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:39:47.408272Z",
     "start_time": "2024-09-06T13:39:47.402392Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def train_loop(model, criterion, optimizer, iterator, epoch, save_every=10):\n",
    "    \"\"\"\n",
    "    This function is used to train a model for one epoch.\n",
    "    :param model: The model to be trained\n",
    "    :param criterion: The loss function\n",
    "    :param optim: The optimizer\n",
    "    :param iterator: The training data iterator\n",
    "    :return: The average loss for this epoch\n",
    "    \"\"\"\n",
    "    model.train() # Is used to put the model in training mode\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(iterator, total=len(iterator), desc=\"Training Model\"):\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get the predictions\n",
    "        predictions = model(x.to(device))\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(predictions, y.long().to(device))\n",
    "                \n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Add the loss to the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "\n",
    "    average_loss = total_loss / len(iterator)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd15bb79",
   "metadata": {},
   "source": [
    "### 3.4. Validation Loop [3 points - Programming]\n",
    "\n",
    "The validation loop in PyTorch is where the model's performance is evaluated on a dataset different from the one used for training. It does not involve updating the model's weights, focusing instead on assessing how well the model generalizes to new data. Here's how it typically works:\n",
    "\n",
    "1. Iterate Over Validation Dataset: Loop through the validation dataset, usually in mini-batches, without the need for shuffling as in the training loop.\n",
    "2. Forward Pass: Feed the input data to the model to obtain predictions.\n",
    "3. Compute Loss: Calculate the loss (e.g., Cross-Entropy, Mean Squared Error) to assess the performance on the validation dataset.\n",
    "4. Calculate Metrics: Besides loss, other performance metrics like accuracy, F1 score, etc., are computed to evaluate model performance.\n",
    "\n",
    "Note: No Backpropagation: Unlike the training loop, there is no backward pass or weight updates.\n",
    "\n",
    "The validation loop is crucial for monitoring overfitting and tuning hyperparameters. It provides insight into how the model is likely to perform on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de0dacf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:39:49.367274Z",
     "start_time": "2024-09-06T13:39:49.362458Z"
    }
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def val_loop(model, criterion, iterator):\n",
    "    \"\"\"\n",
    "    This function is used to evaluate a model on the validation set.\n",
    "    :param model: The model to be evaluated\n",
    "    :param iterator: The validation data iterator\n",
    "    :return: true: a Python boolean array of all the ground truth values\n",
    "             pred: a Python boolean array of all model predictions.\n",
    "            average_loss: The average loss over the validation set\n",
    "    \"\"\"\n",
    "    \n",
    "    true, pred = [], []\n",
    "    \n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Don't calculate gradients\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Loop through the iterator\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        for x, y in tqdm(iterator, total=len(iterator), desc=\"Validating Model\"):\n",
    "            \n",
    "            # Get the predictions\n",
    "            predictions = model(x.to(device))\n",
    "            loss = criterion(predictions, y.long().to(device))\n",
    "            total_loss += loss.item()\n",
    "                        \n",
    "            _, predicted = torch.max(predictions, dim=1)\n",
    "            \n",
    "            # Add the predictions and labels to the lists\n",
    "            pred.extend(predicted.tolist())\n",
    "            true.extend(y)\n",
    "\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    average_loss = total_loss / len(iterator)\n",
    "    return true, pred, average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q2to0kWVFEIi",
   "metadata": {
    "id": "Q2to0kWVFEIi"
   },
   "source": [
    "### 3.5. Training NBOW [3 points - Non-programming]\n",
    "Assign and tune the below hyperparameters to optimize your model. Make sure that the output graph of the cell where training happens is clear in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "118a4748",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:39:52.905374Z",
     "start_time": "2024-09-06T13:39:52.902829Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# Assigning hyperparameters and training parameters\n",
    "# Experiment with different values for these hyperparaters to optimize your model's performance\n",
    "def get_hyperparams_nbow():\n",
    "  ### your hyper parameters\n",
    "  learning_rate = 0.005\n",
    "  epochs = 15\n",
    "  embedding_dim = 256\n",
    "  ### \n",
    "  return learning_rate, epochs, embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829cc27a",
   "metadata": {},
   "source": [
    "Since the NBOW model is rather basic, assuming you haven't added any additional layers, there's really only one hyperparameter for the model architecture: the size of the embedding dimension.\n",
    "\n",
    "The vocab_size parameter here is based on the number of unique words kept in the vocab after removing those occurring too infrequently, so this is determined by our dataset and is in turn not a true hyperparameter (though the cutoff we used previously might be). The embedding_dim parameter dictates what size vector each word can be embedded as.\n",
    "\n",
    "A special note concerning the model initialization: We're specifically sending the model to the device set in Part 1, to speed up training if the GPU is available. **Be aware**, you'll have to ensure other tensors are on the same device inside your training and validation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5701279e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:55:54.107586Z",
     "start_time": "2024-09-06T13:55:54.104787Z"
    }
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def get_nbow_model(vocab_size, embedding_dim):\n",
    "    \"\"\"\n",
    "    This function returns an instance of the NBOW model.\n",
    "    \"\"\"\n",
    "    model = NBOW(vocab_size = vocab_size, embedding_dim = embedding_dim)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N-iuqkKCFEIj",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:56:14.603139Z",
     "start_time": "2024-09-06T13:55:55.252263Z"
    },
    "id": "N-iuqkKCFEIj",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is the main training loop. You'll need to complete the train_loop and val_loop functions.\n",
    "# You'll also need to complete the criterion and optimizer functions.\n",
    "# Feel free to experiment with different optimizers and learning rates.\n",
    "# Do not change anything else in this cell\n",
    "learning_rate, epochs, embedding_dim = get_hyperparams_nbow()\n",
    "nbow_model = get_nbow_model(vocab_size= len(train_vocab.keys()), embedding_dim = embedding_dim).to(device)\n",
    "criterion = get_criterion()\n",
    "optimizer = get_optimizer(nbow_model, learning_rate)\n",
    "train_loss_over_time_nbow = []\n",
    "val_loss_over_time_nbow = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_loop(nbow_model, criterion, optimizer, train_iterator, epoch, save_every=2)\n",
    "    true, pred, val_loss = val_loop(nbow_model, criterion, val_iterator)\n",
    "    accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "    print(f\"Epoch {epoch+1} -- Train_Loss: {train_loss} -- Val_Loss: {val_loss} -- Val_Accuracy: {accuracy} -- Val_F1: {f1}\")\n",
    "    train_loss_over_time_nbow.append(train_loss)\n",
    "    val_loss_over_time_nbow.append(val_loss)\n",
    "save_checkpoint(nbow_model, 'nbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d11cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL - retain the outputs in submission PDF for credits  \n",
    "plot_loss(train_loss_over_time_nbow, val_loss_over_time_nbow, 'NBOW')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_l91F4ooFEIj",
   "metadata": {
    "id": "_l91F4ooFEIj"
   },
   "source": [
    "### 3.6. Model Evaluation [2 points - Programming]\n",
    "The final points for this will be awarded as per Gradescope's test split, which is different from the local versions. The cell below is just for a sanity check. Your metrics here may not exactly match with the ones on Gradescope, but if your model is fairly generalized, it should not be far off.\n",
    "- 0 points for accuracy <= 84%\n",
    "- 1 point for accuracy > 84% but <= 88%\n",
    "- 2 points for accuracy > 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vs8Fy_ncFEIo",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:57:50.036837Z",
     "start_time": "2024-09-06T13:57:49.521419Z"
    },
    "id": "vs8Fy_ncFEIo"
   },
   "outputs": [],
   "source": [
    "# load best model from checkpoint\n",
    "# DO NOT CHANGE THIS CELL\n",
    "learning_rate, epochs, embedding_dim = get_hyperparams_nbow()\n",
    "nbow_model = get_nbow_model(vocab_size= len(train_vocab.keys()), embedding_dim = embedding_dim).to(device)\n",
    "load_checkpoint(nbow_model, 'nbow', map_location=device)\n",
    "\n",
    "# evaluate model \n",
    "true, pred, val_loss = val_loop(nbow_model, criterion, val_iterator)\n",
    "accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "print(f\"Final Validation Accuracy: {accuracy}\")\n",
    "print(f\"Final Validation F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311270f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL \n",
    "plot_confusion_matrix(true, pred, classes=id2label.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9604440dfb05da0",
   "metadata": {},
   "source": [
    "## 4. Simple Deep Averaging Networks (DAN) [5 points - Programming + 3 points - Non-programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c5bd67",
   "metadata": {},
   "source": [
    "Now, let's look at how to improve performance of the NBOW model. One such way without drastically changing the model complexity is DAN.\n",
    "\n",
    "The core idea of a DAN is to simplify the process of understanding text by averaging the embeddings of words in a sentence or document. This creates a single vector representation that captures the overall meaning of the text.\n",
    "\n",
    "In implementation, a DAN typically involves the following steps:\n",
    "\n",
    "1. Convert each token into an embedding.\n",
    "2. Average these embeddings to create a single vector that represents the entire document.\n",
    "3. Pass this averaged vector through one hidden fully connected neural network layer.\n",
    "4. Use ReLU activation\n",
    "5. Use the output of these layers for tasks like classification.\n",
    "\n",
    "This approach is simpler and often faster than more complex architectures like LSTMs or Transformers, while still providing robust performance for many tasks. However, it might not capture nuances in language as effectively as these more complex models.\n",
    "\n",
    "*NOTE*: Use the same approach to handle pad_tokens as you used in NBOW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260eb012",
   "metadata": {},
   "source": [
    "### 4.1. Model Definition [3 points - Programming]\n",
    "\n",
    "In the following cell, define the architecture of a DAN in the same way as you implemented NBOW-RAND in Section 2 with. Use the following image as a reference along with Section 3 and Figure 1 (right) of [this paper](https://www.aclweb.org/anthology/P15-1162.pdf).\n",
    "\n",
    "Refer to the following equation on how to define DAN -\n",
    "\n",
    "\n",
    "$$ h_{avg} = \\frac{1}{n} \\sum_t emb(x_t) $$\n",
    "\n",
    "$$ h_2 = (w_1h_{avg}) $$\n",
    "\n",
    "$$ h'_{2} = max(0, h_2) $$\n",
    "\n",
    "The probability of a data instance belonging to class $y_i$ is given by:\n",
    "\n",
    "$$ p(y|x) = softmax(w_2^T h'_{2} + b) $$\n",
    "\n",
    "where $w \\in R^d$ is a parameter vector.\n",
    "\n",
    "*Hint*: Make sure to handle the case where the input contains pad tokens. We don't want to consider them in our average.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c142df6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:45:06.583458Z",
     "start_time": "2024-09-06T13:45:06.573213Z"
    }
   },
   "outputs": [],
   "source": [
    "# export\n",
    "class DAN(nn.Module):\n",
    "    # Instantiate layers for your model-\n",
    "    #\n",
    "    # Your model architecture will be a feed-forward neural network.\n",
    "    #\n",
    "    # You'll need 5 nn.Modules:\n",
    "    # 1. An embeddings layer (see nn.Embedding)\n",
    "    # 2. A linear layer (see nn.Linear)\n",
    "    # 3. A ReLU activation (see nn.ReLU)\n",
    "    # 4. A linear layer (see nn.Linear)\n",
    "    #\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes=20):\n",
    "        # voab_size is the size of the vocabulary\n",
    "        # use bias in your hidden layer\n",
    "        # embedding_dim is the dimension of the word embeddings\n",
    "        # hidden_dim is the dimension of the hidden layer outputs, i.e., the 2nd module as per the definition above\n",
    "        super().__init__()\n",
    "        ## YOUR CODE STARTS HERE ##\n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.hidden = nn.Linear(in_features=embedding_dim, out_features=hidden_dim, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(in_features=hidden_dim, out_features=num_classes, bias=True)\n",
    "        ## YOUR CODE ENDS HERE ##\n",
    "\n",
    "    # Complete the forward pass of the model.\n",
    "    #\n",
    "    # Use the output of the embedding layer to create\n",
    "    # the average vector, which will be input into the\n",
    "    # linear layer.\n",
    "    #\n",
    "    # args:\n",
    "    # x - 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
    "    #     This is the same output that comes out of the collate_fn function you completed\n",
    "    def forward(self, x):\n",
    "        ## YOUR CODE STARTS HERE ##\n",
    "        embedded = self.embed(x)\n",
    "        embedded = embedded * (x != 0).unsqueeze(-1).float()\n",
    "        x = torch.sum(embedded, dim=1) / torch.sum((x!=0), dim=-1, keepdim=True) # [batch_size, embedding_dim]\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "        ## YOUR CODE ENDS HERE ##\n",
    "    \n",
    "    def get_embeddings(self, x):\n",
    "        '''\n",
    "        This function returns the embeddings of the input x\n",
    "        '''\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        embed = self.embed(x)\n",
    "        return embed\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "    \n",
    "    def set_embedding_weight(self, weight):\n",
    "        '''\n",
    "        This function sets the embedding weights to the input weight ensure you aren't recording gradients for this\n",
    "        Args:\n",
    "            weight: torch.tensor of shape (vocab_size, embedding_dim)\n",
    "        '''\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        with torch.no_grad():\n",
    "            self.embed.weight = nn.Parameter(weight)\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "    \n",
    "    def get_hidden(self, x):\n",
    "        '''\n",
    "        This function returns the embeddings of the input x\n",
    "        '''\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        embedded = self.embed(x)\n",
    "        embedded = embedded * (x != 0).unsqueeze(-1).float()\n",
    "        x = torch.sum(embedded, dim=1) / torch.sum((x!=0), dim=-1, keepdim=True) # [batch_size, embedding_dim]\n",
    "        x = self.hidden(x)\n",
    "        hidden = x \n",
    "        return hidden\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "    \n",
    "    def set_hidden_weight(self, weight, bias):\n",
    "        '''\n",
    "        This function sets the embedding weights to the input weight ensure you aren't recording gradients for this\n",
    "        Args:\n",
    "            weight: torch.tensor of shape (embedding_dim, hidden_dim)\n",
    "            bias: torch.tensor of shape (1, hidden_dim)\n",
    "        '''\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        with torch.no_grad():\n",
    "            self.hidden.weight = nn.Parameter(weight)\n",
    "            self.hidden.bias = nn.Parameter(bias)\n",
    "        ### YOUR CODE ENDS HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bcaf9910149a3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:58:46.013995Z",
     "start_time": "2024-09-06T13:58:46.005274Z"
    }
   },
   "outputs": [],
   "source": [
    "# local test for sanity:\n",
    "# DO NOT CHANGE THIS CELL\n",
    "def dan_test_local_embeddings():\n",
    "    embedding_dim = 3\n",
    "    vocab_size = 5\n",
    "    model = DAN(embedding_dim=embedding_dim, vocab_size=vocab_size, hidden_dim=10)\n",
    "    model.set_embedding_weight(torch.arange(15).reshape(5,3) / 50)\n",
    "    embeddings = model.get_embeddings(torch.tensor([[1,2,3,4,1],[1,2,3,0,0]]))\n",
    "    correct_embeddings = torch.tensor([[[0.0600, 0.0800, 0.1000],\n",
    "                                        [0.1200, 0.1400, 0.1600],\n",
    "                                        [0.1800, 0.2000, 0.2200],\n",
    "                                        [0.2400, 0.2600, 0.2800],\n",
    "                                        [0.0600, 0.0800, 0.1000]],\n",
    "\n",
    "                                        [[0.0600, 0.0800, 0.1000],\n",
    "                                        [0.1200, 0.1400, 0.1600],\n",
    "                                        [0.1800, 0.2000, 0.2200],\n",
    "                                        [0.0000, 0.0200, 0.0400],\n",
    "                                        [0.0000, 0.0200, 0.0400]]])\n",
    "    if torch.allclose(embeddings, correct_embeddings, rtol=0.001):\n",
    "        print(\"Passed local embedding test\")\n",
    "    else:\n",
    "        print(f\"Embedding Test failed, expected value was\\n{correct_embeddings}\\nbut you got:\\n{embeddings}\")\n",
    "    \n",
    "def dan_test_local_hidden_layer():\n",
    "    vocab_size = 5\n",
    "    embedding_dim = 3\n",
    "    hidden_dim = 3\n",
    "    \n",
    "    model = DAN(vocab_size, embedding_dim, hidden_dim)\n",
    "    model.set_embedding_weight(torch.arange(15).reshape(5,3) / 50)\n",
    "    model.set_hidden_weight(torch.arange(9).reshape(embedding_dim, hidden_dim) / 50, torch.arange(3).reshape(1, hidden_dim) / 50)\n",
    "    \n",
    "    output = model.get_hidden(torch.tensor([[1,2,3,4,1],[1,2,3,0,0]]))\n",
    "    correct_output = torch.tensor([[0.0099, 0.0573, 0.1046],\n",
    "                                    [0.0092, 0.0544, 0.0996]])\n",
    "    \n",
    "    if torch.allclose(output, correct_output, atol=0.001):\n",
    "        print(\"Passed local hidden layer test\")\n",
    "    else:\n",
    "        print(f\"Embedding Test failed, expected value was\\n{correct_output}\\nbut you got:\\n{output}\")\n",
    "\n",
    "dan_test_local_embeddings()\n",
    "dan_test_local_hidden_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7774e866",
   "metadata": {},
   "source": [
    "### 4.2. DAN Training [3 points - Non-programming]\n",
    "\n",
    "In this section (and all later sections), you will leverage the same functions defined in Section 3 to train your DAN. To do this, simply initialize your DAN Model and pass that object to the training and evaluation loop to train your model.\n",
    "\n",
    "Assign and tune the below hyperparameters to optimize your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a67e812e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:58:47.917291Z",
     "start_time": "2024-09-06T13:58:47.914036Z"
    }
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def get_dan_model(vocab_size, embedding_dim, hidden_dim):\n",
    "    \"\"\"\n",
    "    This function returns an instance of the DAN model. Initialize the DAN model here and return it. Note that the hidden_dim will be the dimension of the hidden layer in DAN.\n",
    "    \"\"\"\n",
    "    model = None\n",
    "    ## YOUR CODE STARTS HERE ##\n",
    "    model = DAN(vocab_size = vocab_size, embedding_dim = embedding_dim, hidden_dim=hidden_dim)\n",
    "    ## YOUR CODE ENDS HERE ##\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "715ffda1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:58:48.958313Z",
     "start_time": "2024-09-06T13:58:48.955593Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# Assign hyperparameters and training parameters\n",
    "# Experiment with different values for these hyperparaters to optimize your model's performance\n",
    "def get_hyperparams_dan():\n",
    "  ### your hyper parameters\n",
    "    learning_rate = 0.002\n",
    "    epochs = 15\n",
    "    hidden_layer_dimensions = 32\n",
    "    embedding_dim = 256\n",
    "    ### \n",
    "    return learning_rate, epochs, hidden_layer_dimensions, embedding_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5e8143",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:59:29.633396Z",
     "start_time": "2024-09-06T13:58:50.611200Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the main training loop. You'll need to complete the train_loop and val_loop functions.\n",
    "# You'll also need to complete the criterion and optimizer functions.\n",
    "# Feel free to experiment with different optimizers and learning rates.\n",
    "# Do not change anything else in this cell\n",
    "learning_rate, epochs, hidden_layer_dimensions, embedding_dim = get_hyperparams_dan()\n",
    "dan_model = get_dan_model(len(train_vocab.keys()), embedding_dim, hidden_layer_dimensions).to(device)\n",
    "criterion = get_criterion()\n",
    "optimizer = get_optimizer(dan_model, learning_rate)\n",
    "train_loss_over_time_dan = []\n",
    "val_loss_over_time_dan = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_loop(dan_model, criterion, optimizer, train_iterator, epoch, save_every=2)\n",
    "    true, pred, val_loss = val_loop(dan_model, criterion, val_iterator)\n",
    "    accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "    train_loss_over_time_dan.append(train_loss)\n",
    "    val_loss_over_time_dan.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1} -- Train_Loss: {train_loss} -- Val_Loss: {val_loss} -- Val_Accuracy: {accuracy} -- Val_F1: {f1}\")\n",
    "\n",
    "save_checkpoint(dan_model, 'dan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3662383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL - retain the outputs in submission PDF to get credits  \n",
    "plot_loss(train_loss_over_time_dan, val_loss_over_time_dan, 'DAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c756e6",
   "metadata": {},
   "source": [
    "### 4.3. Model Evaluation [2 points - Programming]\n",
    "The final points for this will be awarded as per Gradescope's test split, which is different from the local versions. The cell below is just for a sanity check. Your metrics here may not exactly match with the ones on Gradescope, but if your model is fairly generalized, it should not be far off.\n",
    "- 0 points for accuracy <= 84%\n",
    "- 1 point for accuracy > 84% but <= 88%\n",
    "- 2 points for accuracy > 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708bf89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:01:27.924820Z",
     "start_time": "2024-09-06T14:01:27.149608Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "learning_rate, epochs, hidden_layer_dimensions, embedding_dim = get_hyperparams_dan()\n",
    "dan_model = get_dan_model(len(train_vocab.keys()), embedding_dim, hidden_layer_dimensions).to(device)\n",
    "load_checkpoint(dan_model, 'dan', map_location=device)\n",
    "\n",
    "true, pred, val_loss = val_loop(dan_model, criterion, val_iterator)\n",
    "accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "print(f\"Final Validation Accuracy: {accuracy}\")\n",
    "print(f\"Final Validation F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc4d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "plot_confusion_matrix(true, pred, classes=id2label.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f011d1e5aa84ae54",
   "metadata": {},
   "source": [
    "## 5. Attention-based Models [21 points - Programming + 9 points - Non-programming]\n",
    " In the simplest terms, attention allows a network to differentially focus on specific input words rather considering their importance equally, as done in the previous sections by averaging. For example, often times the mere presence of word \"election\" is enough to ascertain the category of the sentence to be politics. \n",
    "\n",
    " There are various types of attention which we will dsicuss in much more depth throughout course. This section is just to provide a conceptual flavor of attention as a concept. In the below parts, you will work with three different simple types of attention. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20687e406d1feaa4",
   "metadata": {},
   "source": [
    "### 5.1. Attention-weighted NBOW [7 points - Programming + 3 points - Non-programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659df7f91798f1d5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You will now define an encoder that uses a simple attention function to produce a weight for each word in the sentence followed by a sum of the attention-weighted word embeddings. Simple attention allows the model to learn a weight vector $\\alpha_t$ which represents how important will different tokens in a document be.\n",
    "\n",
    "Consider $u$ to be a single attention head (a learnable PyTorch parameter). With this,\n",
    "\n",
    "$$ \\alpha_t \\varpropto \\exp\\{cos(u,emb(x_t))\\} $$\n",
    "\n",
    "*Note*: This needs to be normalized.\n",
    "\n",
    "$$ h_{att} = \\sum_t \\alpha_t emb(x_t) $$\n",
    "\n",
    "The probability of a data instance belonging to class $y_i$ is given by:\n",
    "\n",
    "$$ p(y|x) = softmax(w^T h_{att}) $$\n",
    "\n",
    "where $w \\in R^d$ is a parameter vector.\n",
    "\n",
    "In this model, the unnormalized attention weight for a word $x$ is computed using the cosine similarity between a learnable parameter $u$ and the word embedding for $x$ followed by exponentiation. To get normalized weights $\\alpha_t$, normalize across all words in the sentence. Then multiply the attention weights by the word embeddings and sum the attention-weighted embeddings.\n",
    "\n",
    "*HINT*: See if Softmax function can help with this\n",
    "\n",
    "*Hint*: Make sure to handle the case where the input contains pad tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5194f9",
   "metadata": {},
   "source": [
    "#### 5.1.1. Model Definition [5 points - Programming]\n",
    "\n",
    "Define your simple attention model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6f08f67204699bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:01:45.380976Z",
     "start_time": "2024-09-06T14:01:45.375347Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class SimpleAttentionNBOW(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the Attention-weighted Neural Bag of Words model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes=20):\n",
    "        super(SimpleAttentionNBOW, self).__init__()\n",
    "        ## YOUR CODE STARTS HERE ##\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, num_classes, bias=False)\n",
    "        self.u = nn.Parameter(torch.randn(embedding_dim)/ embedding_dim) \n",
    "        \n",
    "        ## YOUR CODE ENDS HERE ##\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## YOUR CODE STARTS HERE ##\n",
    "        \n",
    "        # Get the embeddings\n",
    "        embeddings = self.embeddings(x)\n",
    "        \n",
    "        # Get the attention weights\n",
    "        attention_weights = torch.exp(torch.cosine_similarity(embeddings, self.u[None,None,:], dim=2))\n",
    "\n",
    "        # correct the attn weights so they remove anything which is not a token ie 0 padding\n",
    "        attention_weights = torch.where(x != 0, attention_weights, 0)\n",
    "        \n",
    "        # Normalize the attention weights\n",
    "        attention_weights = attention_weights / torch.sum(attention_weights, dim=1).unsqueeze(1)\n",
    "        \n",
    "        # Multiply the embeddings by the attention weights\n",
    "        attention_embeddings = embeddings * attention_weights.unsqueeze(2)\n",
    "        \n",
    "        # Sum the attention embeddings\n",
    "        attention_embeddings = torch.sum(attention_embeddings, dim=1)\n",
    "        \n",
    "        # Get the predictions\n",
    "        predictions = self.linear(attention_embeddings)\n",
    "        \n",
    "        ## YOUR CODE ENDS HERE ##\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def get_embeddings(self, x):\n",
    "        '''\n",
    "        This function returns the embeddings of the input x\n",
    "        '''\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        embeddings = self.embeddings(x)\n",
    "        return embeddings\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "    \n",
    "    def set_embedding_weight(self, weight):\n",
    "        '''\n",
    "        This function sets the embedding weights to the input weight ensure you aren't recording gradients for this\n",
    "        Args:\n",
    "            weight: torch.tensor of shape (vocab_size, embedding_dim)\n",
    "        '''\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        with torch.no_grad():\n",
    "            self.embeddings.weight = nn.Parameter(weight)\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "            \n",
    "    def set_attention_weights(self, weight):\n",
    "        '''\n",
    "        This function sets the attention weights to the input weight ensure you aren't recording gradients for this\n",
    "        Args:\n",
    "            weight: torch.tensor of shape (embedding_dim)\n",
    "        '''\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        with torch.no_grad():\n",
    "            self.u = nn.Parameter(weight)\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "    \n",
    "    def get_attention_matrix(self, x):\n",
    "        '''\n",
    "        This function returns the normalized attention matrix for the input x\n",
    "        Args:\n",
    "            x: torch.tensor of shape (BATCH_SIZE, max seq length in batch))\n",
    "        Returns:\n",
    "            attention_weights: torch.tensor of shape (BATCH_SIZE, max seq length in batch))\n",
    "        '''\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        embeddings = self.embeddings(x)\n",
    "        attention_weights = torch.exp(torch.cosine_similarity(embeddings, self.u[None,None,:], dim=2))\n",
    "        attention_weights = torch.where(x != 0, attention_weights, 0)\n",
    "        attention_weights = attention_weights / torch.sum(attention_weights, dim=1).unsqueeze(1)\n",
    "        return attention_weights\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b8882f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:01:49.312348Z",
     "start_time": "2024-09-06T14:01:49.298612Z"
    }
   },
   "outputs": [],
   "source": [
    "# local test for sanity:\n",
    "# DO NOT CHANGE THIS CELL\n",
    "def simple_attention_nbow_test_local_embeddings():\n",
    "    model = SimpleAttentionNBOW(embedding_dim=3, vocab_size=5)\n",
    "    model.set_embedding_weight(torch.arange(15).reshape(5,3) / 50)\n",
    "    embeddings = model.get_embeddings(torch.tensor([[1,2,3,4,1],[1,2,3,0,0]]))\n",
    "    correct_embeddings = torch.tensor([[[0.0600, 0.0800, 0.1000],\n",
    "                                        [0.1200, 0.1400, 0.1600],\n",
    "                                        [0.1800, 0.2000, 0.2200],\n",
    "                                        [0.2400, 0.2600, 0.2800],\n",
    "                                        [0.0600, 0.0800, 0.1000]],\n",
    "\n",
    "                                        [[0.0600, 0.0800, 0.1000],\n",
    "                                        [0.1200, 0.1400, 0.1600],\n",
    "                                        [0.1800, 0.2000, 0.2200],\n",
    "                                        [0.0000, 0.0200, 0.0400],\n",
    "                                        [0.0000, 0.0200, 0.0400]]])\n",
    "    if torch.allclose(embeddings, correct_embeddings, rtol=0.001):\n",
    "        print(\"Passed local embedding test\")\n",
    "    else:\n",
    "        print(f\"Embedding Test failed, expected value was\\n{correct_embeddings}\\nbut you got:\\n{embeddings}\")\n",
    "def simple_attention_nbow_test_local_attn():\n",
    "    model = SimpleAttentionNBOW(embedding_dim=3, vocab_size=5)\n",
    "    model.set_embedding_weight(torch.arange(15).reshape(5,3) / 50)\n",
    "    model.set_attention_weights(torch.tensor([0.1, 0.2, 0.3]))\n",
    "    attention_weights = model.get_attention_matrix(torch.tensor([[1,2,3,4,1],[1,2,3,0,0]]))\n",
    "    correct_attention_weights = torch.tensor([[0.2033, 0.1995, 0.1975, 0.1964, 0.2033],\n",
    "                                              [0.3387, 0.3323, 0.3290, 0.0000, 0.0000]])\n",
    "    if torch.allclose(attention_weights, correct_attention_weights, rtol=0.001):\n",
    "        print(\"Passed local Attn test\")\n",
    "    else:\n",
    "        print(f\"Attn Test failed, expected value was\\n{correct_attention_weights}\\nbut you got:\\n{attention_weights}\")\n",
    "\n",
    "simple_attention_nbow_test_local_embeddings()\n",
    "simple_attention_nbow_test_local_attn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ebffb2",
   "metadata": {},
   "source": [
    "#### 5.1.2. Model Training [3 points - Non-Programming]\n",
    "Assign and tune the below hyperparameters to optimize your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f9d019244aa92cce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:04:27.232668Z",
     "start_time": "2024-09-06T14:04:27.229756Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# Assign hyperparameters and training parameters\n",
    "# Experiment with different values for these hyperparaters to optimize your model's performance\n",
    "def get_hyperparams_simple_attention():\n",
    "  ### your hyper parameters\n",
    "  learning_rate = 0.0025\n",
    "  epochs = 10\n",
    "  embedding_dim = 256\n",
    "  ### \n",
    "  return learning_rate, epochs, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3207692320e7c2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:03:21.299462Z",
     "start_time": "2024-09-06T14:03:21.296044Z"
    }
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def get_simple_attention_model(vocab_size, embedding_dim):\n",
    "    \"\"\"\n",
    "    This function returns an instance of the SimpleAttentionNBOW model. Initialize the SimpleAttentionNBOW model here and return it.\n",
    "    \"\"\"\n",
    "    model = None\n",
    "    ## YOUR CODE STARTS HERE ##\n",
    "    model = SimpleAttentionNBOW(vocab_size = vocab_size, embedding_dim = embedding_dim)\n",
    "    ## YOUR CODE ENDS HERE ##\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40187202f104838f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:05:40.236342Z",
     "start_time": "2024-09-06T14:04:28.529888Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the main training loop. You'll need to complete the train_loop and val_loop functions.\n",
    "# You'll also need to complete the criterion and optimizer functions.\n",
    "# Feel free to experiment with different optimizers and learning rates.\n",
    "# Do not change anything else in this cell\n",
    "learning_rate, epochs, embedding_dim = get_hyperparams_simple_attention()\n",
    "simple_attention_model = get_simple_attention_model(vocab_size=len(train_vocab.keys()),embedding_dim=embedding_dim).to(device)\n",
    "criterion = get_criterion()\n",
    "train_loss_over_time_sa = []\n",
    "val_loss_over_time_sa = []\n",
    "optimizer = get_optimizer(simple_attention_model, learning_rate)\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_loop(simple_attention_model, criterion, optimizer, train_iterator, epoch, save_every=2)\n",
    "    true, pred, val_loss = val_loop(simple_attention_model, criterion, val_iterator)\n",
    "    accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "    train_loss_over_time_sa.append(train_loss)\n",
    "    val_loss_over_time_sa.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1} -- Train_Loss: {train_loss} -- Val_Loss: {val_loss} -- Val_Accuracy: {accuracy} -- Val_F1: {f1}\")\n",
    "save_checkpoint(simple_attention_model, 'simple_attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e75ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL - retain the outputs in submission PDF to get credits   \n",
    "plot_loss(train_loss_over_time_sa, val_loss_over_time_sa, 'Simple Attention NBOW')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f55108a",
   "metadata": {},
   "source": [
    "#### 5.1.3. Model Evaluation [2 points - Programming]\n",
    "The final points for this will be awarded as per Gradescope's test split, which is different from the local versions. The cell below is just for a sanity check. Your metrics here may not exactly match with the ones on Gradescope, but if your model is fairly generalized, it should not be far off.\n",
    "- 0 points for accuracy <= 85%\n",
    "- 1 point for accuracy > 85% but <= 90%\n",
    "- 2 points for accuracy > 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e3fba6a2c41556",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:07:27.391423Z",
     "start_time": "2024-09-06T14:07:26.365329Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "learning_rate, epochs, embedding_dim = get_hyperparams_simple_attention()\n",
    "simple_attention_model = get_simple_attention_model(vocab_size=len(train_vocab.keys()),embedding_dim=embedding_dim).to(device)\n",
    "load_checkpoint(simple_attention_model, 'simple_attention', map_location=device)\n",
    "\n",
    "true, pred, val_loss = val_loop(simple_attention_model, criterion, val_iterator)\n",
    "accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "print(f\"Final Validation Accuracy: {accuracy}\")\n",
    "print(f\"Final Validation F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ac30d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "plot_confusion_matrix(true, pred, classes=id2label.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b45e6c8",
   "metadata": {},
   "source": [
    "### 5.2. MultiHead Attention NBOW [7 points - Programming + 3 points - Non-programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b63a64",
   "metadata": {},
   "source": [
    "The prior model only uses a single attention function. In this section, you will implement a multi-head attention model. You will use $k$ attention heads, each with its own parameters $u_i \\in R^d$ ($\\forall i \\in [1..k]$) and a single large vector before the classification to weight them all together $w \\in R^{d \\cdot k}$. \n",
    "\n",
    "$$ \\alpha_{t,i} \\varpropto \\exp\\{cos(u_i,emb(x_t))\\} $$\n",
    "\n",
    "$$ h_{att}(i) = \\sum_t\\alpha_{t,i} emb(x_t) $$\n",
    "\n",
    "With the probability of a task instance belonging to class $y_i$ is given by:\n",
    "\n",
    "$$ p(y|x) = softmax(w^T [h_{att}(1), h_{att}(2), ..., h_{att}(k)]) $$\n",
    "\n",
    "where [a,b] is the concatenation of vectors $a$ and $b$, into a single taller vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2941d83a",
   "metadata": {},
   "source": [
    "#### 5.2.1. Model Definition [5 points - Programming]\n",
    "\n",
    "Define your Multi-head attention below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "51d31762",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:07:38.378912Z",
     "start_time": "2024-09-06T14:07:38.367534Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class MultiHeadAttentionNBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, num_classes=20):\n",
    "        ## YOUR CODE STARTS HERE ##\n",
    "        super(MultiHeadAttentionNBOW, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_tokens = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.embed_head_index = nn.Embedding(num_embeddings=num_heads, embedding_dim=embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim * num_heads, num_classes, bias=False)\n",
    "        ## YOUR CODE ENDS HERE ##\n",
    "    def forward(self, x):\n",
    "        ## YOUR CODE STARTS HERE ##\n",
    "        BATCH = x.shape[0]\n",
    "        x_emb = self.embed_tokens(x) # [BATCH, token, embedding_dim]\n",
    "        head_embeddings = self.embed_head_index(torch.arange(self.num_heads, device=x.device)) # results in [num_heads, embedding_dim]\n",
    "        x_emb_normalized = x_emb / x_emb.norm(dim=-1, keepdim=True)\n",
    "        head_embeddings_normalized = head_embeddings / head_embeddings.norm(dim=-1, keepdim=True)\n",
    "        attn = x_emb_normalized @ head_embeddings_normalized.T\n",
    "        attn = torch.where(torch.tile(x[...,None], dims=(1,1,attn.shape[-1])) != 0, attn, -torch.inf)\n",
    "        attn = torch.softmax(attn, dim=-2) # [B, token, num_heads]\n",
    "        head_values = attn.transpose(-1,-2) @ x_emb # [BATCH, num_heads, embedding_dim]\n",
    "        head_values = head_values.reshape(BATCH, -1) # [BATCH, num_heads * embedding_dim]\n",
    "        x = self.linear(head_values)\n",
    "        ## YOUR CODE ENDS HERE ##\n",
    "        return x\n",
    "\n",
    "\n",
    "    def get_embeddings(self, x):\n",
    "        '''\n",
    "        This function returns the embeddings of the input x\n",
    "        '''\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        embeddings = self.embed_tokens(x)\n",
    "        return embeddings\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "    \n",
    "    def set_embedding_weight(self, weight):\n",
    "        '''\n",
    "        This function sets the embedding weights to the input weight ensure you aren't recording gradients for this\n",
    "        Args:\n",
    "            weight: torch.tensor of shape (vocab_size, embedding_dim)\n",
    "        '''\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        with torch.no_grad():\n",
    "            self.embed_tokens.weight = nn.Parameter(weight)\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "            \n",
    "    def set_attention_weights(self, weight):\n",
    "        '''\n",
    "        This function sets the attention weights to the input weight ensure you aren't recording gradients for this\n",
    "        Args:\n",
    "            weight: torch.tensor of shape (num_heads, embedding_dim)\n",
    "        '''\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        with torch.no_grad():\n",
    "            self.embed_head_index.weight = nn.Parameter(weight)\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "    \n",
    "    def get_attention_matrix(self, x):\n",
    "        '''\n",
    "        This function returns the normalized attention matrix for the input x\n",
    "        Args:\n",
    "            x: torch.tensor of shape (BATCH_SIZE, max seq length in batch))\n",
    "        Returns:\n",
    "            attention_weights: torch.tensor of shape (BATCH_SIZE, max seq length in batch, num_heads))\n",
    "        '''\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        BATCH = x.shape[0]\n",
    "        x_emb = self.embed_tokens(x) # [BATCH, token, embedding_dim]\n",
    "        head_embeddings = self.embed_head_index(torch.arange(self.num_heads, device=x.device)) # results in [num_heads, embedding_dim]\n",
    "        x_emb_normalized = x_emb / x_emb.norm(dim=-1, keepdim=True)\n",
    "        head_embeddings_normalized = head_embeddings / head_embeddings.norm(dim=-1, keepdim=True)\n",
    "        attn = x_emb_normalized @ head_embeddings_normalized.T\n",
    "        attn = torch.where(torch.tile(x[...,None], dims=(1,1,attn.shape[-1])) != 0, attn, -torch.inf)\n",
    "        attn = torch.softmax(attn, dim=-2)\n",
    "        return attn\n",
    "        ### YOUR CODE ENDS HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba094ac8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:08:16.954544Z",
     "start_time": "2024-09-06T14:08:16.945182Z"
    }
   },
   "outputs": [],
   "source": [
    "# local test for sanity:\n",
    "# DO NOT CHANGE THIS CELL\n",
    "def multihead_attn_nbow_test_local():\n",
    "    embedding_dim = 10\n",
    "    vocab_size = 10\n",
    "    num_heads = 3\n",
    "    model = MultiHeadAttentionNBOW(vocab_size=vocab_size, embedding_dim=embedding_dim, num_heads=num_heads)\n",
    "    for _, module in model.named_parameters():\n",
    "        if hasattr(module, \"data\"):\n",
    "            nn.init.constant_(module, 0.3)\n",
    "    input = torch.tensor([[1,2,3,4,0,0,0],\n",
    "                          [5,6,7,0,0,0,0]]) % vocab_size\n",
    "    expected_result = torch.tensor(\n",
    "        [[2.7000, 2.7000, 2.7000, 2.7000, 2.7000, 2.7000, 2.7000, 2.7000, 2.7000,\n",
    "         2.7000, 2.7000, 2.7000, 2.7000, 2.7000, 2.7000, 2.7000, 2.7000, 2.7000,\n",
    "         2.7000, 2.7000],\n",
    "        [2.7000, 2.7000, 2.7000, 2.7000, 2.7000, 2.7000, 2.7000, 2.7000, 2.7000,\n",
    "         2.7000, 2.7000, 2.7000, 2.7000, 2.7000, 2.7000, 2.7000, 2.7000, 2.7000,\n",
    "         2.7000, 2.7000]]\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        local_result = model(input)\n",
    "    if torch.allclose(expected_result, local_result, rtol=0.001):\n",
    "        print(\"Passed local check\")\n",
    "    else:\n",
    "        print(f\"Test failed, expected value was\\n{expected_result}\\nbut you got:\\n{local_result}\")\n",
    "\n",
    "# local test for sanity:\n",
    "def multi_attention_nbow_test_local_embeddings():\n",
    "    model = MultiHeadAttentionNBOW(embedding_dim=3, vocab_size=5, num_heads=4)\n",
    "    model.set_embedding_weight(torch.arange(15).reshape(5,3) / 50)\n",
    "    embeddings = model.get_embeddings(torch.tensor([[1,2,3,4,1],[1,2,3,0,0]]))\n",
    "    correct_embeddings = torch.tensor([[[0.0600, 0.0800, 0.1000],\n",
    "                                        [0.1200, 0.1400, 0.1600],\n",
    "                                        [0.1800, 0.2000, 0.2200],\n",
    "                                        [0.2400, 0.2600, 0.2800],\n",
    "                                        [0.0600, 0.0800, 0.1000]],\n",
    "\n",
    "                                        [[0.0600, 0.0800, 0.1000],\n",
    "                                        [0.1200, 0.1400, 0.1600],\n",
    "                                        [0.1800, 0.2000, 0.2200],\n",
    "                                        [0.0000, 0.0200, 0.0400],\n",
    "                                        [0.0000, 0.0200, 0.0400]]])\n",
    "    if torch.allclose(embeddings, correct_embeddings, rtol=0.001):\n",
    "        print(\"Passed local embedding test\")\n",
    "    else:\n",
    "        print(f\"Embedding Test failed, expected value was\\n{correct_embeddings}\\nbut you got:\\n{embeddings}\")\n",
    "def multi_attention_nbow_test_local_attn():\n",
    "    model = MultiHeadAttentionNBOW(embedding_dim=3, vocab_size=5, num_heads=4)\n",
    "    model.set_embedding_weight(torch.arange(15).reshape(5,3) / 50)\n",
    "    model.set_attention_weights(torch.tensor([[0.1, 0.2, 0.3],[0.1, 0.2, 0.3],[0.2, 0.2, 0.2],[0, 0.3, 0.3]]))\n",
    "    attention_weights = model.get_attention_matrix(torch.tensor([[1,2,3,4,1],[1,2,3,0,0]]))\n",
    "    correct_attention_weights = torch.tensor([[[0.2033, 0.2033, 0.1981, 0.2052],\n",
    "                                                [0.1995, 0.1995, 0.2007, 0.1990],\n",
    "                                                [0.1975, 0.1975, 0.2014, 0.1961],\n",
    "                                                [0.1964, 0.1964, 0.2017, 0.1945],\n",
    "                                                [0.2033, 0.2033, 0.1981, 0.2052]],\n",
    "\n",
    "                                                [[0.3387, 0.3387, 0.3300, 0.3419],\n",
    "                                                [0.3323, 0.3323, 0.3344, 0.3314],\n",
    "                                                [0.3290, 0.3290, 0.3356, 0.3267],\n",
    "                                                [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "                                                [0.0000, 0.0000, 0.0000, 0.0000]]])\n",
    "    if torch.allclose(attention_weights, correct_attention_weights, rtol=0.001):\n",
    "        print(\"Passed local Attn test\")\n",
    "    else:\n",
    "        print(f\"Attn Test failed, expected value was\\n{correct_attention_weights}\\nbut you got:\\n{attention_weights}\")\n",
    "\n",
    "multi_attention_nbow_test_local_embeddings()\n",
    "multi_attention_nbow_test_local_attn()\n",
    "multihead_attn_nbow_test_local()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b569d6",
   "metadata": {},
   "source": [
    "#### 5.2.2. Model Training [3 points - Non-Programming]\n",
    "\n",
    "Assign and tune the below hyperparameters to optimize your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3813f290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T02:37:30.396855Z",
     "start_time": "2024-09-07T02:37:30.393939Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# Assign hyperparameters and training parameters\n",
    "# Experiment with different values for these hyperparaters to optimize your model's performance\n",
    "def get_hyperparams_multihead():\n",
    "    learning_rate = 0.002\n",
    "    epochs = 15\n",
    "    num_heads = 5\n",
    "    embedding_dim = 256\n",
    "    return learning_rate, epochs, num_heads, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f4d8d40b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T02:37:31.443498Z",
     "start_time": "2024-09-07T02:37:31.440545Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_multihead_attention_model(vocab_size, embedding_dim, num_heads):\n",
    "    \"\"\"\n",
    "    This function returns an instance of the MultiHeadAttentionNBOW model. Initialize the MultiHeadAttentionNBOW model here and return it.\n",
    "    \"\"\"\n",
    "    model = None\n",
    "    ## YOUR CODE STARTS HERE ##\n",
    "    model = MultiHeadAttentionNBOW(vocab_size = vocab_size, embedding_dim = embedding_dim, num_heads=num_heads).to(device)\n",
    "    ## YOUR CODE ENDS HERE ##\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ddf7cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T02:38:27.919330Z",
     "start_time": "2024-09-07T02:37:32.862917Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the main training loop. You'll need to complete the train_loop and val_loop functions.\n",
    "# You'll also need to complete the criterion and optimizer functions.\n",
    "# Feel free to experiment with different optimizers and learning rates.\n",
    "# Do not change anything else in this cell\n",
    "learning_rate, epochs, num_heads, embedding_dim = get_hyperparams_multihead()\n",
    "multihead_attention_model = get_multihead_attention_model(vocab_size=len(train_vocab.keys()),embedding_dim=embedding_dim, num_heads=num_heads).to(device)\n",
    "criterion = get_criterion()\n",
    "optimizer = get_optimizer(multihead_attention_model, learning_rate)\n",
    "train_loss_over_time_ma = []\n",
    "val_loss_over_time_ma = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_loop(multihead_attention_model, criterion, optimizer, train_iterator, epoch, save_every=2)\n",
    "    true, pred, val_loss = val_loop(multihead_attention_model, criterion, val_iterator)\n",
    "    accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "    train_loss_over_time_ma.append(train_loss)\n",
    "    val_loss_over_time_ma.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1} -- Train_Loss: {train_loss} -- Val_Loss: {val_loss} -- Val_Accuracy: {accuracy} -- Val_F1: {f1}\")\n",
    "save_checkpoint(multihead_attention_model, 'multihead_attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af63f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL - retain the outputs in submission PDF for credits \n",
    "plot_loss(train_loss_over_time_ma, val_loss_over_time_ma, 'Multihead Attention NBOW')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a2143a",
   "metadata": {},
   "source": [
    "#### 5.2.3. Model Evaluation [2 points - Programming]\n",
    "The final points for this will be awarded as per Gradescope's test split, which is different from the local versions. The cell below is just for a sanity check. Your metrics here may not exactly match with the ones on Gradescope, but if your model is fairly generalized, it should not be far off.\n",
    "- 0 points for accuracy <= 85%\n",
    "- 1 point for accuracy > 85% but <= 90%\n",
    "- 2 points for accuracy > 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8115a2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:13:43.903039Z",
     "start_time": "2024-09-06T14:13:43.058502Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "learning_rate, epochs, num_heads, embedding_dim = get_hyperparams_multihead()\n",
    "multihead_attention_model = get_multihead_attention_model(vocab_size=len(train_vocab.keys()),embedding_dim=embedding_dim, num_heads=num_heads).to(device)\n",
    "load_checkpoint(multihead_attention_model, 'multihead_attention')\n",
    "\n",
    "true, pred, val_loss = val_loop(multihead_attention_model, criterion, val_iterator)\n",
    "accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "print(f\"Final Validation Accuracy: {accuracy}\")\n",
    "print(f\"Final Validation F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d23454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "plot_confusion_matrix(true, pred, classes=id2label.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4227e",
   "metadata": {},
   "source": [
    "### 5.3. Self-Attention NBOW [7 points - Programming + 3 points - Non-programming]\n",
    "\n",
    "Self-attention is a mechanism in neural networks that enables each element in a sequence to consider and weigh the importance of every other element. This facilitates a more nuanced and context-aware representation of the sequence, greatly enhancing the capabilities of models in tasks involving sequential data, particularly in NLP. It has gained prominence with the introduction and success of Transformer models, like BERT, GPT (including GPT-3), and others. This is not a full-fledged implementation of it, but instead a conceptual flavor of the mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974fdb51",
   "metadata": {},
   "source": [
    "We will now define an encoder that uses a simple form of self-attention when producing attention weights for each word in the sentence:\n",
    "$$a_{ts} = emb(x_t)^Temb(x_s)$$\n",
    "$$_t \\propto exp(\\sum_s a_{ts})$$\n",
    "$$h_{self} = \\sum _temb(x_t)$$\n",
    "Then, the probability of a class is given by\n",
    "$$softmax(w^Th_{self})$$\n",
    "The unnormalized attention weight for a word $x$ is computed using the dot product between its embedding and those for all other words in the sentence, followed by a summation and exponentiation. Unlike the model in Section 5.1., this model does not introduce any new parameters for computing the attention function, simply using the same word embeddings for the attention. Therefore, this model has the same number of parameters as the model in Section 2.\n",
    "For improved stability, we can also add a residual connection, which would change Eq. 1 to\n",
    "$$softmax(w^T(h_{self} + h_{avg}))$$\n",
    "where $h_{avg}$ is computed as in Section 2 (though using the same word embeddings as in $h_{self}$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f4df1",
   "metadata": {},
   "source": [
    "#### 5.3.1. Model Definition [5 points - Programming]\n",
    "Define your self attention model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "63c9a563",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:13:49.118448Z",
     "start_time": "2024-09-06T14:13:49.111063Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class SelfAttentionNBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes=20):\n",
    "        super(SelfAttentionNBOW, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        BATCH, SEQ = x.shape\n",
    "        embedded = self.embedding(x)  # [batch_size, sentence_length, embedding_dim]\n",
    "        embedded = embedded * (x != 0).unsqueeze(-1).float()\n",
    "        attention_scores = torch.bmm(embedded, embedded.transpose(1, 2))  # [batch_size, sentence_length, sentence_length]\n",
    "        attention_scores = attention_scores.sum(-1) # [batch_size, sentence_length]\n",
    "        attention_scores = torch.where((x != 0), attention_scores, -torch.inf)\n",
    "        attention = F.softmax(attention_scores, dim=1) # [batch_size, sentence_length]\n",
    "        h_self = torch.bmm(attention.unsqueeze(1), embedded).squeeze(1) # [batch_size, embedding_dim]\n",
    "        h_avg = torch.sum(embedded, dim=1) / torch.sum((x!=0), dim=-1, keepdim=True) # [batch_size, embedding_dim]\n",
    "        h_combined = h_self + h_avg  # [batch_size, embedding_dim]\n",
    "        out = self.linear(h_combined)  # [batch_size, output_dim]\n",
    "        return out\n",
    "    def get_embeddings(self, x):\n",
    "        '''\n",
    "        This function returns the embeddings of the input x\n",
    "        '''\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        embeddings = self.embedding(x)\n",
    "        return embeddings\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "    def set_embedding_weight(self, weight):\n",
    "        '''\n",
    "        This function sets the embedding weights to the input weight ensure you aren't recording gradients for this\n",
    "        Args:\n",
    "            weight: torch.tensor of shape (vocab_size, embedding_dim)\n",
    "        '''\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        with torch.no_grad():\n",
    "            self.embedding.weight = nn.Parameter(weight)\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "    def get_attention_matrix(self, x):\n",
    "        '''\n",
    "        This function returns the normalized attention matrix for the input x\n",
    "        Args:\n",
    "            x: torch.tensor of shape (BATCH_SIZE, max seq length in batch)\n",
    "        Returns:\n",
    "            attention_weights: torch.tensor of shape (BATCH_SIZE, max seq length in batch, max seq length in batch)\n",
    "        '''\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        BATCH, SEQ = x.shape\n",
    "        embedded = self.embedding(x)  # [batch_size, sentence_length, embedding_dim]\n",
    "        embedded = embedded * (x != 0).unsqueeze(-1).float()\n",
    "        attention_scores = torch.bmm(embedded, embedded.transpose(1, 2))  # [batch_size, sentence_length, sentence_length]\n",
    "        attention_scores = attention_scores.sum(-1) # [batch_size, sentence_length]\n",
    "        attention_scores = torch.where((x != 0), attention_scores, -torch.inf)\n",
    "        attention = F.softmax(attention_scores, dim=1) # [batch_size, sentence_length]\n",
    "        return attention\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ce212",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:14:10.886664Z",
     "start_time": "2024-09-06T14:14:10.878505Z"
    }
   },
   "outputs": [],
   "source": [
    "# local test for sanity:\n",
    "def self_attention_nbow_test_local():\n",
    "    model = SelfAttentionNBOW(vocab_size=10, embedding_dim=10)\n",
    "    for _, module in model.named_parameters():\n",
    "        if hasattr(module, \"data\"):\n",
    "            nn.init.constant_(module, 0.3)\n",
    "    input = torch.tensor([[1,2,3,4,0,0,0],\n",
    "                          [5,6,7,0,0,0,0]]) % 10\n",
    "    \n",
    "    expected_result = torch.tensor(\n",
    "        [[1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000,\n",
    "         1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000,\n",
    "         1.8000, 1.8000],\n",
    "        [1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000,\n",
    "         1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000,\n",
    "         1.8000, 1.8000]]\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        local_result = model(input)\n",
    "    if torch.allclose(expected_result, local_result, rtol=0.001):\n",
    "        print(\"Passed local check\")\n",
    "    else:\n",
    "        print(f\"Test failed, expected value was\\n{expected_result}\\nbut you got:\\n{local_result}\")\n",
    "\n",
    "def self_attention_nbow_test_local_embeddings():\n",
    "    model = SelfAttentionNBOW(vocab_size=5, embedding_dim=3)\n",
    "    model.set_embedding_weight(torch.arange(15).reshape(5, 3) / 50)\n",
    "    embeddings = model.get_embeddings(torch.tensor([[1, 2, 3, 4, 1], [1, 2, 3, 0, 0]]))\n",
    "    correct_embeddings = torch.tensor([[[0.0600, 0.0800, 0.1000],\n",
    "                                        [0.1200, 0.1400, 0.1600],\n",
    "                                        [0.1800, 0.2000, 0.2200],\n",
    "                                        [0.2400, 0.2600, 0.2800],\n",
    "                                        [0.0600, 0.0800, 0.1000]],\n",
    "\n",
    "                                        [[0.0600, 0.0800, 0.1000],\n",
    "                                        [0.1200, 0.1400, 0.1600],\n",
    "                                        [0.1800, 0.2000, 0.2200],\n",
    "                                        [0.0000, 0.0200, 0.0400],\n",
    "                                        [0.0000, 0.0200, 0.0400]]])\n",
    "    if torch.allclose(embeddings, correct_embeddings, rtol=0.001):\n",
    "        print(\"Passed local embedding test\")\n",
    "    else:\n",
    "        print(f\"Embedding Test failed, expected value was\\n{correct_embeddings}\\nbut you got:\\n{embeddings}\")\n",
    "\n",
    "def self_attention_nbow_test_local_attn():\n",
    "    model = SelfAttentionNBOW(vocab_size=5, embedding_dim=3)\n",
    "    model.set_embedding_weight(torch.arange(15).reshape(5,3) / 50)\n",
    "    attention_weights = model.get_attention_matrix(torch.tensor([[1,2,3,4,1],[1,2,3,0,0]]))\n",
    "    correct_attention_weights = torch.tensor([[0.1675, 0.1921, 0.2203, 0.2526, 0.1675],\n",
    "        [0.3085, 0.3327, 0.3588, 0.0000, 0.0000]])\n",
    "    if torch.allclose(attention_weights, correct_attention_weights, rtol=0.001):\n",
    "        print(\"Passed local Attn test\")\n",
    "    else:\n",
    "        print(f\"Attn Test failed, expected value was\\n{correct_attention_weights}\\nbut you got:\\n{attention_weights}\")\n",
    "self_attention_nbow_test_local()\n",
    "self_attention_nbow_test_local_embeddings()\n",
    "self_attention_nbow_test_local_attn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ae90cc4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:14:13.399905Z",
     "start_time": "2024-09-06T14:14:13.397111Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_self_attention_model(vocab_size, embedding_dim):\n",
    "    \"\"\"\n",
    "    This function returns an instance of the Self Attention model. Initialize the Self Attention model here and return it.\n",
    "    \"\"\"\n",
    "    model = None\n",
    "    ## YOUR CODE STARTS HERE ##\n",
    "    model = SelfAttentionNBOW(vocab_size = vocab_size, embedding_dim = embedding_dim)\n",
    "    ## YOUR CODE ENDS HERE ##\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c1a348",
   "metadata": {},
   "source": [
    "#### 5.3.2. Model Training [3 points - Non-Programming]\n",
    "Assign and tune the below hyperparameters to optimize your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "135edd83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:22:31.700556Z",
     "start_time": "2024-09-06T14:22:31.695913Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# Assign hyperparameters and training parameters\n",
    "# Experiment with different values for these hyperparaters to optimize your model's performance\n",
    "def get_hyperparams_self_attn():\n",
    "    learning_rate = 0.002\n",
    "    epochs = 25\n",
    "    embedding_dim = 256\n",
    "    return learning_rate, epochs, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9671794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:23:24.581441Z",
     "start_time": "2024-09-06T14:22:34.449703Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is the main training loop. You'll need to complete the train_loop and val_loop functions.\n",
    "# You'll also need to complete the criterion and optimizer functions.\n",
    "# Feel free to experiment with different optimizers and learning rates.\n",
    "# Do not change anything else in this cell \n",
    "learning_rate, epochs, embedding_dim = get_hyperparams_self_attn()\n",
    "self_attention_model = get_self_attention_model(len(train_vocab.keys()),embedding_dim).to(device)\n",
    "criterion = get_criterion()\n",
    "optimizer = get_optimizer(self_attention_model, learning_rate)\n",
    "train_loss_over_time_sea = []\n",
    "val_loss_over_time_sea = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_loop(self_attention_model, criterion, optimizer, train_iterator, epoch, save_every=2)\n",
    "    true, pred, val_loss = val_loop(self_attention_model, criterion, val_iterator)\n",
    "    accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "    train_loss_over_time_sea.append(train_loss)\n",
    "    val_loss_over_time_sea.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1} -- Train_Loss: {train_loss} -- Val_Loss: {val_loss} -- Val_Accuracy: {accuracy} -- Val_F1: {f1}\")\n",
    "save_checkpoint(self_attention_model, 'self_attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL - retain the outputs in submission PDF for credits \n",
    "plot_loss(train_loss_over_time_sea, val_loss_over_time_sea, 'Self Attention NBOW')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4a628",
   "metadata": {},
   "source": [
    "#### 5.3.3. Model Evaluation Model Evaluation [2 points - Programming]\n",
    "The final points for this will be awarded as per Gradescope's test split, which is different from the local versions. The cell below is just for a sanity check. Your metrics here may not exactly match with the ones on Gradescope, but if your model is fairly generalized, it should not be far off.\n",
    "- 0 points for accuracy <= 85%\n",
    "- 1 point for accuracy > 85% but <= 90%\n",
    "- 2 points for accuracy > 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3be9d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:24:34.940756Z",
     "start_time": "2024-09-06T14:24:34.280144Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "learning_rate, epochs, embedding_dim = get_hyperparams_self_attn()\n",
    "self_attention_model = get_self_attention_model(len(train_vocab.keys()),embedding_dim).to(device)\n",
    "load_checkpoint(self_attention_model, 'self_attention', map_location=device)\n",
    "\n",
    "true, pred, val_loss = val_loop(self_attention_model, criterion, val_iterator)\n",
    "accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "print(f\"Final Validation Accuracy: {accuracy}\")\n",
    "print(f\"Final Validation F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7172d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "plot_confusion_matrix(true, pred, classes=id2label.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d07564",
   "metadata": {},
   "source": [
    "## 6. Perceptron and Hinge Losses (16 Points - Programming)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968228bd",
   "metadata": {},
   "source": [
    "### 6.1. Perceptron Loss (5 points - Programming)\n",
    "\n",
    "The perceptron loss penalizes the model only when the true class is not the most confident prediction. If the model incorrectly assigns a higher score to another class, the perceptron loss encourages the model to adjust its weights to fix this mistake.\n",
    "\n",
    "This is achieved by considering the difference between the maximum score among all other classes and the score of the true class. If the maximum incorrect score is higher than the true class score, the model receives a penalty proportional to how much worse the true class was predicted compared to the highest incorrect class.\n",
    "\n",
    "Given a set of predictions from the perceptron model for a batch of samples, we denote:\n",
    "\n",
    "- $ \\mathbf{y} \\in \\mathbb{R}^{B \\times C} $: The matrix of predicted scores, where $B$ is the batch size and $C$ is the number of classes. Each row represents the predicted scores for one sample.\n",
    "- $\\mathbf{y}_{\\text{true}} \\in \\{0, 1, \\ldots, C-1\\}^{B}$: The ground truth labels, where each entry is an integer representing the correct class label for each sample.\n",
    "\n",
    "For a given sample \\( i \\), let:\n",
    "\n",
    "- $s_j $ be the score for class $j$ (from the predicted score vector).\n",
    "- $ s_{\\text{true}} $ be the score for the true class.\n",
    "\n",
    "The **perceptron loss** for data instance $x_i$ is defined as:\n",
    "\n",
    "$$\n",
    "L_{\\text{perceptron}}(i) = \\max \\left( 0, \\max_{j} (s_j) - s_{\\text{true}} \\right)\n",
    "$$\n",
    "\n",
    "For batches, we compute the loss for each sample and take the mean over the batch\n",
    "\n",
    "Implement this PerceptronLoss in the forward method below.\n",
    "\n",
    "NOTE: The scores are logits, the predictions of models before doing any softmax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2acb6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class PerceptronLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptronLoss, self).__init__()\n",
    "\n",
    "    def forward(self, predictions, labels):\n",
    "        \"\"\"\n",
    "        Calculate the perceptron loss between predictions and labels.\n",
    "\n",
    "        Args:\n",
    "            predictions (torch.Tensor): The predictions from the model for a batch of inputs.\n",
    "                                        Shape should be (batch_size, num_classes).\n",
    "            labels (torch.Tensor): The ground truth labels for each input in the batch.\n",
    "                                   Shape should be (batch_size,) with each value between 0 and num_classes-1.\n",
    "\n",
    "        Returns:\n",
    "            scalar: The mean perceptron loss for the batch.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        # YOUR CODE STARTS HERE\n",
    "        true_class_scores = predictions[torch.arange(predictions.size(0)), labels]\n",
    "        max_scores, _ = torch.max(predictions, dim=1)\n",
    "        losses = torch.relu(max_scores - true_class_scores)\n",
    "        loss = torch.mean(losses)\n",
    "        # YOUR CODE ENDS HERE\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f7d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "perceptron_loss = PerceptronLoss()\n",
    "\n",
    "def test_correct_classification():\n",
    "    predictions = torch.tensor([[3.0, 2.0, 1.0],\n",
    "                                [1.0, 4.0, 2.0]])\n",
    "    labels = torch.tensor([0, 1])\n",
    "    loss = perceptron_loss(predictions, labels).item()\n",
    "    expected_loss = 0.0\n",
    "    rtol = 0.001  # Relative tolerance\n",
    "    if abs(expected_loss - loss) <= rtol * abs(expected_loss):\n",
    "        print('Test case passed for correct classification')\n",
    "    else:\n",
    "        print(f\"Test case failed for correct classification, expected value was\\n{expected_loss}\\nbut you got:\\n{loss}\")\n",
    "\n",
    "# Test for incorrect classification\n",
    "def test_incorrect_classification():\n",
    "    predictions = torch.tensor([[1.0, 3.0, 2.0],\n",
    "                                [1.0, 2.0, 4.0]])\n",
    "    labels = torch.tensor([0, 1])\n",
    "    \n",
    "    expected_loss = 2.0\n",
    "    loss = perceptron_loss(predictions, labels).item()\n",
    "    \n",
    "    rtol = 0.001  # Relative tolerance\n",
    "    if abs(expected_loss - loss) <= rtol * abs(expected_loss):\n",
    "        print('Test case passed for incorrect classification')\n",
    "    else:\n",
    "        print(f\"Test case failed for incorrect classification, expected value was\\n{expected_loss}\\nbut you got:\\n{loss}\")\n",
    "\n",
    "# Execute test cases\n",
    "test_correct_classification()\n",
    "test_incorrect_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e23769",
   "metadata": {},
   "source": [
    "### 6.2. NBOW Training using Perceptron Loss (3 points - Programming)\n",
    "\n",
    "Credits will be awarded as per the following final results on the Gradescope split -\n",
    "- 0 points for accuracy <= 75%\n",
    "- 1 point for accuracy > 75% but <= 80% \n",
    "- 2 points for accuracy > 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5cc940",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is the main training loop. You'll need to complete the train_loop and val_loop functions.\n",
    "# You'll also need to complete the criterion and optimizer functions.\n",
    "# Feel free to experiment with different optimizers and learning rates.\n",
    "# Do not change anything else in this cell\n",
    "learning_rate, epochs, embedding_dim = get_hyperparams_nbow()\n",
    "nbow_model = get_nbow_model(vocab_size= len(train_vocab.keys()), embedding_dim = embedding_dim).to(device)\n",
    "criterion = PerceptronLoss()\n",
    "optimizer = get_optimizer(nbow_model, learning_rate)\n",
    "train_loss_over_time_perceptron = []\n",
    "val_loss_over_time_perceptron = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_loop(nbow_model, criterion, optimizer, train_iterator, epoch, save_every=2)\n",
    "    true, pred, val_loss = val_loop(nbow_model, criterion, val_iterator)\n",
    "    accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "    train_loss_over_time_perceptron.append(train_loss)\n",
    "    val_loss_over_time_perceptron.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1} -- Train_Loss: {train_loss} -- Val_Loss: {val_loss} -- Val_Accuracy: {accuracy} -- Val_F1: {f1}\")\n",
    "    \n",
    "save_checkpoint(nbow_model, 'nbow', loss_fn='perceptron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266b528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "plot_loss(train_loss_over_time_perceptron, val_loss_over_time_perceptron, 'NBOW with Perceptron Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4931b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# load best model from checkpoint \n",
    "learning_rate, epochs, embedding_dim = get_hyperparams_nbow()\n",
    "nbow_model = get_nbow_model(vocab_size= len(train_vocab.keys()), embedding_dim = embedding_dim).to(device)\n",
    "load_checkpoint(nbow_model, 'nbow', 'perceptron', map_location=device)\n",
    "\n",
    "# evaluate model \n",
    "true, pred, val_loss = val_loop(nbow_model, criterion, val_iterator)\n",
    "accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "print(f\"Final Validation Accuracy: {accuracy}\")\n",
    "print(f\"Final Validation F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6fbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "plot_confusion_matrix(true, pred, classes=id2label.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35442a25",
   "metadata": {},
   "source": [
    "### 6.3. Hinge Loss (5 points - Programming)\n",
    "\n",
    "Read through the dataset documentation link provided with this notebook, or feel free to Google and read about 20 Newsgroups dataset, it's quite popular. If you try to understand the labels, they are more similar to some of them than others, semantically. For example, `talk.politics.mideast` is more closer to `talk.politics.misc` than `sci.space`. \n",
    "\n",
    "Mathematically speaking, it means that misclassification of a label to some classes may be less penalizable than some other classes. This is the perfect scenario of using hinge loss.\n",
    "\n",
    "The **hinge loss** is another loss function commonly used for classification, especially in **support vector machines (SVMs)**. It is designed to maximize the margin between the decision boundary and the closest data points from each class.\n",
    "\n",
    "The hinge loss penalizes predictions based on how confident the model is about the correct class relative to other classes. It aims to push the score of the true class far above the scores of all other classes, ensuring that the model not only predicts the correct class but does so confidently.\n",
    "\n",
    "For each input sample $x_i$, the model computes a score for each class. The hinge loss compares the score for the true class to the scores for all other classes and penalizes the model if the true class score is not sufficiently higher than the scores for the other classes.\n",
    "\n",
    "Given a set of predictions from the model for a batch of samples, we denote:\n",
    "\n",
    "- $\\mathbf{y} \\in \\mathbb{R}^{B \\times C}$: The matrix of predicted scores, where $B$ is the batch size and $C$ is the number of classes. Each row represents the predicted scores for one sample.\n",
    "- $\\mathbf{y}_{\\text{true}} \\in \\{0, 1, \\ldots, C-1\\}^{B}$: The ground truth labels, where each entry is an integer representing the correct class label for each sample.\n",
    "\n",
    "For a given sample \\( i \\), let:\n",
    "\n",
    "- $s_j$ be the score for class $j$ (from the predicted score vector).\n",
    "- $s_{\\text{true}}$ be the score for the true class.\n",
    "- $l(j, true)$ be the cost if a task instance belonging to $true$ has highest score for $j$\n",
    "\n",
    "The **hinge loss** for a task instance \\( x_i \\) is defined as:\n",
    "\n",
    "$$\n",
    "L_{\\text{hinge}}(i) = \\max \\left( 0, \\max_j\\left(s_j + l(j, true)\\right) - s_{\\text{true}}\\right)\n",
    "$$\n",
    "\n",
    "For batches, we compute the loss for each sample and take the mean over the batch to obtain a scalar value representing the average hinge loss.\n",
    "\n",
    "HINT: The non-recommended solution is to use one loop. However, it is highly recommended to not do that for efficiency reasons. `torch.gather()` should be helpful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c04003c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class HingeLoss(nn.Module):\n",
    "    def __init__(self, cost_matrix, device):\n",
    "        super(HingeLoss, self).__init__()\n",
    "        \"\"\"\n",
    "        cost_matrix is a 2D list. Convert it to a tensor on appropriate device.\n",
    "        \"\"\"\n",
    "        # YOUR CODE STARTS HERE\n",
    "        self.cost_matrix = torch.tensor(cost_matrix, device=device)\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "    def forward(self, predictions, labels):\n",
    "        \"\"\"\n",
    "        Calculate the hinge loss between predictions and labels, adjusting for cost.\n",
    "\n",
    "        Args:\n",
    "            predictions (torch.Tensor): The predictions from the model for a batch of inputs.\n",
    "                                        Shape should be (batch_size, num_classes).\n",
    "            labels (torch.Tensor): The ground truth labels for each input in the batch.\n",
    "                                   Shape should be (batch_size,) with each value between 0 and num_classes-1.\n",
    "\n",
    "        Returns:\n",
    "            scalar: The mean hinge loss for the batch, adjusted for the defined cost.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        # YOUR CODE STARTS HERE\n",
    "        batch_size, num_classes = predictions.shape\n",
    "        cost_adjusted_predictions = predictions + self.cost_matrix[labels.unsqueeze(1), torch.arange(num_classes)]\n",
    "        true_class_scores = cost_adjusted_predictions.gather(1, labels.unsqueeze(1)).squeeze(1)\n",
    "        max_scores, _ = torch.max(cost_adjusted_predictions, dim=1)\n",
    "        losses = torch.relu(max_scores - true_class_scores)\n",
    "        loss = torch.mean(losses)\n",
    "        # YOUR CODE ENDS HERE\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa5891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "def test_correct_classification(hinge_loss):\n",
    "    \"\"\"\n",
    "    Test case where the true class has the highest score.\n",
    "    The loss should be 0.\n",
    "    \"\"\"\n",
    "    rtol = 0.0001\n",
    "    predictions = torch.tensor([[3.0, 2.0, 1.0], \n",
    "                                [1.0, 4.0, 2.0]])  \n",
    "    labels = torch.tensor([0, 1])  \n",
    "    loss = hinge_loss(predictions, labels).item()\n",
    "    expected_loss = 0.0\n",
    "    if abs(expected_loss - loss) <= rtol * abs(expected_loss):\n",
    "        print('Test case 1 passed')\n",
    "    else:\n",
    "        print(f\"Test case 1 failed, expected value was\\n{expected_loss}\\nbut you got:\\n{loss}\")\n",
    "    \n",
    "def test_incorrect_classification(hinge_loss):\n",
    "    \"\"\"\n",
    "    Test case where the true class does not have the highest score.\n",
    "    The loss should be greater than 0.\n",
    "    \"\"\"\n",
    "    rtol = 0.0001\n",
    "    predictions = torch.tensor([[1.0, 3.0, 2.0], \n",
    "                                [1.0, 2.0, 4.0]])  \n",
    "    labels = torch.tensor([0, 1])  \n",
    "\n",
    "    expected_loss = 3.0\n",
    "    loss = hinge_loss(predictions, labels).item()\n",
    "    \n",
    "    if abs(expected_loss - loss) <= rtol * abs(expected_loss):\n",
    "        print('Test case 2 passed')\n",
    "    else:\n",
    "        print(f\"Test case 2 failed, expected value was\\n{expected_loss}\\nbut you got:\\n{loss}\")\n",
    "        \n",
    "cost_matrix = [[0, 1, 1], [1, 0, 1], [1, 1, 0]]\n",
    "hinge_loss = HingeLoss(costslack_matrix, device='cpu')\n",
    "test_correct_classification(hinge_loss)\n",
    "test_incorrect_classification(hinge_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af004c",
   "metadata": {},
   "source": [
    "### 6.4. NBOW Training using Hinge Loss [3 points - Programming]\n",
    "\n",
    "First, define a cost matrix. Take inspiration from the confusion matrix of `PerceptronLoss` results, `id2label` map and your knowledge of what dataset labels are. This will help is constructing a good cost matrix.\n",
    "\n",
    "Credits will be awarded on the following cutoffs on Gradescope split -\n",
    "- 0 points for accuracy <= 84%, \n",
    "- 1 point for accuracy > 84% but <= 88%, \n",
    "- 2 points for accuracy > 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "be1af43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_cost_matrix(num_classes=20):\n",
    "    \"\"\"\n",
    "    Generates a cost matrix for a specified number of classes using Python lists.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): The number of classes for which the cost matrix is to be created.\n",
    "\n",
    "    Returns:\n",
    "        list of lists: A 2D list where element (i, j) is the absolute difference between i and j,\n",
    "                       set to zero if i equals j.\n",
    "    \"\"\"\n",
    "    cost_matrix = None\n",
    "    # YOUR CODE STARTS HERE\n",
    "    cost_matrix = [[0] * num_classes for _ in range(num_classes)]\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            if i != j:\n",
    "                cost_matrix[i][j] = abs(i - j)\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return cost_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "def test_cost(matrix):\n",
    "    n = len(matrix)\n",
    "    \n",
    "    # Check if the matrix is square\n",
    "    for row in matrix:\n",
    "        if len(row) != n:\n",
    "            print('Incorrect cost matrix: Not a square matrix.')\n",
    "    \n",
    "    # Check for symmetry and zero diagonal elements\n",
    "    for i in range(n):\n",
    "        if matrix[i][i] != 0:\n",
    "            print('Incorrect cost matrix: Diagonal elements are not zero.')\n",
    "        \n",
    "        for j in range(i + 1, n):\n",
    "            if matrix[i][j] != matrix[j][i]:\n",
    "                print('Incorrect cost matrix: Not a symmetric matrix.')\n",
    "    \n",
    "    print('Valid cost matrix')\n",
    "\n",
    "cost_matrix = get_cost_matrix()\n",
    "test_cost(cost_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d2dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the main training loop. You'll need to complete the train_loop and val_loop functions.\n",
    "# You'll also need to complete the criterion and optimizer functions.\n",
    "# Feel free to experiment with different optimizers and learning rates.\n",
    "# Do not change anything else in this cell\n",
    "learning_rate, epochs, embedding_dim = get_hyperparams_nbow()\n",
    "nbow_model = get_nbow_model(vocab_size= len(train_vocab.keys()), embedding_dim = embedding_dim).to(device)\n",
    "cost_matrix = get_cost_matrix()\n",
    "criterion = HingeLoss(cost_matrix, device=device)\n",
    "optimizer = get_optimizer(nbow_model, learning_rate)\n",
    "train_loss_over_time_hinge = []\n",
    "val_loss_over_time_hinge = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_loop(nbow_model, criterion, optimizer, train_iterator, epoch, save_every=2)\n",
    "    true, pred, val_loss = val_loop(nbow_model, criterion, val_iterator)\n",
    "    accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "    train_loss_over_time_hinge.append(train_loss)\n",
    "    val_loss_over_time_hinge.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1} -- Train_Loss: {train_loss} -- Val_Loss: {val_loss} -- Val_Accuracy: {accuracy} -- Val_F1: {f1}\")\n",
    "    \n",
    "save_checkpoint(nbow_model, 'nbow', loss_fn='hinge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a52fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "plot_loss(train_loss_over_time_hinge, val_loss_over_time_hinge, 'NBOW with Hinge Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf8cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# load best model from checkpoint \n",
    "learning_rate, epochs, embedding_dim = get_hyperparams_nbow()\n",
    "nbow_model = get_nbow_model(vocab_size= len(train_vocab.keys()), embedding_dim = embedding_dim).to(device)\n",
    "load_checkpoint(nbow_model, 'nbow', 'hinge', map_location=device)\n",
    "\n",
    "# evaluate model \n",
    "true, pred, val_loss = val_loop(nbow_model, criterion, val_iterator)\n",
    "accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "print(f\"Final Validation Accuracy: {accuracy}\")\n",
    "print(f\"Final Validation F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5289b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "plot_confusion_matrix(true, pred, classes=id2label.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aea6785",
   "metadata": {},
   "source": [
    "## 7. Analysis [21 Points - Non-programming]\n",
    "\n",
    "These are some analytical questions based on implementations done above.\n",
    "\n",
    "Note for all analysis questions: Be sure to isolate all your code/textual answers into separate cells without modifying code in other exported functions as they are still used for grade scope test cases. Feel free to add as many code and markdown cells as you see fit to explain your answer.\n",
    "\n",
    "Code should be in code cells and write-ups should strictly be in markdown cells. Please note, these will be manually evaluated due to large variation in possible answers. So, visibility of code, explanation and output in the PDF is the key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2d5e7",
   "metadata": {},
   "source": [
    "### 7.1. Analyzing NBOW Weights [7 points - Non-programming]\n",
    "\n",
    "Load your trained `NBOW` model here, and let $w$ be the weight of your linear layer of the model. It will be of the shape of `(num_classes, embedding_dim)`.\n",
    "\n",
    "For this tensor, compute $ww^T$ and show it as a heatmap (a sample code to generate heatmap is shown below). \n",
    "\n",
    "Explain the generated output. What does it resemble? What do high and low values of coefficients at position i, j indicate? With the help of the dataset documentation and `id2label` map displayed earlier in this notebook, can you reason why certain values are high and why certain values are low? What does it tell you about the class labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0a33681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Example Heatmap Generation Code\n",
    "# matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# # Display heatmap using Matplotlib\n",
    "# plt.imshow(matrix, cmap='hot', interpolation='nearest')\n",
    "# plt.colorbar()  # Add a color bar to show the scale\n",
    "# plt.title('Heatmap using Matplotlib')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f308afec",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4df34b1",
   "metadata": {},
   "source": [
    "### 7.2. Word Embeddings and the Attention Vector [7 points - Non Programming]\n",
    "\n",
    "From your trained `SimpleAttentionNBOW` model, analyze all word embeddings and the attention vector `u`. Look at the words which have the highest cosine similarity with `u`. Print the 15 words with highest cosine similarity to `u` and the 15 with lowest cosine similarity to `u`. Why do you think those words have high/low cosine similarity to `u` (and therefore high/low attention weights on average)? Form a hypothesis to explain what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d019448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03afff04",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87948930",
   "metadata": {},
   "source": [
    "### 7.3. Analysis of Cost Matrix in Hinge Loss [7 points - Non-programming]\n",
    "\n",
    "Display the confusion matrices of prediction of `NBOW` with `PerceptronLoss` and `HingeLoss`. Also print the cost matrix you used for the Hinge Loss. \n",
    "\n",
    "Using this confusion matrix, `id2label` mapping provided earlier, your knowledge about what labels in the dataset represent, explain the motivation behind creating the provided cost matrix. Why were some of the coefficients higher than others (if)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850dd25e",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359d26e4a7fc359d",
   "metadata": {},
   "source": [
    "## 8. Improving Attention Models [BONUS] [10 points - Non-programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b09c0394963ab9",
   "metadata": {},
   "source": [
    "Hopefully youve developed some intuition for using attention for this task. Now, come up with your own ways of modifying the attention function and experiment with them. Can you find an idea that outperforms your models from Sections 5?\n",
    "Some potential ideas are below:\n",
    "- Use transformation matrices to distinguish key, query, and value representations\n",
    "- Add additional layers of self-attention before the attention-weighted sum of embeddings\n",
    "- Compute features in the attention function based on characteristics of where the word is in the sentence, e.g., features of the sentence length, nearby words, the presence of negation words before or after the word, information from a part-of-speech tagger or syntactic parse of the sentence, etc.\n",
    "- Use multiple word embedding spaces for when words are used as keys, queries, and values, or some subset of the three.\n",
    "\n",
    "Describe your best new attention function formally below, along with the execution code and experimental results. Add as many code and markdown cells as you want, and submit the complete working with explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db6978",
   "metadata": {},
   "source": [
    "## 9. Submitting Your Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e93176",
   "metadata": {},
   "source": [
    "This is the end. Congratulations!  \n",
    "\n",
    "Now, follow the steps below to submit your homework on Gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e47daf5",
   "metadata": {},
   "source": [
    "### 9.1. Programming\n",
    "\n",
    "The programming will be evaluated through an autograder. To create the file to submit for autograder, follow the steps below -\n",
    "1. Open a terminal from the root directory of the project\n",
    "2. Run the collect_submission.py file\n",
    "3. Agree to the Late Policy and Honor Pledge\n",
    "4. After the file is executed, your root project will have a submission directory.\n",
    "5. Submit all the contents of this file to GradeScope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13221269",
   "metadata": {},
   "source": [
    "### 9.2. Non-Programming\n",
    "\n",
    "The analysis parts will be evaluated manually. For this, export the notebook to a PDF file, and submit it on GradeScope. Please ensure no written code or output is clipped when you create your PDF. One reliable way to do it is first download it as HTML through Jupyter Notebook and then print it to get PDF."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
