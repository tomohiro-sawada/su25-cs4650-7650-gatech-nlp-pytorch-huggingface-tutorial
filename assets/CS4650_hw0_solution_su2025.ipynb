{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ffbb14288fe2c51",
   "metadata": {},
   "source": [
    "# CS 4650 - Natural Language Processing - HW - 0 \n",
    "Georgia Tech, Summer 2025 (Instructor: Kartik Goyal)\n",
    "\n",
    "Welcome to the first full programming assignment for CS 4650! \n",
    "\n",
    "In this assignment, you will be implementing different evaluation methodologies for Large Language Models (LLMs) across three key tasks: Multiple Choice Questions (MCQ), Machine Translation (MT), and Short Story Generation (SSG). This assignment will cover fundamental evaluation metrics, prompt engineering techniques, and analysis of model performance across different domains.\n",
    "\n",
    "DO NOT CHANGE the names of any of the files and contents outside the cells where you have to write code.\n",
    "\n",
    "NOTE: DO NOT USE ANY OTHER EXTERNAL LIBRARIES FOR THIS ASSIGNMENT\n",
    "\n",
    "<!-- TODO: add deadlines -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SiFjTT4jFEIW",
   "metadata": {
    "id": "SiFjTT4jFEIW"
   },
   "source": [
    "The assignment is broken down into 6 Sections. The sections are as follows:\n",
    "\n",
    "| Section | Part                                      | Points |\n",
    "|---------|-------------------------------------------|--------|\n",
    "\n",
    "<!-- TODO: assign points appropriately. -->\n",
    "\n",
    "\n",
    "\n",
    "All the best and happy coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e26764d",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Check what version of Python is running\n",
    "import sys \n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32e55cf463e82c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries - DO NOT CHANGE THIS CELL\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, List, Sequence, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e52d8b77a802b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining global constants - DO NOT CHANGE THESE VALUES\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "PADDING_VALUE = 0\n",
    "UNK_VALUE     = 1\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb7e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how we select a GPU if it's available on your computer or in the Colab environment.\n",
    "print('Device of execution - ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83360462",
   "metadata": {},
   "source": [
    "## 1. Utility Classes [15 points - Programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0Fulh0MZ8y8b",
   "metadata": {},
   "source": [
    "### 1.1. LLM Generation Configuration [0 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b4373",
   "metadata": {},
   "source": [
    "The following cell defines a configuration class for LLM generation. This is so that it is easier to keep track of the parameters for the generation in later parts of the notebook.\n",
    "\n",
    "\n",
    "Here are the generation hyperparameters we will be using: \n",
    "\n",
    "- temperature: Controls randomness in token selection. A value of 0 means deterministic (always pick highest probability token), while higher values (e.g., 0.7-1.0) increase randomness. Lower temperature produces more focused/predictable text, higher temperature produces more diverse/creative text. \n",
    "\n",
    "- top_p (nucleus sampling): Only considers tokens whose cumulative probability exceeds a threshold p.\n",
    "  For example, if top_p=0.9, the model selects from the smallest set of tokens whose cumulative probability reaches 90%.\n",
    "  This helps balance between diversity and quality by dynamically adjusting the candidate pool.\n",
    "\n",
    "- top_k: Limits token selection to only the k highest probability tokens. For example, if top_k=40,\n",
    "  only the 40 most likely next tokens are considered. This prevents the model from selecting very low probability tokens.\n",
    "\n",
    "- max_new_tokens: Sets the maximum length of generated text by limiting the number of tokens the model will produce.\n",
    "  This prevents excessively long outputs and controls computational resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ctNnE1Ui8oKw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "@dataclass\n",
    "class LLMGenerationConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for LLM generation parameters.\n",
    "    \n",
    "    Args:\n",
    "        temperature (float): Controls randomness in sampling. Higher values make output more random.\n",
    "        top_p (float): Nucleus sampling parameter. Only tokens with cumulative probability <= top_p are considered.\n",
    "        top_k (int): Only the top k tokens are considered for sampling.\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "    \"\"\"\n",
    "    temperature: float = 0.7\n",
    "    top_p: Optional[float] = 0.95\n",
    "    top_k: Optional[int] = 40\n",
    "    max_new_tokens: int = 100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MiUlTSBB9Wx6",
   "metadata": {},
   "source": [
    "### 1.2. LLM Wrapper Class [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe1b130",
   "metadata": {},
   "source": [
    "In the following cell, implement a wrapper around the Huggingface transformers API so that we can run inference on the model. \n",
    "\n",
    "You need to implement the following: \n",
    "\n",
    "- Get the tokenizer and model from Huggingface and assign it to the `tokenizer` and `model` attributes.\n",
    "- Implement the `generate()` method to run inference on the model. We need this for generating the completions from the LLMs. \n",
    "- Implement the `logits()` method to get the logits for the next token. We will need this for MCQ task.  \n",
    "- Implement the `perplexity()` method to get the perplexity of the model. We will need this when we need this for evaluating the SSG task. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vqtdrhF8FEIZ",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    \"\"\"\n",
    "    A wrapper class for Hugging Face language models that provides a unified interface\n",
    "    for text generation, logit computation, and perplexity calculation.\n",
    "    \n",
    "    If transformers library is not available, falls back to deterministic stubs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_id: str = \"gpt2\", device: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the LLM wrapper.\n",
    "        \n",
    "        Args:\n",
    "            hf_id (str): Hugging Face model identifier\n",
    "            device (str): Device to load model on ('cuda', 'cpu', 'mps')\n",
    "        \"\"\"      \n",
    "        self.hf_id = hf_id\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Use auto-loading with device_map=\"auto\" for faster loading and automatic memory management\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(hf_id)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            hf_id,\n",
    "            device_map=\"auto\",  # Automatically determine optimal device mapping\n",
    "            torch_dtype=torch.float16  # Use half precision for faster loading and less memory\n",
    "        ).eval()\n",
    "                \n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, prompt: str, config: LLMGenerationConfig = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate text continuation for the given prompt using the underlying language model.\n",
    "        \n",
    "        This method takes a text prompt and generates additional text that continues from\n",
    "        the prompt in a coherent manner. The generation process can be controlled through\n",
    "        various parameters specified in the config object.\n",
    "        \n",
    "        The method tokenizes the input prompt, passes it through the model, and then\n",
    "        decodes the generated token IDs back to text, excluding the original prompt tokens.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): Input text prompt that the model will continue from\n",
    "            config (LLMGenerationConfig): Configuration object containing generation parameters\n",
    "                such as temperature, top_p, top_k, and max_new_tokens. If None, default\n",
    "                parameters will be used.\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated text continuation without the original prompt. The text is\n",
    "                stripped of leading/trailing whitespace and special tokens are removed\n",
    "                during decoding.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            # Deterministic stub - always returns \"A\" for MCQ compatibility\n",
    "            return \"A\"\n",
    "\n",
    "        config = config or LLMGenerationConfig()\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "            \n",
    "        if config.temperature > 0:\n",
    "            output = self.model.generate(\n",
    "                input_ids,\n",
    "                do_sample=True, \n",
    "                temperature=config.temperature,\n",
    "                top_p=config.top_p,\n",
    "                top_k=config.top_k,\n",
    "                max_length=input_ids.shape[1] + config.max_new_tokens,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "        else:\n",
    "            output = self.model.generate(\n",
    "                input_ids,\n",
    "                do_sample=False, \n",
    "                max_length=input_ids.shape[1] + config.max_new_tokens,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        generated = self.tokenizer.decode(\n",
    "            output[0, input_ids.shape[1]:], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        return generated.strip()\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def logits(self, prompt: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get next-token logits for the given prompt.\n",
    "        \n",
    "        This method computes and returns the logits (raw, unnormalized prediction scores) \n",
    "        for the next token that would follow the given prompt. These logits represent the model's\n",
    "        prediction distribution over the entire vocabulary for the next token position.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): Input text prompt for which to compute next-token predictions\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of shape (vocab_size,) containing the logits for each\n",
    "                possible next token in the vocabulary. Higher values indicate tokens the\n",
    "                model considers more likely to follow the prompt.\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.model(**tokens)\n",
    "        # Return logits for the last token position\n",
    "        return outputs.logits[0, -1].cpu()\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def perplexity(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate perplexity of the given text.\n",
    "        \n",
    "        Perplexity is a measurement of how well a probability model predicts a sample.\n",
    "        Lower perplexity indicates the model is better at predicting the text.\n",
    "        It is calculated as the exponentiated average negative log-likelihood of a sequence.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text for which to calculate perplexity\n",
    "        Returns:\n",
    "            float: Perplexity value\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            return 100.0  # Fixed stub value\n",
    "\n",
    "        encodings = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encodings)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # Shift logits and labels for next-token prediction\n",
    "        shift_logits = logits[:, :-1].contiguous()\n",
    "        shift_labels = encodings.input_ids[:, 1:].contiguous()\n",
    "        \n",
    "        # Calculate cross-entropy loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "        loss = loss_fct(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)), \n",
    "            shift_labels.view(-1)\n",
    "        )\n",
    "        \n",
    "        return np.exp(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dbd769",
   "metadata": {},
   "source": [
    "In the following cells, let's load two different LLMs and test each of the methods we implemented in the previous cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf8229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "llama = LLM(hf_id=\"meta-llama/Llama-3.1-8B-Instruct\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79963dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "prompt = \"Hello, how are you?\"\n",
    "\n",
    "# Greedy decoding \n",
    "generation_config = LLMGenerationConfig(\n",
    "    temperature=0,\n",
    "    top_p=None,\n",
    "    top_k=None,\n",
    "    max_new_tokens=10\n",
    ")\n",
    "\n",
    "# Let's test the generate method using greedy decoding \n",
    "assert llama.device.type == \"cuda\", \"Device is not loaded to cuda\"\n",
    "assert llama.generate(prompt, generation_config) == \"I am doing well, thanks for asking. I\", \"Greedy decoding is incorrect\"\n",
    "# TODO: does temp=0 case return the same outputs even with different hardware? \n",
    "# TODO: how to test temp > 0 case? \n",
    "# TODO: make sure device is loaded to cuda \n",
    "\n",
    "\n",
    "assert torch.argmax(llama.logits(prompt)) == 358, \"Logit is incorrect\"\n",
    "# assert llama.perplexity(prompt) == 1.0, \"Perplexity is incorrect\"\n",
    "assert np.isclose(llama.perplexity(prompt), 17.969428099556087, atol=1e-1), \"Perplexity is incorrect\"\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2b7ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# qwen = LLM(hf_id=\"Qwen/Qwen2.5-7B-Instruct\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb01b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# # Let's test the generate method using greedy decoding \n",
    "# assert qwen.device.type == \"cuda\", \"Device is not loaded to cuda\"\n",
    "# assert qwen.generate(prompt, generation_config) == \"I'm sorry, but as a language model,\", \"Greedy decoding is incorrect\"\n",
    "# assert torch.argmax(qwen.logits(prompt)) == 358, \"Logit is incorrect\"\n",
    "# assert np.isclose(qwen.perplexity(prompt), 7.505416801107283, atol=1e-1), \"Perplexity is incorrect\"\n",
    "# print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b94263",
   "metadata": {},
   "source": [
    "### 1.3. Embedding Model [4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50ff39e",
   "metadata": {},
   "source": [
    "Next, we will implement a wrapper around the Huggingface SentenceTransformer API for generating embeddings. \n",
    "Embedding models convert text into dense vector representations (embeddings) that capture semantic meaning.\n",
    "These vectors allow us to measure similarity between texts in a high-dimensional space.\n",
    "#\n",
    "Key points about embedding models:\n",
    "1. They transform variable-length text inputs into fixed-dimension vectors\n",
    "2. They have a maximum context length, so longer inputs will be truncated\n",
    "3. Similar texts will have embeddings that are close to each other in the vector space\n",
    "#\n",
    "We'll use these embeddings when evaluating LLM outputs based on semantic similarity rather than\n",
    "exact string matching, which is particularly useful for tasks like machine translation and\n",
    "short story generation where multiple valid outputs are possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "532ebf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel:\n",
    "    \"\"\"\n",
    "    A wrapper around the Huggingface SentenceTransformer API for generating embeddings.\n",
    "    This model creates semantic embeddings that can be used for measuring similarity\n",
    "    between texts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_id: str = \"sentence-transformers/all-MiniLM-L6-v2\", dim: int = None):\n",
    "        \"\"\"\n",
    "        Initialize the embedding model with the specified model ID.\n",
    "        \n",
    "        Args:\n",
    "            hf_id (str): Hugging Face model ID for the SentenceTransformer model.\n",
    "                         Default is \"sentence-transformers/all-MiniLM-L6-v2\".\n",
    "            dim (int): Not used for SentenceTransformer models as the dimension is\n",
    "                       determined by the model itself, but kept for API compatibility.\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(hf_id,  trust_remote_code=True)\n",
    "        self.dim = self.model.get_sentence_embedding_dimension()\n",
    "\n",
    "    def embed(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create an embedding for the given text using the SentenceTransformer model.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text to embed. Can be of any length.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Embedding vector representing the semantic content of the input text.\n",
    "        \"\"\"\n",
    "        # SentenceTransformer returns numpy array, convert to torch tensor\n",
    "        embedding = self.model.encode(text, convert_to_tensor=True)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7753b",
   "metadata": {},
   "source": [
    "LaBSE (Language-agnostic BERT Sentence Embedding) is a multilingual embedding model specifically trained for machine translation tasks. It can encode sentences from 109 different languages into a shared embedding space, allowing for effective cross-lingual similarity comparison. This makes it particularly useful for evaluating machine translation outputs by measuring semantic similarity between translations and references, rather than relying on exact string matching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c821131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL \n",
    "\n",
    "labse = EmbeddingModel(hf_id=\"sentence-transformers/LaBSE\")\n",
    "\n",
    "\n",
    "embedding1 = labse.embed(\"Hello, how are you?\")\n",
    "embedding2 = labse.embed(\"Goodbye, see you later!\")\n",
    "\n",
    "\n",
    "assert embedding1.shape == embedding2.shape\n",
    "assert embedding1.shape == (768,)\n",
    "assert embedding2.shape == (768,)\n",
    "\n",
    "assert torch.isclose(torch.cosine_similarity(embedding1, embedding2, dim=0), torch.tensor(0.4207), atol=1e-1)\n",
    "assert torch.isclose(torch.cosine_similarity(embedding1, embedding1, dim=0), torch.tensor(1.0), atol=1e-3)\n",
    "assert torch.isclose(torch.cosine_similarity(embedding2, embedding2, dim=0), torch.tensor(1.0), atol=1e-3)\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a619cd58",
   "metadata": {},
   "source": [
    "The following is an embedding model from Alibaba's GTE (General Text Embedding) family. \n",
    "It is a multilingual embedding model supporting 70 languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddbfa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL \n",
    "\n",
    "gte = EmbeddingModel(hf_id=\"Alibaba-NLP/gte-multilingual-base\")\n",
    "\n",
    "\n",
    "# TODO: implement test.\n",
    "\n",
    "embedding1 = gte.embed(\"Hello, how are you?\")\n",
    "embedding2 = gte.embed(\"Goodbye, see you later!\")\n",
    "\n",
    "assert embedding1.shape == embedding2.shape\n",
    "assert embedding1.shape == (768,)\n",
    "assert embedding2.shape == (768,)\n",
    "\n",
    "assert torch.isclose(torch.cosine_similarity(embedding1, embedding2, dim=0), torch.tensor(0.5835), atol=1e-1)\n",
    "assert torch.isclose(torch.cosine_similarity(embedding1, embedding1, dim=0), torch.tensor(1.0), atol=1e-3)\n",
    "assert torch.isclose(torch.cosine_similarity(embedding2, embedding2, dim=0), torch.tensor(1.0), atol=1e-3)\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GDI72x8XFEIc",
   "metadata": {},
   "source": [
    "## 2. Multiple Choice Questions (MCQ) Evaluation [20 points - Programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af0dd78",
   "metadata": {},
   "source": [
    "In this secton, we'll be evaluating LLMs on the [MMLU (Massive Multitask Language Understanding) benchmark](https://arxiv.org/abs/2009.03300), a multiple choice question benchmark. It covers a wide range of subjects including mathematics, history, computer science, law, and more, making it a comprehensive test of an LLM's knowledge and reasoning abilities.\n",
    "\n",
    "We'll implement two different evaluation approaches:\n",
    "\n",
    "1. **Regex-based Evaluation**: A simple approach that extracts answers from model outputs using regular expressions, then compares them to ground truth answers.\n",
    "\n",
    "2. **Logit-based Evaluation**: A more sophisticated approach that directly accesses the model's internal probability distributions (logits) to determine which answer choice the model predicts as most likely.\n",
    "\n",
    "Each approach has its advantages and limitations, which we'll explore throughout this section.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zeo9kX6i9pbH",
   "metadata": {},
   "source": [
    "### 2.1. Helper Functions [3 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dee26a",
   "metadata": {},
   "source": [
    "In this section, we define some useful helper functions for evaluating the LLMs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c57877",
   "metadata": {},
   "source": [
    "N-grams are contiguous sequences of n items from a given sample of text or speech. \n",
    "For example, in the sentence \"I love natural language processing\":\n",
    "- 1-grams (unigrams): [\"I\", \"love\", \"natural\", \"language\", \"processing\"]\n",
    "- 2-grams (bigrams): [(\"I\", \"love\"), (\"love\", \"natural\"), (\"natural\", \"language\"), (\"language\", \"processing\")]\n",
    "- 3-grams (trigrams): [(\"I\", \"love\", \"natural\"), (\"love\", \"natural\", \"language\"), (\"natural\", \"language\", \"processing\")]\n",
    "#\n",
    "N-grams are useful for various NLP tasks including:\n",
    "- Language modeling: Predicting the next word based on previous words\n",
    "- Text similarity: Comparing documents based on shared n-grams\n",
    "- Machine translation evaluation: Metrics like BLEU use n-gram overlap\n",
    "#\n",
    "Implement the function below which extracts all possible n-grams from a sequence of tokens. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae2129fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ngrams(tokens: Sequence[str], n: int) -> List[Tuple[str, ...]]:\n",
    "    \"\"\"\n",
    "    Extract n-grams from a sequence of tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens (Sequence[str]): A sequence of tokens (words, characters, etc.)\n",
    "        n (int): The size of each n-gram\n",
    "        \n",
    "    Returns:\n",
    "        List[Tuple[str, ...]]: A list of tuples, where each tuple contains n consecutive tokens\n",
    "                              from the input sequence\n",
    "    \"\"\"\n",
    "    return [tuple(tokens[i : i + n]) for i in range(len(tokens) - n + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e50b95",
   "metadata": {},
   "source": [
    "### 2.2. Regex-based Accuracy [4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e55f04",
   "metadata": {},
   "source": [
    "In the following cell, implement a function which extracts the first letter generated by the LLM and compares it with the reference answer. We implement this using a regular expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fCFfEHv1hnI",
   "metadata": {},
   "outputs": [],
   "source": [
    "_mcq_regex = re.compile(r\"\\b([A-D])\\b\", re.IGNORECASE)\n",
    "\n",
    "def compute_regex_accuracy(generation: str, reference: str) -> float:\n",
    "    \"\"\"\n",
    "    Extract the first letter A-D from the generation and compare with reference.\n",
    "    \n",
    "    Args:\n",
    "        generation (str): Generated text from the model\n",
    "        reference (str): Ground truth answer (A, B, C, or D)\n",
    "        \n",
    "    Returns:\n",
    "        float: 1.0 if correct, 0.0 if incorrect\n",
    "    \"\"\"\n",
    "    match = _mcq_regex.search(generation)\n",
    "    pred = match.group(1).upper() if match else None\n",
    "    return float(pred == reference.strip().upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "3bb94a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement test. Have a dummy generated text, feed it into the function, and see if it matches the reference answer correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8-qTQQa2FEIe",
   "metadata": {},
   "source": [
    "### 2.3. Logit-based Accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1382c3",
   "metadata": {},
   "source": [
    "In the following cell, implement a function which calculates the accuracy based on the logits for the first token generated by the LLM.\n",
    "That is, calculate the most likely token based on the model's logits and compare it with the reference answer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "tqt9q92J1QKK",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logit_accuracy(logits: torch.Tensor, reference: str, model: LLM) -> float:\n",
    "    \"\"\"\n",
    "    Compute accuracy based on logits for the first token generated by the LLM.\n",
    "    \n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits from the model for the first token\n",
    "        reference (str): Ground truth answer (A, B, C, or D)\n",
    "        model (LLM): The language model used for generation, needed for tokenizer access\n",
    "        \n",
    "    Returns:\n",
    "        float: 1.0 if the token with highest logit matches reference, 0.0 otherwise\n",
    "    \"\"\"\n",
    "    # get the predicted letter from the logits\n",
    "    pred_idx = int(torch.argmax(logits))\n",
    "    pred = model.tokenizer.decode([pred_idx]).strip()\n",
    "\n",
    "    # TODO: am I handling whitespace correctly? \n",
    "\n",
    "    # compute accuracy\n",
    "    return float(pred == reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "594f3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement test. Have a test case prompt for which the answer is very obvious. Make sure logit is correctly extracted.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OayoJRTeFEIf",
   "metadata": {},
   "source": [
    "### 2.5. Run MCQ Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97736d3",
   "metadata": {},
   "source": [
    "In the following cells we will run evaluation on the LLMs with the MMLU benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9439a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# Load MMLU dataset from Huggingface\n",
    "mmlu_test_raw = load_dataset(\"lighteval/mmlu\", \"all\", split=\"test\")\n",
    "\n",
    "# Convert to format expected by evaluate_pair_metric\n",
    "mmlu_test_data = []\n",
    "LETTERS = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "# See: `prompts/mcq/default.txt` for the prompt template format \n",
    "for item in mmlu_test_raw:\n",
    "    eval_item = {\n",
    "        \"question\": item[\"question\"], \n",
    "        \"option_A\": item[\"choices\"][0],\n",
    "        \"option_B\": item[\"choices\"][1],\n",
    "        \"option_C\": item[\"choices\"][2],\n",
    "        \"option_D\": item[\"choices\"][3],\n",
    "        \"reference\": LETTERS[item[\"answer\"]],\n",
    "        \"reference_idx\": item[\"answer\"]\n",
    "    }\n",
    "    mmlu_test_data.append(eval_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f16a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acc: \n",
    "# - regex: 0.967 for 30 samples. \n",
    "# - logit: 0.933 for 30 samples. \n",
    "\n",
    "# TODO: See how long evals take for the Llama and Qwen on PACE GPUs. \n",
    "# This shouldn't take long given that students have access to PACE GPUs. \n",
    "# ~120 seconds for Llama on A40 for 30 samples. \n",
    "# TODO: [the reported scores](https://huggingface.co/Qwen/Qwen2-7B-Instruct#evaluation) \n",
    "# are around ~70% for both Llama and Qwen. Let's prompt/hp tune to match these scores? \n",
    "\n",
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "config = LLMGenerationConfig(\n",
    "    temperature=0,\n",
    "    top_p=None,\n",
    "    top_k=None,\n",
    "    max_new_tokens=5\n",
    ")\n",
    "prompt_template = open(\"prompts/mcq/default.txt\").read()\n",
    "\n",
    "llama_mmlu_scores = defaultdict(list)\n",
    "for item in tqdm(mmlu_test_data[:30], desc=\"Evaluating dataset\"):\n",
    "    prompt = prompt_template.format(**item) if prompt_template else item[\"question\"]\n",
    "    reference, reference_idx = item[\"reference\"], item[\"reference_idx\"]\n",
    "    \n",
    "    # compute regex accuracy\n",
    "    hypothesis = llama.generate(prompt, config)\n",
    "    regex_accuracy = compute_regex_accuracy(hypothesis, reference)\n",
    "    llama_mmlu_scores[\"regex_accuracy\"].append(regex_accuracy)\n",
    "\n",
    "    # compute logit accuracy\n",
    "    logits = llama.logits(prompt)\n",
    "    logit_accuracy = compute_logit_accuracy(logits, reference, llama)\n",
    "    llama_mmlu_scores[\"logit_accuracy\"].append(logit_accuracy)\n",
    "\n",
    "\n",
    "for metric_name, metric_scores in llama_mmlu_scores.items():\n",
    "    print(f\"{metric_name}: {np.mean(metric_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f660f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acc: 5 seconds for 30 samples. \n",
    "# - regex: 0.933 for 30 samples. \n",
    "# - logit: 0.900 for 30 samples. \n",
    "\n",
    "# TODO: See how long evals take for the Llama and Qwen on PACE GPUs. \n",
    "# This shouldn't take long given that students have access to PACE GPUs. \n",
    "# ~120 seconds for Llama on A40 for 30 samples. \n",
    "# TODO: [the reported scores](https://huggingface.co/Qwen/Qwen2-7B-Instruct#evaluation) \n",
    "# are around ~70% for both Llama and Qwen. Let's prompt/hp tune to match these scores? \n",
    "\n",
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "config = LLMGenerationConfig(\n",
    "    temperature=0,\n",
    "    top_p=None,\n",
    "    top_k=None,\n",
    "    max_new_tokens=5\n",
    ")\n",
    "prompt_template = open(\"prompts/mcq/default.txt\").read()\n",
    "\n",
    "qwen_mmlu_scores = defaultdict(list)\n",
    "for item in tqdm(mmlu_test_data[:30], desc=\"Evaluating dataset\"):\n",
    "    prompt = prompt_template.format(**item) if prompt_template else item[\"question\"]\n",
    "    reference, reference_idx = item[\"reference\"], item[\"reference_idx\"]\n",
    "    \n",
    "    # compute regex accuracy\n",
    "    hypothesis = qwen.generate(prompt, config)\n",
    "    regex_accuracy = compute_regex_accuracy(hypothesis, reference)\n",
    "    qwen_mmlu_scores[\"regex_accuracy\"].append(regex_accuracy)\n",
    "\n",
    "    # compute logit accuracy\n",
    "    logits = qwen.logits(prompt)\n",
    "    logit_accuracy = compute_logit_accuracy(logits, reference, qwen)\n",
    "    qwen_mmlu_scores[\"logit_accuracy\"].append(logit_accuracy)\n",
    "\n",
    "\n",
    "for metric_name, metric_scores in qwen_mmlu_scores.items():\n",
    "    print(f\"{metric_name}: {np.mean(metric_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0613130c",
   "metadata": {},
   "source": [
    "### 2.6 Shuffling Choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d62e8f",
   "metadata": {},
   "source": [
    "When evaluating LLMs on multiple choice questions, we need to be careful about memorization effects.\n",
    "Memorization occurs when an LLM has seen very similar questions during training,\n",
    "and can simply recall the correct answer rather than reasoning about the question.\n",
    "#\n",
    "For example, if an LLM was trained on practice tests that contained the exact same multiple choice\n",
    "question with answers in the same order (A, B, C, D), it might memorize that \"A\" was correct without\n",
    "actually understanding the question.\n",
    "#\n",
    "To help distinguish between true reasoning ability and memorization, we can randomly shuffle the order\n",
    "of answer choices while preserving which answer is correct. This way, even if the LLM has seen the\n",
    "question before, it needs to identify the correct answer based on content rather than position.\n",
    "# \n",
    "Re-evaluate the two models in the shuffled case and report the resulting accuracies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "5fe1dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# Load MMLU dataset from Huggingface\n",
    "mmlu_test_raw = load_dataset(\"lighteval/mmlu\", \"all\", split=\"test\")\n",
    "\n",
    "# Convert to format expected by evaluate_pair_metric\n",
    "shuffled_mmlu_test_data = []\n",
    "LETTERS = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "# See: `prompts/mcq/default.txt` for the prompt template format \n",
    "for item in mmlu_test_raw:\n",
    "    shuffled_indices = list(range(len(item[\"choices\"])))\n",
    "    eval_item = {\n",
    "        \"question\": item[\"question\"], \n",
    "        \"option_A\": item[\"choices\"][shuffled_indices[0]],\n",
    "        \"option_B\": item[\"choices\"][shuffled_indices[1]],\n",
    "        \"option_C\": item[\"choices\"][shuffled_indices[2]],\n",
    "        \"option_D\": item[\"choices\"][shuffled_indices[3]],\n",
    "        \"reference\": LETTERS[item[\"answer\"]],\n",
    "        \"reference_idx\": item[\"answer\"]\n",
    "    }\n",
    "    shuffled_mmlu_test_data.append(eval_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acc: 5 seconds for 30 samples. \n",
    "# - regex: 0.933 for 30 samples. \n",
    "# - logit: 0.900 for 30 samples. \n",
    "\n",
    "# TODO: See how long evals take for the Llama and Qwen on PACE GPUs. \n",
    "# This shouldn't take long given that students have access to PACE GPUs. \n",
    "# ~120 seconds for Llama on A40 for 30 samples. \n",
    "# TODO: [the reported scores](https://huggingface.co/Qwen/Qwen2-7B-Instruct#evaluation) \n",
    "# are around ~70% for both Llama and Qwen. Let's prompt/hp tune to match these scores? \n",
    "\n",
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "config = LLMGenerationConfig(\n",
    "    temperature=0,\n",
    "    top_p=None,\n",
    "    top_k=None,\n",
    "    max_new_tokens=5\n",
    ")\n",
    "prompt_template = open(\"prompts/mcq/default.txt\").read()\n",
    "\n",
    "shuffled_llama_mmlu_scores = defaultdict(list)\n",
    "for item in tqdm(shuffled_mmlu_test_data[:30], desc=\"Evaluating dataset\"):\n",
    "    prompt = prompt_template.format(**item) if prompt_template else item[\"question\"]\n",
    "    reference, reference_idx = item[\"reference\"], item[\"reference_idx\"]\n",
    "    \n",
    "    # compute regex accuracy\n",
    "    hypothesis = llama.generate(prompt, config)\n",
    "    regex_accuracy = compute_regex_accuracy(hypothesis, reference)\n",
    "    shuffled_llama_mmlu_scores[\"regex_accuracy\"].append(regex_accuracy)\n",
    "\n",
    "    # compute logit accuracy\n",
    "    logits = llama.logits(prompt)\n",
    "    logit_accuracy = compute_logit_accuracy(logits, reference, llama)\n",
    "    shuffled_llama_mmlu_scores[\"logit_accuracy\"].append(logit_accuracy)\n",
    "\n",
    "\n",
    "for metric_name, metric_scores in shuffled_llama_mmlu_scores.items():\n",
    "    print(f\"{metric_name}: {np.mean(metric_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c696b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acc: 5 seconds for 30 samples. \n",
    "# - regex: 0.933 for 30 samples. \n",
    "# - logit: 0.900 for 30 samples. \n",
    "\n",
    "# TODO: See how long evals take for the Llama and Qwen on PACE GPUs. \n",
    "# This shouldn't take long given that students have access to PACE GPUs. \n",
    "# ~120 seconds for Llama on A40 for 30 samples. \n",
    "# TODO: [the reported scores](https://huggingface.co/Qwen/Qwen2-7B-Instruct#evaluation) \n",
    "# are around ~70% for both Llama and Qwen. Let's prompt/hp tune to match these scores? \n",
    "\n",
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "config = LLMGenerationConfig(\n",
    "    temperature=0,\n",
    "    top_p=None,\n",
    "    top_k=None,\n",
    "    max_new_tokens=5\n",
    ")\n",
    "prompt_template = open(\"prompts/mcq/default.txt\").read()\n",
    "\n",
    "qwen_mmlu_scores = defaultdict(list)\n",
    "for item in tqdm(shuffled_mmlu_test_data[:30], desc=\"Evaluating dataset\"):\n",
    "    prompt = prompt_template.format(**item) if prompt_template else item[\"question\"]\n",
    "    reference, reference_idx = item[\"reference\"], item[\"reference_idx\"]\n",
    "    \n",
    "    # compute regex accuracy\n",
    "    hypothesis = qwen.generate(prompt, config)\n",
    "    regex_accuracy = compute_regex_accuracy(hypothesis, reference)\n",
    "    qwen_mmlu_scores[\"regex_accuracy\"].append(regex_accuracy)\n",
    "\n",
    "    # compute logit accuracy\n",
    "    logits = qwen.logits(prompt)\n",
    "    logit_accuracy = compute_logit_accuracy(logits, reference, qwen)\n",
    "    qwen_mmlu_scores[\"logit_accuracy\"].append(logit_accuracy)\n",
    "\n",
    "\n",
    "for metric_name, metric_scores in qwen_mmlu_scores.items():\n",
    "    print(f\"{metric_name}: {np.mean(metric_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BWLK7T1uFEIg",
   "metadata": {},
   "source": [
    "## 3. Machine Translation Evaluation [25 points - Programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491813d0",
   "metadata": {},
   "source": [
    "In this section, we'll be evaluating LLMs on German to English machine translation tasks. In machine translation, models automatically converting text from one language to another while preserving meaning and fluency.\n",
    "\n",
    "We'll implement three different evaluation metrics:\n",
    "\n",
    "1. **BLEU Score**: A precision-based metric that measures the overlap of n-grams between the machine translation and reference translations. It's one of the most widely used metrics in machine translation evaluation.\n",
    "# \n",
    "2. **N-gram Overlap (BLEU)**: BLEU works by counting matching sequences of words (n-grams) between the candidate and reference translations. For example, a 1-gram (unigram) checks individual word matches, while a 4-gram checks matches of four consecutive words. This approach focuses on precision but can miss semantic equivalence when different words express the same meaning.\n",
    "#\n",
    "3. **Embedding-based Similarity**: Instead of direct word matching, we use an embedding model to convert sentences into vector representations that capture semantic meaning. \n",
    "\n",
    "Each metric has its strengths and limitations, which we'll explore throughout this section. We'll use these metrics to evaluate the translation capabilities of our LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pZDPs0Sf-H3V",
   "metadata": {},
   "source": [
    "### 3.1. BLEU Score [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02cff37",
   "metadata": {},
   "source": [
    "BLEU (Bilingual Evaluation Understudy) is a widely used metric for evaluating machine translation quality. It works by:\n",
    "\n",
    "1. Comparing n-grams (sequences of n consecutive words) between the candidate translation and reference translation\n",
    "2. Calculating precision scores for different n-gram sizes (typically 1-4)\n",
    "3. Applying a brevity penalty to penalize translations that are too short\n",
    "\n",
    "The final BLEU score is a geometric mean of these precision scores, multiplied by the brevity penalty.\n",
    "(In practice, there are a lot more bells and whistles. See [SacreBLEU](https://github.com/mjpost/sacrebleu/tree/master) for a popular implementation.)\n",
    "\n",
    "In the following cells, we'll implement:\n",
    "- A helper function to generate n-grams from a sequence of tokens\n",
    "- A function to calculate modified precision for each n-gram size\n",
    "- The main BLEU score computation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "jzGx2q0jLqyU",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _modified_precision(candidate: List[str], reference: List[str], n: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate modified precision for n-grams used in the BLEU score calculation.\n",
    "    \n",
    "    This function computes the clipped count of n-grams in the candidate translation\n",
    "    that appear in the reference translation, divided by the total number of n-grams\n",
    "    in the candidate.\n",
    "    \n",
    "    Args:\n",
    "        candidate (List[str]): List of tokens from the candidate translation\n",
    "        reference (List[str]): List of tokens from the reference translation\n",
    "        n (int): The n-gram size to consider\n",
    "            \n",
    "    Returns:\n",
    "        float: The modified precision score for the specified n-gram size\n",
    "    \"\"\"\n",
    "    cand_ngrams = Counter(_ngrams(candidate, n))\n",
    "    ref_ngrams = Counter(_ngrams(reference, n))\n",
    "    \n",
    "    overlap = {ng: min(count, ref_ngrams[ng]) for ng, count in cand_ngrams.items()}\n",
    "    num = sum(overlap.values())\n",
    "    denom = sum(cand_ngrams.values())\n",
    "    \n",
    "    return (num / denom) if denom > 0 else 0.0\n",
    "\n",
    "def compute_bleu(candidate: str, reference: str, max_n: int = 4) -> float:\n",
    "    \"\"\"\n",
    "    Compute BLEU score between candidate and reference translations.\n",
    "    \n",
    "    The BLEU (Bilingual Evaluation Understudy) score is a metric for evaluating\n",
    "    machine translation quality. It measures the precision of n-grams in the\n",
    "    candidate translation with respect to the reference translation, combined\n",
    "    with a brevity penalty for short translations.\n",
    "    \n",
    "    Args:\n",
    "        candidate (str): Generated translation text\n",
    "        reference (str): Reference translation text\n",
    "        max_n (int): Maximum n-gram order to consider (default: 4)\n",
    "        \n",
    "    Returns:\n",
    "        float: BLEU score between 0 and 1, where higher values indicate better translations\n",
    "    \"\"\"\n",
    "    cand_tokens = candidate.split()\n",
    "    ref_tokens = reference.split()\n",
    "    \n",
    "    # Calculate modified precision for each n-gram order\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        precisions.append(_modified_precision(cand_tokens, ref_tokens, n))\n",
    "    \n",
    "    # Geometric mean with smoothing\n",
    "    eps = 1e-9\n",
    "    geo_mean = np.exp(sum(np.log(p + eps) for p in precisions) / max_n)\n",
    "    \n",
    "    # Brevity penalty\n",
    "    c, r = len(cand_tokens), len(ref_tokens)\n",
    "    bp = 1.0 if c > r else np.exp(1 - r / max(c, 1))\n",
    "    \n",
    "    return bp * geo_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15475acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f6141a",
   "metadata": {},
   "source": [
    "### 3.2. N-gram Overlap. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723eac8",
   "metadata": {},
   "source": [
    "N-gram overlap is a simple metric that measures how many n-grams (sequences of n consecutive words) are shared between the generated text and the reference text. Unlike BLEU, which considers precision across multiple n-gram lengths with a brevity penalty, n-gram overlap focuses on a single n-gram size and calculates the recall - what fraction of reference n-grams appear in the candidate text.\n",
    "\n",
    "This metric is useful for evaluating:\n",
    "- Content coverage: How much of the reference content is captured in the generation\n",
    "- Lexical similarity: The degree to which the same word sequences are used\n",
    "\n",
    "N-gram overlap is particularly helpful for tasks where we want to ensure specific information from a reference is included in the generated text, such as summarization or story generation.\n",
    "\n",
    "Implement the `compute_ngram_overlap()` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "60d6e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ngram_overlap(candidate: str, reference: str, n: int = 2) -> float:\n",
    "    \"\"\"\n",
    "    This function computes the n-gram overlap between a candidate text and a reference text.\n",
    "    \n",
    "    The n-gram overlap is calculated as the fraction of unique reference n-grams that are also\n",
    "    found in the candidate text. This metric helps evaluate how well the generated text\n",
    "    captures the content of the reference text.\n",
    "    \n",
    "    Args:\n",
    "        candidate (str): Generated text to be evaluated\n",
    "        reference (str): Reference text to compare against\n",
    "        n (int): The size of n-grams to consider (default: 2, which means bigrams)\n",
    "        \n",
    "    Returns:\n",
    "        float: Fraction of reference n-grams found in candidate, ranging from 0.0 to 1.0,\n",
    "              where higher values indicate better overlap\n",
    "    \"\"\"\n",
    "    cand_ngrams = set(_ngrams(candidate.split(), n))\n",
    "    ref_ngrams = set(_ngrams(reference.split(), n))\n",
    "    \n",
    "    if not ref_ngrams:\n",
    "        return 0.0\n",
    "        \n",
    "    return len(cand_ngrams & ref_ngrams) / len(ref_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "a5365d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542f53a1",
   "metadata": {},
   "source": [
    "### 3.3. Embedding-based Similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e0f0f0",
   "metadata": {},
   "source": [
    "Embedding-based similarity metrics go beyond surface-level text matching by capturing semantic relationships between words and phrases. Unlike n-gram overlap or BLEU, which rely on exact matches, embedding-based methods can recognize when different words express similar meanings.\n",
    "\n",
    "This approach works by:\n",
    "1. Converting texts into dense vector representations (embeddings) using pre-trained models\n",
    "2. Measuring the similarity between these vectors using metrics like cosine similarity\n",
    "3. Producing a score that reflects semantic similarity rather than lexical overlap\n",
    "\n",
    "Implement the `compute_embedding_similarity()` function below to calculate the semantic similarity between generated and reference texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "116d61c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_similarity(\n",
    "    generation: str, \n",
    "    reference: str, \n",
    "    embedder: EmbeddingModel = None\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    This function computes the semantic similarity between a generated text and a reference text\n",
    "    using embeddings and cosine similarity.\n",
    "    \n",
    "    Embedding-based similarity captures semantic meaning beyond exact word matches, allowing\n",
    "    for evaluation of paraphrases and texts that convey similar meaning with different words.\n",
    "    \n",
    "    Args:\n",
    "        generation (str): Generated text to be evaluated\n",
    "        reference (str): Reference text to compare against\n",
    "        embedder (EmbeddingModel): Model to create text embeddings. If None, a default model is used.\n",
    "        \n",
    "    Returns:\n",
    "        float: Cosine similarity between the embeddings, ranging from -1.0 to 1.0,\n",
    "              where higher values indicate greater semantic similarity\n",
    "    \"\"\"\n",
    "    gen_emb = embedder.embed(generation).numpy()\n",
    "    ref_emb = embedder.embed(reference).numpy()\n",
    "    \n",
    "    return F.cosine_similarity(gen_emb, ref_emb, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a884ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad2a8d9",
   "metadata": {},
   "source": [
    "### 3.4. Test Machine Translation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a4188",
   "metadata": {},
   "source": [
    "Now let's test our machine translation metrics on a small dataset of German-to-English translations.\n",
    "\n",
    "In this section, we will:\n",
    "1. Load a test dataset containing German source sentences and English reference translations\n",
    "2. Use our LLM to generate English translations from the German source\n",
    "3. Evaluate the translations using the metrics we've implemented (BLEU and n-gram overlap)\n",
    "\n",
    "This will demonstrate how these metrics can be used to assess machine translation quality.\n",
    "Note that in a real-world scenario, you would typically compare multiple MT systems or\n",
    "evaluate against human judgments to get a more comprehensive assessment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f005d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# load data\n",
    "\n",
    "source_text = open(\"data/mt/test.de-en.de\", 'r').read().split(\"\\n\")\n",
    "target_text = open(\"data/mt/test.de-en.en\", 'r').read().split(\"\\n\")\n",
    "\n",
    "mt_test_raw = list(zip(source_text, target_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a16cf1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# Convert to format expected by evaluate_pair_metric\n",
    "mt_test_data = []\n",
    "\n",
    "# See: `prompts/mcq/default.txt` for the prompt template format \n",
    "for item in mt_test_raw:\n",
    "    eval_item = {\n",
    "        \"source_language\": \"de\",\n",
    "        \"target_language\": \"en\",\n",
    "        \"source_text\": item[0],\n",
    "        \"target_text\": item[1]\n",
    "    }\n",
    "    mt_test_data.append(eval_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d6e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acc: \n",
    "# - regex: 0.967 for 30 samples. \n",
    "# - logit: 0.933 for 30 samples. \n",
    "\n",
    "# TODO: See how long evals take for the Llama and Qwen on PACE GPUs. \n",
    "# This shouldn't take long given that students have access to PACE GPUs. \n",
    "# ~120 seconds for Llama on A40 for 30 samples. \n",
    "# TODO: [the reported scores](https://huggingface.co/Qwen/Qwen2-7B-Instruct#evaluation) \n",
    "# are around ~70% for both Llama and Qwen. Let's prompt/hp tune to match these scores? \n",
    "\n",
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "\n",
    "max_new_tokens = max(len(item[\"target_text\"].split()) for item in mt_test_data)\n",
    "print(f\"max_new_tokens: {max_new_tokens}\")\n",
    "\n",
    "config = LLMGenerationConfig(\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=None,\n",
    "    max_new_tokens=max_new_tokens\n",
    ")\n",
    "prompt_template = open(\"prompts/mt/default.txt\").read()\n",
    "\n",
    "llama_mt_scores = defaultdict(list)\n",
    "for item in tqdm(mt_test_data[:5], desc=\"Evaluating dataset\"):\n",
    "    prompt = prompt_template.format(**item) if prompt_template else item[\"source_text\"]\n",
    "    reference = item[\"target_text\"]\n",
    "\n",
    "    completion = llama.generate(prompt, config)[:len(reference)]\n",
    "    print(f\"prompt: {prompt}\")\n",
    "    print(f\"COMPLETION: {completion}\")\n",
    "    print(f\"REFERENCE: {reference}\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    # # compute BLEU score accuracy \n",
    "    # bleu_score = compute_bleu(completion, reference)\n",
    "    # llama_mt_scores[\"bleu\"].append(bleu_score)\n",
    "\n",
    "    # # compute n-gram overlap\n",
    "    # ngram_overlap = compute_ngram_overlap(completion, reference)\n",
    "    # llama_mt_scores[\"ngram_overlap\"].append(ngram_overlap)\n",
    "\n",
    "    # compute embedding similarity\n",
    "    embedding_similarity = compute_embedding_similarity(completion, reference)\n",
    "    llama_mt_scores[\"embedding_similarity\"].append(embedding_similarity)\n",
    "\n",
    "\n",
    "for metric_name, metric_scores in llama_mt_scores.items():\n",
    "    print(f\"{metric_name}: {np.mean(metric_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "943777be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: semantic similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83360462",
   "metadata": {},
   "source": [
    "## 4. Short Story Generation Evaluation [25 points - Programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0Fulh0MZ8y8b",
   "metadata": {},
   "source": [
    "### 4.1. Distinct N-grams [6 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e069a0",
   "metadata": {},
   "source": [
    "In this section, we'll implement a metric to evaluate the diversity of generated text.\n",
    "\n",
    "Distinct n-grams is a common metric used to measure the diversity and repetitiveness of generated text.\n",
    "It calculates the ratio of unique n-grams to the total number of n-grams in the text.\n",
    "\n",
    "A higher distinct n-gram ratio indicates:\n",
    "- More diverse vocabulary usage\n",
    "- Less repetition in the generated text\n",
    "- Potentially more creative and interesting content\n",
    "#\n",
    "This metric is particularly useful for evaluating story generation, where we want\n",
    "the model to produce varied and engaging content rather than repetitive patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "MiUlTSBB9Wx6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distinct_ngrams(text: str, n: int = 2) -> float:\n",
    "    \"\"\"\n",
    "    This function computes the distinct n-gram ratio, which is a measure of text diversity.\n",
    "    \n",
    "    The distinct n-gram ratio is calculated by dividing the number of unique n-grams\n",
    "    by the total number of n-grams in the text. A higher ratio indicates more diverse text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The generated text to evaluate\n",
    "        n (int): The n-gram order (default: 2 for bigrams)\n",
    "        \n",
    "    Returns:\n",
    "        float: The ratio of unique n-grams to total n-grams, ranging from 0.0 to 1.0\n",
    "              where 1.0 means all n-grams are unique\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    total = max(len(tokens) - n + 1, 1)\n",
    "    unique = len(set(_ngrams(tokens, n)))\n",
    "    \n",
    "    return unique / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vqtdrhF8FEIZ",
   "metadata": {},
   "source": [
    "### 4.2. Embedding Diversity [7 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff7a74",
   "metadata": {},
   "source": [
    "In this section, we'll implement a metric to evaluate the diversity of generated text using embeddings.\n",
    "\n",
    "While distinct n-grams measure lexical diversity (word-level), embedding diversity captures semantic diversity.\n",
    "This metric uses vector representations (embeddings) of the generated texts to measure how different they are\n",
    "from each other in the semantic space.\n",
    "\n",
    "The embedding diversity metric works by:\n",
    "1. Converting each generated text into an embedding vector\n",
    "2. Computing the cosine similarity between pairs of embeddings\n",
    "3. Calculating 1 minus the average similarity as the diversity score\n",
    "\n",
    "A higher embedding diversity score indicates:\n",
    "- More varied semantic content across generations\n",
    "- Less thematic repetition between different generated texts\n",
    "- Potentially more creative exploration of the topic space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e0b94263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_diversity(\n",
    "    generations: List[str], \n",
    "    embedder: EmbeddingModel = None\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    This function computes the diversity of multiple generated texts using embeddings.\n",
    "    \n",
    "    The diversity score is calculated by embedding each generation, computing the cosine\n",
    "    similarity between the first generation (used as reference) and all other generations,\n",
    "    and then returning 1 minus the mean similarity. A higher score indicates more diverse\n",
    "    generations.\n",
    "    \n",
    "    Args:\n",
    "        generations (List[str]): A list of generated texts to evaluate for diversity\n",
    "        embedder (EmbeddingModel): The embedding model to use for converting text to vectors.\n",
    "                                  If None, a default EmbeddingModel will be instantiated.\n",
    "        \n",
    "    Returns:\n",
    "        float: Diversity score ranging from 0.0 to 1.0, where higher values indicate\n",
    "              more diverse generations. Returns 0.0 if fewer than 2 generations are provided.\n",
    "    \"\"\"\n",
    "    if len(generations) < 2:\n",
    "        return 0.0\n",
    "        \n",
    "    embedder = embedder or EmbeddingModel()\n",
    "    \n",
    "    # Use first generation as reference\n",
    "    ref_emb = embedder.embed(generations[0]).numpy()\n",
    "    \n",
    "    similarities = []\n",
    "    for gen in generations[1:]:\n",
    "        gen_emb = embedder.embed(gen).numpy()\n",
    "        similarities.append(F.cosine_similarity(gen_emb, ref_emb))\n",
    "    \n",
    "    # Diversity = 1 - mean similarity\n",
    "    return 1.0 - np.mean(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "20ffebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532ebf95",
   "metadata": {},
   "source": [
    "### 4.3. Perplexity Computation [6 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7d46a7",
   "metadata": {},
   "source": [
    "In this section, we'll implement a function to compute the perplexity of generated text.\n",
    "Perplexity is a common metric used to evaluate language models, measuring how \"surprised\" \n",
    "a model is by a given text. Lower perplexity indicates that the model finds the text more \n",
    "predictable and natural. This metric will help students understand how to quantitatively \n",
    "assess the quality of language model outputs beyond just human evaluation.\n",
    "\n",
    "Perplexity is defined as the exponentiated average negative log-likelihood of a sequence:\n",
    "\n",
    "$$P(W) = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^N \\log P(w_i|w_1,\\dots,w_{i-1})\\right)$$\n",
    "\n",
    "where W = (w_1, w_2, ..., w_N) is a sequence of N words, and P(w_i|w_1,...,w_{i-1}) is the\n",
    "conditional probability of word w_i given the preceding words w_1 through w_{i-1}.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f698ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(text: str, big_llm: LLM = None) -> float:\n",
    "    \"\"\"\n",
    "    This function computes the perplexity of a given text using a language model.\n",
    "    \n",
    "    Perplexity is a measurement of how well a probability model predicts a sample.\n",
    "    Lower perplexity indicates the language model is better at predicting the text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The generated text to evaluate. This should be a coherent piece\n",
    "                   of text that we want to measure the perplexity of.\n",
    "        big_llm (LLM): A language model instance used for perplexity computation.\n",
    "                      If None is provided, a default LLM instance will be created.\n",
    "        \n",
    "    Returns:\n",
    "        float: The perplexity value of the input text. Lower values indicate the text\n",
    "              is more predictable according to the language model.\n",
    "    \"\"\"\n",
    "    big_llm = big_llm or LLM()  # Will fall back to stub if HF missing\n",
    "    return big_llm.perplexity(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ddfc965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TQVT6HUA9htQ",
   "metadata": {},
   "source": [
    "### 4.4. Coherence Metric [6 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba883cc",
   "metadata": {},
   "source": [
    "Coherence is another important metric for evaluating language model outputs, particularly for\n",
    "tasks like story generation. It measures how well the generated text maintains consistent meaning,\n",
    "logical flow, and semantic relatedness to a reference text or prompt.\n",
    "#\n",
    "In this implementation, we use embedding-based semantic similarity to quantify coherence.\n",
    "The model computes embeddings for both the generated text and reference text, then calculates\n",
    "their cosine similarity. \n",
    "#\n",
    "Higher similarity scores indicate better coherence between the generated and reference texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "GDI72x8XFEIc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence(generations: List[str], reference: str, model: EmbeddingModel) -> float:\n",
    "    \"\"\"\n",
    "    This function computes the coherence between generated text and a reference text\n",
    "    by computing the embedding-based semantic similarity between the two texts.\n",
    "    \n",
    "    Coherence measures how well the generated text maintains consistent meaning and\n",
    "    logical flow compared to the reference. Higher similarity suggests the\n",
    "    generated text follows similar patterns to the reference.\n",
    "    \n",
    "    Args:\n",
    "        generations (List[str]): The list of generated texts to evaluate for coherence\n",
    "        reference (str): The reference text (gold standard) to compare against\n",
    "        model (EmbeddingModel): The embedding model to use for computing text embeddings\n",
    "        \n",
    "    Returns:\n",
    "        float: Coherence score based on embedding similarity. Higher values indicate\n",
    "              better coherence between the generated and reference texts.\n",
    "    \"\"\"\n",
    "    # Get the embedding for the reference text\n",
    "    reference_embedding = model.embed(reference)\n",
    "    \n",
    "    # Get embeddings for all generated texts\n",
    "    generation_embeddings = [model.embed(gen) for gen in generations]\n",
    "    \n",
    "    # Calculate cosine similarity between reference and each generation\n",
    "    similarities = []\n",
    "    for gen_embedding in generation_embeddings:\n",
    "        # Compute cosine similarity\n",
    "        similarity = F.cosine_similarity(reference_embedding, gen_embedding, dim=0)\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    # Return the average similarity as the coherence score\n",
    "    return float(np.mean(similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "57e9ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zeo9kX6i9pbH",
   "metadata": {},
   "source": [
    "### 4.5. Ensemble Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177dbe0e",
   "metadata": {},
   "source": [
    "Now we'll implement a function to evaluate ensemble metrics for short story generation.\n",
    "This function will:\n",
    "1. Generate multiple completions for each example in the dataset\n",
    "2. Apply a metric function that works on multiple generations (like diversity or coherence)\n",
    "3. Return the mean score across all examples and per-example scores\n",
    "#\n",
    "This is similar to evaluate_pair_metric() but designed for metrics that operate on\n",
    "multiple generations rather than pairs of generations and references.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "rcmX931OFEId",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble_metric(\n",
    "    dataset: List[Dict],\n",
    "    generate_fn: Callable[[Dict], str],\n",
    "    metric_fn: Callable[[List[str], str], float],\n",
    "    num_completions: int = 5,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a dataset using metrics that work on multiple generations per example.\n",
    "    \n",
    "    Args:\n",
    "        dataset (List[Dict]): List of evaluation examples\n",
    "        generate_fn (Callable): Function that generates text from an example\n",
    "        metric_fn (Callable): Function that computes metric from list of generations and reference\n",
    "        num_completions (int): Number of completions to generate per example\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, float]: Results with 'mean' score and 'per_example' scores\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for item in dataset:\n",
    "        # Generate multiple completions\n",
    "        hypotheses = [generate_fn(item) for _ in range(num_completions)]\n",
    "        reference = item[\"reference\"]\n",
    "        scores.append(metric_fn(hypotheses, reference))\n",
    "    \n",
    "    return {\n",
    "        \"mean\": float(np.mean(scores)),\n",
    "        \"per_example\": scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e50b95",
   "metadata": {},
   "source": [
    "### 4.6. Test Short Story Generation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5fCFfEHv1hnI",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8-qTQQa2FEIe",
   "metadata": {},
   "source": [
    "## 5. Sampling Hyperparameters and Prompt Optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7dfd2",
   "metadata": {},
   "source": [
    "In this section, we'll explore how different sampling hyperparameters and prompt engineering techniques affect the the LLM's performance on MMLU. \n",
    "\n",
    "\n",
    "Sampling hyperparameters like temperature, top_p, and top_k control how the model selects the next token during generation:\n",
    "- **Temperature**: Controls randomness. Higher values (e.g., 1.5) produce more diverse outputs, while lower values (e.g., 0.1) make outputs more deterministic.\n",
    "- **Top_p (nucleus sampling)**: Only considers tokens whose cumulative probability exceeds the threshold p. Lower values restrict to more likely tokens.\n",
    "- **Top_k**: Only considers the k most likely next tokens. Lower values are more restrictive.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tqt9q92J1QKK",
   "metadata": {},
   "source": [
    "### 5.1. Sampling Hyperparameters. \n",
    "\n",
    "In this section, you will experiment with different sampling hyperparameters to see how they affect model performance. Try varying temperature, top_p, and top_k values and observe the impact on generation quality. Report the combinations which resulted in highest and lowest performance for MMLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f4aa6f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3359ee79",
   "metadata": {},
   "source": [
    "### 5.2. Chain-of-Thought Prompting\n",
    "\n",
    "Chain-of-Thought (CoT) prompting ([Wei 2022](https://arxiv.org/abs/2201.11903)) is a technique that encourages the model to break down complex reasoning tasks into intermediate steps. By asking the model to \"think step by step\" or show its reasoning process, CoT can significantly improve performance on tasks that require multi-step reasoning.\n",
    "\n",
    "Implement CoT prompting for MMLU. Compare the model's performance with and without CoT prompting. Analyze how explicitly asking the model to reason through each answer choice affects its accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c24b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n9iBiSKF1yXA",
   "metadata": {},
   "source": [
    "### 5.3. Few-shot Prompting.\n",
    "\n",
    "Implement few-shot prompting for MMLU using the provided dev sets. Show how providing examples in the prompt can improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "lfXSbxoFFEIe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Zp1aQAvn1_mz",
   "metadata": {},
   "source": [
    "### 5.4. Maximize Performance.\n",
    "\n",
    "Design prompt templates that maximize performance for each task (MCQ, MT, SSG). Consider instruction clarity, example quality, and output format specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "OayoJRTeFEIf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db6978",
   "metadata": {},
   "source": [
    "## 6. Submitting Your Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e93176",
   "metadata": {},
   "source": [
    "This is the end. Congratulations!  \n",
    "\n",
    "Now, follow the steps below to submit your homework on Gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e47daf5",
   "metadata": {},
   "source": [
    "### 6.1. Programming\n",
    "\n",
    "The programming will be evaluated through an autograder. To create the file to submit for autograder, follow the steps below -\n",
    "1. Open a terminal from the root directory of the project\n",
    "2. Run the collect_submission.py file\n",
    "3. Agree to the Late Policy and Honor Pledge\n",
    "4. After the file is executed, your root project will have a submission directory.\n",
    "5. Submit all the contents of this file to GradeScope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13221269",
   "metadata": {},
   "source": [
    "### 6.2. Non-Programming\n",
    "\n",
    "The analysis parts will be evaluated manually. For this, export the notebook to a PDF file, and submit it on GradeScope. Please ensure no written code or output is clipped when you create your PDF. One reliable way to do it is first download it as HTML through Jupyter Notebook and then print it to get PDF."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
