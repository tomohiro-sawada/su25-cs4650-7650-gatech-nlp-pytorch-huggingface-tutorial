{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxiBjiN7caiJ"
   },
   "source": [
    "# CS 4650 - Natural Language Understanding - HW - 2 \n",
    "Georgia Tech, Spring 2024 (Instructor: Kartik Goyal)\n",
    "\n",
    "Welcome to the last full programming assignment for CS 4650! \n",
    "\n",
    "In this assignment, you will be implementing a Neural Machine Translation model for a TED Talks transcript dataset.\n",
    "\n",
    "You are expected to have a good understanding of NumPy and PyTorch before starting this assignment.\n",
    "\n",
    "- NumPy Quickstarter Guide: https://numpy.org/doc/stable/user/quickstart.html\n",
    "- A good tutorial on PyTorch: https://www.youtube.com/watch?v=OIenNRt2bjg\n",
    "- Detailed Documentation of PyTorch: https://pytorch.org/docs/stable/index.html\n",
    "\n",
    "<font color='red'> DEADLINE: April 24, 2024, 11:59 PM  </font><br>\n",
    "\n",
    "Please note the following -\n",
    "- We believe that this assignment will require a considerable effort. So, please start early.\n",
    "- The Gradescope tests might take time to run. Therefore, as much as possible, use local tests to debug your code. We have provided similar local test cases for most of the methods.\n",
    "\n",
    "Please note the following points breakup of the assignment:\n",
    "\n",
    "| **Section** | **Topic**                | **Points** |\n",
    "|-------------|--------------------------|------------|\n",
    "| 1           | Load and Preprocess Data | 3          |\n",
    "| 2           | NMT Model                | 48         |\n",
    "| 3           | Sampling                 | 34         |\n",
    "| 4           | BONUS                    | 15         |\n",
    "\n",
    "\n",
    "**To start, first make a copy of this notebook to your local drive, so you can edit it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Colab Setup\n",
    "\n",
    "Note - It is heavily recommended you utilize high-end GPUs (A100/V100) of Colab Pro for this assignment. However, be smart to not finish your compute units. You don't need GPUs to test the forward passes and unit tests of your models. It is only needed to train your model and calculate BLEU scores (where forward pass on the entire dataset is needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install tqdm\n",
    "\n",
    "# Add any other required packages here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, upload the entire directory to your Google drive and do the following -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"/content/drive/MyDrive/.../directory_of_this_amazing_NLP_hw\" # Change this to the directory where you have the files of the homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T15:46:51.119263Z",
     "start_time": "2024-04-09T15:46:51.069128Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8VfrUcHqKAa"
   },
   "source": [
    "## 1. Load and Preprocess Data [Programming] [3 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCA9aMQxcjoc"
   },
   "source": [
    "In this section, you will load and preprocess the data for the Neural Machine Translation model. The data is in the form of parallel sentences in two languages - German and English. The data is already tokenized and lowercased.\n",
    "\n",
    "You will need to implement the following functions:\n",
    "1. `build` in the `Vocab` class - 3 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T15:52:50.681049Z",
     "start_time": "2024-04-09T15:52:50.648113Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE BLOCK\n",
    "\n",
    "from vocab import Vocab\n",
    "from utils import read_corpus, set_model_weights\n",
    "from nmt_model import NMT\n",
    "from model_training import train\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T15:52:51.808425Z",
     "start_time": "2024-04-09T15:52:51.789765Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE BLOCK\n",
    "\n",
    "language_mapper = {\n",
    "    'english': 'en',\n",
    "    'german': 'de'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T15:52:53.203491Z",
     "start_time": "2024-04-09T15:52:53.182892Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE BLOCK\n",
    "\n",
    "SOURCE = 'german'\n",
    "TARGET = 'english'\n",
    "VOCAB_SIZE = 30000\n",
    "FREQUENCY_CUTOFF = 2\n",
    "RANDOM_SEED = 42\n",
    "CLIP_GRAD = 5.0\n",
    "LOG_EVERY = 10000\n",
    "VALID_NITER = 2400\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T15:52:54.695909Z",
     "start_time": "2024-04-09T15:52:54.671394Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the training corpus\n",
    "train_src = f'./data/train.{language_mapper[SOURCE]}-{language_mapper[TARGET]}.{language_mapper[SOURCE]}.wmixerprep'\n",
    "train_tgt = f'./data/train.{language_mapper[SOURCE]}-{language_mapper[TARGET]}.{language_mapper[TARGET]}.wmixerprep'\n",
    "\n",
    "# Read the validation corpus\n",
    "valid_src = f\"./data/valid.{language_mapper[SOURCE]}-{language_mapper[TARGET]}.{language_mapper[SOURCE]}\"\n",
    "valid_tgt = f\"./data/valid.{language_mapper[SOURCE]}-{language_mapper[TARGET]}.{language_mapper[TARGET]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T22:06:54.512057Z",
     "start_time": "2024-04-09T22:06:50.371355Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2372,
     "status": "ok",
     "timestamp": 1698290760754,
     "user": {
      "displayName": "David Heineman",
      "userId": "17978281563156891521"
     },
     "user_tz": 240
    },
    "id": "NvvU1FzFsz3f",
    "outputId": "087c9171-5c8b-48c6-ed08-57c0571f3afd"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE BLOCK\n",
    "\n",
    "src_sentences = read_corpus(train_src, source='src')\n",
    "tgt_sentences = read_corpus(train_tgt, source='tgt')\n",
    "\n",
    "vocab = Vocab.build(src_sentences, tgt_sentences, VOCAB_SIZE, FREQUENCY_CUTOFF)\n",
    "print('Generated Vocabulary\\nSource: %d words\\nTarget: %d words' % (len(vocab.src), len(vocab.tgt)))\n",
    "\n",
    "assert len(vocab.src) == 30003, \"Training source vocabulary is of incorrect size\"\n",
    "assert len(vocab.tgt) == 22824, \"Training target vocabulary is of incorrect size\"\n",
    "\n",
    "print(\"All tests passed\")\n",
    "\n",
    "vocab.save(\"./vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T15:53:17.305269Z",
     "start_time": "2024-04-09T15:53:14.660010Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE BLOCK\n",
    "\n",
    "train_data_src = read_corpus(train_src, source='src')\n",
    "train_data_tgt = read_corpus(train_tgt, source='tgt')\n",
    "\n",
    "valid_data_src = read_corpus(valid_src, source='src')\n",
    "valid_data_tgt = read_corpus(valid_tgt, source='tgt')\n",
    "\n",
    "train_data = list(zip(train_data_src, train_data_tgt))\n",
    "valid_data = list(zip(valid_data_src, valid_data_tgt))\n",
    "\n",
    "vocab = Vocab.load('./vocab.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions - DO NOT CHANGE\n",
    "from utils import read_corpus\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import sys\n",
    "TEST_SOURCE_FILE = \"data/test.de-en.de\"\n",
    "TEST_TARGET_FILE = \"data/test.de-en.en\"\n",
    "def beam_search(model, test_data_src, beam_size, max_decoding_time_step):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    hypotheses = []\n",
    "    with torch.no_grad():\n",
    "        for src_sent in tqdm(test_data_src, desc='Decoding'):\n",
    "            example_hyps = model.beam_search(src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step)\n",
    "\n",
    "            hypotheses.append(example_hyps)\n",
    "\n",
    "    if was_training: model.train(was_training)\n",
    "\n",
    "    return hypotheses\n",
    "def compute_corpus_level_bleu_score(references, hypotheses):\n",
    "    \"\"\"\n",
    "    Given decoding results and reference sentences, compute corpus-level BLEU score\n",
    "\n",
    "    Args:\n",
    "        references: a list of gold-standard reference target sentences\n",
    "        hypotheses: a list of hypotheses, one for each reference\n",
    "\n",
    "    Returns:\n",
    "        bleu_score: corpus-level BLEU score\n",
    "    \"\"\"\n",
    "\n",
    "    if references[0][0] == '<s>':\n",
    "        references = [ref[1:-1] for ref in references]\n",
    "\n",
    "    bleu_score = corpus_bleu([[ref] for ref in references],\n",
    "                             [hyp.value for hyp in hypotheses])\n",
    "\n",
    "    return bleu_score * 100\n",
    "\n",
    "def decode(model, beam_size=5, max_decoding_time_step=100, device='cuda'):\n",
    "    \"\"\"\n",
    "    Performs decoding on a test set, and save the best-scoring decoding results.\n",
    "    If the target gold-standard sentences are given, the function also computes\n",
    "    corpus-level BLEU score.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # for the purposes of this assignment, we will evaluate bleu over the val set, where normally you would look over a test set.\n",
    "    test_data_src = valid_data_src\n",
    "    test_data_tgt = valid_data_tgt\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    hypotheses = beam_search(model, test_data_src,\n",
    "                             beam_size=int(beam_size),\n",
    "                             max_decoding_time_step=int(max_decoding_time_step))\n",
    "\n",
    "    top_hypotheses = [hyps[0] for hyps in hypotheses]\n",
    "    bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)\n",
    "    print(f'Corpus BLEU: {bleu_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. NMT Model [Programming + Non-Programming] [48 points]\n",
    "\n",
    "In this section, you will implement the Neural Machine Translation model. The model is based on the paper \"Effective Approaches to Attention-based Neural Machine Translation\" by Luong et al. (2015). The paper can be found in the following [link](https://arxiv.org/abs/1508.04025). The model that is being implemented is an LSTM encoder decoder, which is augmented by attention for the decoder. The model is implemented in the `NMT` class in the `nmt_model.py` file.\n",
    "\n",
    "You will need to implement the following functions in the `NMT` class in the `nmt_model.py` file:\n",
    "1. `__init__` and `forward` - 3 points\n",
    "2. `encode` - 6 points\n",
    "3. `dot_prod_attention` - 6 points\n",
    "4. `step` - 6 points\n",
    "5. `decode` - 6 points\n",
    "6. `beam_search` - 3 points\n",
    "\n",
    "You will need to implement the following functions in the `LabelSmoothingLoss` in the `utils.py` file:\n",
    "1. `__init__` and `forward` - 6 points\n",
    "\n",
    "Apart from this, the following parts should be done in the notebook:\n",
    "1. Reporting BLEU score - 6 points\n",
    "2. Trying different Label Smoothing variations - 6 points\n",
    "\n",
    "The description for each function is provided in the respective files. Please read the description carefully before implementing the functions.\n",
    "\n",
    "The following tests for the above implementation does not require cuda, and can be run locally to sanity check your model implementation. We will initialize a dummy model for this purpose.\n",
    "\n",
    "A recommendation to get started implementing the functions is to read through the paper for which this model definition is based off of, and understand the equations (this could take about 4 hours if read thoroughly with proper note taking), then proceed with implementation. We have removed input_feeding from the model for a minor simplificaiton from the paper's global attention model.\n",
    "\n",
    "At this point the forward function local test should have the provided return value, and you are expected to be able to train a label smoothing enabled model from scratch! The expected validation metric for this model on decoding is 8.81 ppl during training eval on the dev set, which is used to evaluate early stopping (This is with the default settings --batch-size 64 --hidden-size 256 --embed-size 256 --uniform-init 0.1 --label-smoothing 0.1 --dropout 0.2, --clip-grad 5.0 --lr-decay 0.5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T15:58:15.587620Z",
     "start_time": "2024-04-09T15:58:14.053357Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE BLOCK\n",
    "\n",
    "model_dummy = NMT(embed_size=256, hidden_size=256, dropout_rate=0.2, vocab=vocab, label_smoothing=0.1)\n",
    "set_model_weights(model_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T15:58:51.876207Z",
     "start_time": "2024-04-09T15:58:50.521733Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE BLOCK\n",
    "\n",
    "model_dummy.eval() # this is so the model is deterministic with the dropout\n",
    "input = \"hello there ! in german\".split(\" \")\n",
    "target = ['<s>'] + \"hello there ! in english\".split(\" \") + ['</s>'] + [\"<pad>\"]\n",
    "print(\"input\", input)\n",
    "print(\"target\", target)\n",
    "src_sents = [input, input[2:]]\n",
    "tgt_sents = [target, [\"<s>\"] + target[2:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1 Initialization and Forward Pass [Programming] (3 points)\n",
    "\n",
    "The forward function in the NMT class is the main function that is called when the model is run. It takes in the source and target sentences and returns the loss. The forward function is implemented in the `NMT` class in the `nmt_model.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T15:59:13.132662Z",
     "start_time": "2024-04-09T15:59:13.087619Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model_dummy.forward(src_sents, tgt_sents))\n",
    "print(\"Expected:\", torch.tensor([-454.5613, -341.4756]))\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Encoder [Programming] (6 points)\n",
    "\n",
    "The encoder is a bidirectional LSTM that takes in the source sentence and produces a context vector for the decoder. The encoder is implemented in the `encode` function in the `NMT` class in the `nmt_model.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T15:59:15.030780Z",
     "start_time": "2024-04-09T15:59:14.981991Z"
    }
   },
   "outputs": [],
   "source": [
    "src_sents_var = model_dummy.vocab.src.to_input_tensor(src_sents, device=model_dummy.device)\n",
    "tgt_sents_var = model_dummy.vocab.tgt.to_input_tensor(tgt_sents, device=model_dummy.device)\n",
    "src_sents_len = [len(s) for s in src_sents]\n",
    "\n",
    "src_encodings, decoder_init_vec = model_dummy.encode(src_sents_var, src_sents_len)\n",
    "src_sent_masks = model_dummy.get_attention_mask(src_encodings, src_sents_len)\n",
    "(decoder_init_state, decoder_init_cell) = decoder_init_vec\n",
    "\n",
    "assert src_encodings.shape == torch.Size([2, 5, 512]), f\"Incorrect shape for src_encodings {src_encodings.shape}\"\n",
    "assert decoder_init_state.shape == torch.Size([2, 256]), f\"Incorrect shape for decoder_init_state {decoder_init_state.shape}\"\n",
    "assert decoder_init_cell.shape == torch.Size([2, 256]), f\"Incorrect shape for decoder_init_cell {decoder_init_cell.shape}\"\n",
    "assert abs(src_encodings[0,0,0] - -3.3668e-05) < 0.001, f\"Incorrect first entry in src_encodings {src_encodings[0,0,0]}\"\n",
    "assert abs(src_encodings[-1,-2,-1] - 0) < 0.001, f\"Incorrect second to last entry in src_encodings {src_encodings[-1,-2,-1]}\"\n",
    "assert abs(src_encodings[-1,-3,-1] - -0.7592) < 0.001, f\"Incorrect third last entry in src_encodings {src_encodings[-1,-3,-1]}\"\n",
    "assert abs(decoder_init_state[0,0] - 1) < 0.001, f\"Incorrect first entry in decoder_init_state {decoder_init_state[0,0]}\"\n",
    "assert abs(decoder_init_state[-1,-5] - 0.9273) < 0.001, f\"Incorrect fifth last entry in decoder_init_state {decoder_init_state[-1,-5]}\"\n",
    "assert abs(decoder_init_cell[0,0] - 25.2386) < 0.001, f\"Incorrect first entry in decoder_init_cell {decoder_init_cell[0,0]}\"\n",
    "assert abs(decoder_init_cell[-1,-1] - 30.4197) < 0.001, f\"Incorrect last entry in decoder_init_cell {decoder_init_cell[-1,-1]}\"\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Attention [Programming] (6 points)\n",
    "\n",
    "The attention mechanism is used to focus on different parts of the source sentence at each step of the decoding process. The attention mechanism is implemented in the `dot_prod_attention` function in the `NMT` class in the `nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T15:59:31.909662Z",
     "start_time": "2024-04-09T15:59:31.859778Z"
    }
   },
   "outputs": [],
   "source": [
    "ctx_t, alpha_t = model_dummy.dot_prod_attention(h_t=torch.ones((2, 256)).float(), src_encoding=src_encodings, src_encoding_att_linear=torch.ones((2, 5, 256)).float(), mask=src_sent_masks)\n",
    "\n",
    "assert ctx_t.shape == torch.Size([2, 512]), f\"Incorrect shape for ctx_t: {ctx_t.shape}\"\n",
    "assert alpha_t.shape == torch.Size([2, 5]), f\"Incorrect shape for alpha_t: {alpha_t.shape}\"\n",
    "assert abs(ctx_t[0, 0].item() - 0.1514) < 0.001, f\"Incorrect value for ctx_t[0, 0]: {ctx_t[0, 0].item()}\"\n",
    "assert abs(alpha_t[0, 0].item() - 0.2000) < 0.001, f\"Incorrect value for alpha_t[0, 0]: {alpha_t[0, 0].item()}\"\n",
    "assert abs(ctx_t[-1, -1].item() - (-0.2546)) < 0.001, f\"Incorrect value for ctx_t[-1, -1]: {ctx_t[-1, -1].item()}\"\n",
    "assert abs(alpha_t[-1, -1].item() - 0.0) < 0.001, f\"Incorrect value for alpha_t[-1, -1]: {alpha_t[-1, -1].item()}\"\n",
    "assert abs(alpha_t[-1, 0].item() - 0.3333) < 0.001, f\"Incorrect value for alpha_t[-1, 0]: {alpha_t[-1, 0].item()}\"\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Decoder Step [Programming] (6 points)\n",
    "\n",
    "The decoder step is the core of the decoder, where the decoder LSTM cell is run for one step. The decoder step is implemented in the `step` function in the `NMT` class in the `nmt_model.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T15:59:36.118251Z",
     "start_time": "2024-04-09T15:59:36.096222Z"
    }
   },
   "outputs": [],
   "source": [
    "(h_t, cell_t), att_t, alpha_t = model_dummy.step(x=torch.arange(2*256).reshape(2, 256) / 512, h_tm1=decoder_init_vec, src_encodings=src_encodings, src_encoding_att_linear=torch.arange(2*5*256).reshape(2, 5, 256) / (2 * 5 * 256), src_sent_masks=src_sent_masks)\n",
    "\n",
    "assert h_t.shape == torch.Size([2, 256]), f\"Incorrect shape for h_t: {h_t.shape}\"\n",
    "assert cell_t.shape == torch.Size([2, 256]), f\"Incorrect shape for cell_t: {cell_t.shape}\"\n",
    "assert att_t.shape == torch.Size([2, 256]), f\"Incorrect shape for att_t: {att_t.shape}\"\n",
    "assert alpha_t.shape == torch.Size([2, 5]), f\"Incorrect shape for alpha_t: {alpha_t.shape}\"\n",
    "assert abs(h_t[0, 1].item() - (-0.9392)) < 0.001, f\"Incorrect value for the second element of h_t: {h_t[0, 1].item()}\"\n",
    "assert abs(h_t[-1,-2].item() - 0.0155) < 0.001, f\"Incorrect value for the second to last element of h_t: {h_t[-1, -2].item()}\"\n",
    "assert abs(cell_t[0, 0].item() - 25.2386) < 0.001, f\"Incorrect value for the first element of cell_t: {cell_t[0, 0].item()}\"\n",
    "assert abs(cell_t[-1, -1].item() - (31.4197)) < 0.001, f\"Incorrect value for the last element of cell_t: {cell_t[-1, -1].item()}\"\n",
    "assert abs(att_t[0, 1].item() - 0.9978) < 0.001, f\"Incorrect value for the second element of att_t: {att_t[0, 1].item()}\"\n",
    "assert abs(att_t[-1, -3].item() - (-0.9195)) < 0.001, f\"Incorrect value for the third to last element of att_t: {att_t[-1, -3].item()}\"\n",
    "assert abs(alpha_t[0, 0].item() - 0.1153) < 0.001, f\"Incorrect value for the first element of alpha_t: {alpha_t[0, 0].item()}\"\n",
    "assert abs(alpha_t[-1, -3].item() - 0.1293) < 0.001, f\"Incorrect value for the third to last element of alpha_t: {alpha_t[-1, -3].item()}\"\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Decoder [Programming] (6 points)\n",
    "\n",
    "The decoder is an LSTM that takes in the context vector from the encoder and produces the output sequence. The decoder is implemented in the `decode` function in the `NMT` class in the `nmt_model.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:00:09.620735Z",
     "start_time": "2024-04-09T16:00:09.572055Z"
    }
   },
   "outputs": [],
   "source": [
    "att_vecs = model_dummy.decode(src_encodings, src_sent_masks, decoder_init_vec, tgt_sents_var[:-1])\n",
    "\n",
    "assert att_vecs.shape == torch.Size([7, 2, 256]), f\"got wrong shape for src_encodings {att_vecs.shape}\"\n",
    "assert abs(att_vecs[0,0,2] - -0.9456) < 0.001, f\"got wrong third entry in decoder_init_cell {att_vecs[0,0,2]}\"\n",
    "assert abs(att_vecs[-1,-1,-2] - 0.9151) < 0.001, f\"got wrong second to last entry in decoder_init_cell {att_vecs[-1,-1,-2]}\"\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Label Smoothing Loss [Programming] (6 points)\n",
    "\n",
    "The label smoothing loss is a regularization technique used to prevent the model from overfitting to the training data. The label smoothing loss is implemented in the `LabelSmoothingLoss` class in the `utils.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:00:14.394277Z",
     "start_time": "2024-04-09T16:00:14.372386Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import LabelSmoothingLoss\n",
    "num_labels = 1000\n",
    "smoothingloss = LabelSmoothingLoss(0.1,tgt_vocab_size=num_labels, padding_idx=0)\n",
    "output_log_probs = torch.arange(num_labels * 2, dtype=float).reshape(-1, num_labels).log_softmax(dim=-1) # 1 batch\n",
    "targets = torch.zeros_like(output_log_probs).long()\n",
    "targets[0, -2] = 1\n",
    "targets[1, 0] = 1\n",
    "targets = targets.argmax(dim=-1)\n",
    "out = smoothingloss.forward(output_log_probs, targets)\n",
    "\n",
    "print(out)\n",
    "print(\"Expected torch tensor of:\", [-50.2929, 0])\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.7. Model Training [Non-Programming]\n",
    "\n",
    "In this section, you will train the model using the given german-english translation dataset. This section may require you to use a GPU. You can choose to use the free tier of Google Colab to train the model. You can refer to the following link to get started with Google Colab - https://colab.research.google.com/notebooks/intro.ipynb. Alternatively, you can subscribe to the paid version of Google Colab or use any other cloud service that provides GPU support.\n",
    "\n",
    "At the end of this, save the best model you get to carry out further experiments.\n",
    "\n",
    "You are already provided with training and validation loops for this, you don't need to implement them.\n",
    "\n",
    "Note - Credits for this section are distributed entirely in the BLEU Score calculation step, as it just needs hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:00:34.641574Z",
     "start_time": "2024-04-09T16:00:34.601959Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning (Feel free to change these). Retain your best performing model.\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "embed_dim = 256\n",
    "hidden_dim = 256\n",
    "dropout = 0.2\n",
    "label_smoothing = 0.1\n",
    "learning_rate = 0.001\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:00:37.076391Z",
     "start_time": "2024-04-09T16:00:36.863753Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = NMT(embed_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            dropout_rate=dropout,\n",
    "            label_smoothing=label_smoothing,\n",
    "            vocab=vocab)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:00:48.283501Z",
     "start_time": "2024-04-09T16:00:48.177160Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE BLOCK\n",
    "\n",
    "uniform_init = 0.1\n",
    "if np.abs(uniform_init) > 0.:\n",
    "    print('uniformly initialize parameters [-%f, +%f]' % (uniform_init, uniform_init), file=sys.stderr)\n",
    "    for p in model.parameters():\n",
    "        p.data.uniform_(-uniform_init, uniform_init)\n",
    "\n",
    "vocab_mask = torch.ones(len(vocab.tgt))\n",
    "vocab_mask[vocab.tgt['<pad>']] = 0\n",
    "\n",
    "print('Use device: %s' % device, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:00:53.057236Z",
     "start_time": "2024-04-09T16:00:51.649680Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE BLOCK\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:00:57.112476Z",
     "start_time": "2024-04-09T16:00:56.189797Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train(model=model, optimizer=optimizer, train_data=train_data, val_data=valid_data, max_epoch=n_epochs, train_batch_size=BATCH_SIZE, clip_grad=5.0, model_save_path='./model.bin', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.8. Beam Search [Programming] (3 points)\n",
    "\n",
    "Beam search is an advanced strategy for generating text sequences, striking a balance between the brute-force exploration of all possible sequences and the narrow focus of greedy sampling. At each step in generating a sequence, given a context $S = \\{s_1, s_2, \\ldots, s_{n-1}\\}$, the algorithm maintains a set of $b$ candidate sequences, called \"beams\", each with its own cumulative probability. The beam width $b$ controls the number of sequences explored in parallel, with each step extending each candidate sequence by one word and keeping only the top $b$ sequences according to their cumulative probabilities:\n",
    "\n",
    "$$\n",
    "B_n = \\text{Top}_b \\left\\{ B_{n-1} \\oplus v_i : P(S \\oplus v_i|S) \\right\\}\n",
    "$$\n",
    "\n",
    "where $B_n$ represents the set of beams at step $n$, $\\oplus$ denotes sequence concatenation, and $\\text{Top}_b$ selects the $b$ sequences with the highest cumulative probability. The probability of a sequence is typically the product of the probabilities of its constituent words, adjusted for sequence length to prevent bias toward shorter sequences. Beam search continues this process until a stopping condition is reached, such as all beams ending with an end-of-sequence token or reaching a predetermined sequence length.\n",
    "\n",
    "While beam search significantly improves the likelihood of finding high-quality sequences by considering multiple hypotheses, its computational cost increases with the beam width. Moreover, despite exploring more options than greedy sampling, beam search can still favor shorter sequences due to its cumulative probability calculation and may converge to similar sequences within the beams, reducing diversity. Nevertheless, it remains a popular choice in NLP tasks where the balance between output quality and computational efficiency is crucial, offering a pragmatic compromise between exhaustive search and deterministic selection.\n",
    "\n",
    "Beam search is used in sampling too. You can refer to the article in the Sampling section of this assignment for more details on beam search.\n",
    "\n",
    "You can also refer this video - https://www.youtube.com/watch?v=RLWuzLLSIgw\n",
    "\n",
    "Note - This implementation will also be used at other places, so, the 3 points is not all the credits for this part. Please implement this carefully, its credits are distributed across sections.\n",
    "\n",
    "The test below only provides very basic level of sanity check for beam search. Because optimal beam search requires the model to be trained and we need beam search to evaluate your model's BLEU score. You will need to implement the beam search in the `NMT` class in the `nmt_model.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMT.load('../saved_files/model.bin').to(device) # Load your trained here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T22:08:02.845775Z",
     "start_time": "2024-04-09T22:08:01.876887Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Covers only basic check for beam search\n",
    "out = model.beam_search(src_sentences[0], beam_size=5, max_decoding_time_step=100)\n",
    "assert len(out) == 5, \"Incorrect: got incorrect number of outputs \" + str(len(out)) + \" instead of 5\"\n",
    "assert out[0].score >= out[1].score and out[1].score >= out[2].score and out[2].score >= out[3].score and out[3].score >= out[4].score, \"Incorrect: got wrong order of scores\"\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.9. BLEU Score [Non-Programming] (6 points)\n",
    "\n",
    "The models will be evaluated using BLEU score. The BLEU score, which stands for Bilingual Evaluation Understudy, is an algorithm used for evaluating the quality of text that has been machine-translated from one natural language to another. The essence of the BLEU score is to measure how close machine translations are to professional human translations.\n",
    "\n",
    "This will be a combination of your model training and BLEU score implementation.\n",
    "\n",
    "Following are the partial credits for BLEU Score -\n",
    "- bleu < 10.0 - 1 point\n",
    "- 10.0 <= bleu < 12.0 - 2 point\n",
    "- 12.0 <= bleu < 15.0 - 3 points\n",
    "- 15.0 <= bleu < 20.0 - 4 points\n",
    "- 20.0 <= bleu < 25.0 - 5 points\n",
    "- bleu > 25.0 - 6 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 2\n",
    "max_decoding_time_step = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:03:44.496685Z",
     "start_time": "2024-04-09T16:03:44.379205Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decode(model, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.10. Label Smoothing Experiments [Non-Programming] (6 points)\n",
    "\n",
    "In the section below, add as many code cells (code cells only) and train a model with\n",
    "1. Label Smoothing = 0.0\n",
    "2. Label Smoothing = 0.2\n",
    "\n",
    "Report the BLEU score for both the scenarios using Beam Search (you can use the decode function defined before for decoding after training the model)\n",
    "Feel free to save the weights\n",
    "\n",
    "Note - You don't need to write model class here. Just declare the NMT model, train your model and evaluate as done above. Preserve all outputs for credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR IMPLEMENTATION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# USE THIS CODE CELL TO SAVE YOUR MODEL WEIGHTS\n",
    "\n",
    "model.save('./model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sampling [Programming + Non-Programming] (34 points)\n",
    "\n",
    "Sampling refers to the method by which a model selects subsequent words to construct a sentence, based on a probability distribution over the entire vocabulary. This distribution reflects the model's learned understanding of language, predicting the likelihood of each word being the appropriate follow-up in a given context. The choice of sampling strategy is crucial, as it directly influences the balance between creativity and coherence in the generated text. Different strategies navigate this balance in unique ways, impacting the model's output in terms of variability, unpredictability, and fidelity to human-like language patterns.\n",
    "\n",
    "In the following section, we will explore with the following sampling techniques -\n",
    "1. Random Sampling\n",
    "2. Top-p Sampling\n",
    "3. Top-k Sampling\n",
    "4. Greedy Sampling\n",
    "4. Beam Search (already implemented above)\n",
    "\n",
    "Note - All sampling (except Greedy) have 2 parts - one is a forward pass check that will give you 3 points in the autograder, and one is a BLEU score check that will give you 3 points if your BLEU score crosses the provided threshold. This will be evaluated in non-programming part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMT.load('../saved_files/model.bin').to(device) # Replace by the file path of your best model here if needed\n",
    "model.eval()\n",
    " \n",
    "src_sentences = read_corpus(valid_src, source='src')\n",
    "tgt_sentences = read_corpus(valid_tgt, source='tgt')\n",
    " \n",
    "zipped_lists = sorted(zip(src_sentences, tgt_sentences), key=lambda pair: len(pair[0]), reverse=True)\n",
    "src_sentences, tgt_sentences = zip(*zipped_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_2 = 256 # If you run out of memory/crash kernel while computing BLEU Score of Sampling, reduce this number. This will increase compute time but decrease memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Random Sampling [Programming + Non-Programming] (3 + 3 = 6 points)\n",
    "\n",
    "Random sampling, the most straightforward approach, selects the next word purely based on its predicted probability, allowing for a high degree of diversity but at the risk of producing less coherent sentences. It involves selecting the next word in a sequence based on a probability distribution over the vocabulary. This distribution is derived from the model's predictions, indicating how likely each word is to follow the given context. Here's a mathematical breakdown of how random sampling works:\n",
    "\n",
    "Given a sequence of words $S = \\{s_1, s_2, \\ldots, s_{n-1}\\}$, an NLP model predicts the next word $s_n$ by estimating the probability distribution $P(V|S)$ over the vocabulary $V = \\{v_1, v_2, \\ldots, v_M\\}$, where $M$ is the size of the vocabulary. The model outputs a probability $P(v_i|S)$ for each word $v_i$ in the vocabulary, indicating the likelihood of $v_i$ being the next word. These probabilities are normalized so that they sum up to 1:\n",
    "\n",
    "$$\\sum_{i=1}^{M} P(v_i|S) = 1$$\n",
    "\n",
    "Random Sampling involves selecting the next word $s_n$ from the vocabulary $V$ based on the predicted probabilities. Specifically, each word $v_i$ is chosen with a probability $P(v_i|S)$. This selection process can be conceptualized as a random variable $X$ with a multinomial distribution, where the probability mass function (PMF) for $X = v_i$ (the event of choosing word $v_i$ as the next word) is given by:\n",
    "\n",
    "$$P(X = v_i) = P(v_i|S)$$\n",
    "\n",
    "This process is repeated for each new word in the sequence, with the context $S$ being updated to include the newly chosen words, until a termination condition is met (e.g., a maximum sequence length or the selection of a special end-of-sequence token).\n",
    "\n",
    "In the file nmt_model.py, implement Random sampling inside the sample method.\n",
    "\n",
    "Note - Random sampling will be implemented when both k=None and p=None in the function parameters.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Unit Tests for Random Sampling [Programming] (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_fn(logits, dim=None):\n",
    "    return torch.nn.functional.softmax(torch.tensor([100,-100,-100,-100,-100]).float().reshape(1, -1).expand(logits.size(0),-1).to(device), dim=dim)\n",
    "\n",
    "out = model.sample(src_sentences[:1], max_decoding_time_step=20, softmax_fn=softmax_fn)\n",
    "assert len(out) == 1, \"Incorrect: got incorrect number of outputs \" + str(len(out)) + \" instead of 1\"\n",
    "assert len(out[0]) == 5, \"Incorrect: got incorrect sample size of \" + str(len(out)) + \" instead of 5\"\n",
    "out = [vocab.tgt.words2indices(o[0].value) for o in out][0]\n",
    "assert set(out).__len__() == 2, \"Incorrect: got other than 2 unique samples\" + str(set(out).__len__())\n",
    "assert set(out).difference({0, 1}) == set(), \"Incorrect: got wrong samples: \" + str(set(out))\n",
    "assert set(out).intersection({1}) == {1}, \"Incorrect: start of sequence not in samples\"\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. BLEU Score using Random Sampling [Non-Programming] (3 points)\n",
    "\n",
    "Adjust the hyperparameters below to get the best BLEU score.\n",
    "\n",
    "Threshold for points -\n",
    "- bleu < 10.0 - 0 points\n",
    "- 10.0 <= bleu <14 - 1 point \n",
    "- bleu >= 14.0 - 3 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_decoding_time_step = 50 # You can change this for improved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE BLOCK\n",
    "out = []\n",
    "\n",
    "N = len(src_sentences) // BATCH_SIZE_2\n",
    "for b in tqdm(range(N)):\n",
    "  out.extend(model.sample(sorted(src_sentences[b*BATCH_SIZE_2 : (b+1)*BATCH_SIZE_2], key=lambda x: -len(x)), max_decoding_time_step=max_decoding_time_step, sample_size=2))\n",
    "out.extend(model.sample(sorted(src_sentences[(N)*BATCH_SIZE_2:], key=lambda x: -len(x)), max_decoding_time_step=max_decoding_time_step, sample_size=2))\n",
    "best_hyps = []\n",
    "for i in out:\n",
    "    best_hyps.append(i[0])\n",
    "bleu = compute_corpus_level_bleu_score(tgt_sentences, best_hyps)\n",
    "print('BLEU Score with Random Sampling -', bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Top-k Sampling [Programming + Non-Programming] (3 + 3 = 6 points)\n",
    "\n",
    "Top-k sampling is a technique employed in text generation tasks to enhance the selection process of the next word in a sequence. Given a context sequence $S = \\{s_1, s_2, \\ldots, s_{n-1}\\}$, and a vocabulary $V = \\{v_1, v_2, \\ldots, v_M\\}$, the model computes a probability distribution $P(V|S)$, predicting the likelihood of each word in the vocabulary being the next appropriate word.\n",
    "\n",
    "In top-k sampling, the model first identifies the subset $V_k \\subseteq V$ consisting of the top $k$ words with the highest probabilities. Mathematically, this subset is defined as:\n",
    "\n",
    "$$\n",
    "V_k = \\{v | P(v|S) \\text{ is among the top } k \\text{ of all } P(v_i|S), \\forall v_i \\in V\\}\n",
    "$$\n",
    "\n",
    "The probability distribution is then modified to focus solely on these $k$ words, with probabilities outside this set being set to zero. The modified distribution $P'(V|S)$ is defined as:\n",
    "\n",
    "$$\n",
    "P'(v_i|S) = \n",
    "\\begin{cases} \n",
    "P(v_i|S) & \\text{if } v_i \\in V_k \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The probabilities in $P'(V|S)$ are then renormalized so that they sum to 1, ensuring a valid probability distribution. The next word is sampled from this adjusted distribution, effectively narrowing the selection to the most probable candidates and discarding less likely, potentially noisy options.\n",
    "\n",
    "The selection of the next word $s_n$ is, therefore, a random variable $X$ with a distribution reflecting the adjusted probabilities:\n",
    "\n",
    "$$\n",
    "P(X = v_i) = \\frac{P'(v_i|S)}{\\sum_{v_j \\in V_k} P'(v_j|S)}\n",
    "$$\n",
    "\n",
    "Top-k sampling balances the diversity of text generation with the coherence of the generated sequences, making it a preferred choice in scenarios where quality and readability of the generated text are crucial.\n",
    "\n",
    "In the file nmt_model.py, enhance the sample method to include top-k sampling with the provided non-None value of $k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Unit Tests for Top-k Sampling [Programming] (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax_fn(logits, dim=None):\n",
    "    return torch.nn.functional.softmax(torch.tensor([10,10,-10,10,10,-10,-10,-10]).float().reshape(1, -1).expand(logits.size(0),-1).to(device), dim=dim)\n",
    "\n",
    "out = model.sample(src_sentences[:1], max_decoding_time_step=40, k=4, softmax_fn=softmax_fn)\n",
    "assert len(out) == 1, \"Incorrect: got incorrect number of outputs \" + str(len(out)) + \" instead of 1\"\n",
    "assert len(out[0]) == 5, \"Incorrect: got incorrect sample size of \" + str(len(out)) + \" instead of 5\"\n",
    "out = [vocab.tgt.words2indices(o[0].value) for o in out][0]\n",
    "assert set(out).__len__() == 4, \"Incorrect: got other than 4 unique samples\"\n",
    "assert set(out).difference({0, 1, 3, 4}) == set(), \"Incorrect: got wrong samples: \" + str(set(out))\n",
    "assert set(out).intersection({1}) == {1}, \"Incorrect: start of sequence not in samples\"\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. BLEU Score using Top-k Sampling [Non-Programming] (3 points)\n",
    "\n",
    "Adjust the hyperparameters below to get the best BLEU score.\n",
    "\n",
    "Threshold for points -\n",
    "- bleu < 12.0 - 0 points\n",
    "- 12.0 <= bleu <15.0 - 1 point \n",
    "- 15.0 <= bleu < 18.0 - 2 points\n",
    "- bleu >= 18.0 - 3 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_k_value = 4 # You can change this for improved results\n",
    "max_decoding_time_step = 50 # You can change this for improved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE BLOCK\n",
    "out=[]\n",
    "N = len(src_sentences) // BATCH_SIZE_2\n",
    "for b in tqdm(range(N)):\n",
    "  out.extend(model.sample(sorted(src_sentences[b*BATCH_SIZE_2 : (b+1)*BATCH_SIZE_2], key=lambda x: -len(x)), max_decoding_time_step=max_decoding_time_step, sample_size=2, k=top_k_value))\n",
    "out.extend(model.sample(sorted(src_sentences[(N)*BATCH_SIZE_2:], key=lambda x: -len(x)), max_decoding_time_step=max_decoding_time_step, sample_size=2, k=top_k_value))\n",
    "\n",
    "best_hyps = []\n",
    "for i in out:\n",
    "    best_hyps.append(i[0])\n",
    "bleu = compute_corpus_level_bleu_score(tgt_sentences, best_hyps)\n",
    "print('BLEU Score with Top-K Sampling -', bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Nucleus/Top-p Sampling [Programming + Non-Programming] (3 + 3 = 6 points)\n",
    "\n",
    "Nucleus (top-p) sampling is an adaptive technique used in natural language processing (NLP) for generating text sequences that balances the trade-off between randomness and determinism. Unlike top-k sampling that selects the top $k$ most probable words, top-p sampling dynamically selects a subset of the vocabulary by choosing the smallest set of words whose cumulative probability exceeds a threshold $p$. This method focuses on the \"nucleus\" of probable words, discarding the tail of less likely words to ensure both diversity and coherence in the generated text.\n",
    "\n",
    "Given a sequence $S = \\{s_1, s_2, \\ldots, s_{n-1}\\}$ and a vocabulary $V = \\{v_1, v_2, \\ldots, v_M\\}$, the model first computes the probability distribution $P(V|S)$ for the next word. The words in $V$ are then sorted by their probability in descending order, resulting in a sorted list $V_{sorted} = \\{v_{(1)}, v_{(2)}, \\ldots, v_{(M)}\\}$ where $P(v_{(1)}|S) \\ge P(v_{(2)}|S) \\ge \\ldots \\ge P(v_{(M)}|S)$.\n",
    "\n",
    "To determine the subset $V_p \\subseteq V$ for sampling, we find the smallest $k$ such that:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{k} P(v_{(i)}|S) > p\n",
    "$$\n",
    "\n",
    "The cumulative probability distribution is then adjusted to zero out the probabilities of words not included in $V_p$, and the remaining probabilities are renormalized to sum to 1. The next word $s_n$ is sampled from this adjusted distribution.\n",
    "\n",
    "The selection of $s_n$ can be represented as sampling from a random variable $X$ with the modified distribution:\n",
    "\n",
    "$$\n",
    "P'(X = v_{(i)}) = \\frac{P(v_{(i)}|S)}{\\sum_{j=1}^{k} P(v_{(j)}|S)}, \\text{ for } i \\le k\n",
    "$$\n",
    "\n",
    "Nucleus sampling's dynamic nature allows it to automatically adjust the breadth of the sampling space based on the context, making it particularly effective for generating high-quality and varied text outputs. This method has become a popular choice in state-of-the-art NLP models for tasks such as text completion, storytelling, and dialogue generation.\n",
    "\n",
    "In the file nmt_model.py, enhance the sample method to include top-p sampling with the provided non-None value of $p$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. Unit Tests for Top-p Sampling [Programming] (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax_fn(logits, dim=None):\n",
    "    # return torch.nn.functional.softmax(logits, dim=dim)\n",
    "    return torch.nn.functional.softmax(torch.tensor([1,2,3,4,5,6,7,8,9,10]).float().reshape(1, -1).expand(logits.size(0),-1).to(device), dim=dim)\n",
    "\n",
    "out = model.sample(src_sentences[:1], max_decoding_time_step=90, p=.95, softmax_fn=softmax_fn)\n",
    "assert len(out) == 1, \"Incorrect: got incorrect number of outputs \" + str(len(out)) + \" instead of 1\"\n",
    "assert len(out[0]) == 5, \"Incorrect: got incorrect sample size of \" + str(len(out)) + \" instead of 5\"\n",
    "out = [vocab.tgt.words2indices(o[0].value) for o in out][0]\n",
    "assert set(out).__len__() == 4, \"Incorrect: got other than 4 unique samples\"\n",
    "assert set(out).difference({8, 1, 9, 7}) == set(), \"Incorrect: got wrong samples: \" + str(set(out))\n",
    "assert set(out).intersection({1}) == {1}, \"Incorrect: start of sequence not in samples\"\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. BLEU Score using Top-p Sampling [Non-Programming] (3 points)\n",
    "\n",
    "Adjust the hyperparameters below to get the best BLEU score.\n",
    "\n",
    "Threshold for points -\n",
    "- bleu < 12.0 - 0 points\n",
    "- 12.0 <= bleu <15.0 - 1 point \n",
    "- 15.0 <= bleu < 18.0 - 2 points\n",
    "- bleu >= 18.0 - 3 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_p_value = 0.8 # You can change this for improved results\n",
    "max_decoding_time_step = 50 # You can change this for improved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE BLOCK\n",
    "out=[]\n",
    "N = len(src_sentences) // BATCH_SIZE_2\n",
    "for b in tqdm(range(N)):\n",
    "  out.extend(model.sample(sorted(src_sentences[b*BATCH_SIZE_2 : (b+1)*BATCH_SIZE_2], key=lambda x: -len(x)), max_decoding_time_step=max_decoding_time_step, sample_size=2, p=top_p_value))\n",
    "out.extend(model.sample(sorted(src_sentences[(N)*BATCH_SIZE_2:], key=lambda x: -len(x)), max_decoding_time_step=max_decoding_time_step, sample_size=2, p=top_p_value))\n",
    "\n",
    "best_hyps = []\n",
    "for i in out:\n",
    "    best_hyps.append(i[0])\n",
    "bleu = compute_corpus_level_bleu_score(tgt_sentences, best_hyps)\n",
    "print('BLEU Score with Top-P/Nucleus Sampling -', bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Greedy Sampling [Non-Programming] (4 points)\n",
    "\n",
    "Greedy sampling is a deterministic approach in the generation of text sequences from NLP models. At each step of the sequence generation, given a context sequence $S = \\{s_1, s_2, \\ldots, s_{n-1}\\}$, the model computes a probability distribution $P(V|S)$ over the vocabulary $V = \\{v_1, v_2, \\ldots, v_M\\}$. The method then selects the word $v_i$ with the highest probability as the next word in the sequence:\n",
    "\n",
    "$$\n",
    "s_n = \\arg\\max_{v_i \\in V} P(v_i|S)\n",
    "$$\n",
    "\n",
    "This process is repeated iteratively, updating the context $S$ at each step by appending the newly selected word, until a stopping criterion is met, such as reaching a maximum sequence length or encountering an end-of-sequence token.\n",
    "\n",
    "Greedy sampling is characterized by its simplicity and efficiency, as it avoids the computational overhead associated with evaluating and sampling from complex probability distributions. However, this simplicity comes at the cost of diversity. The generated sequences may lack variability and could suffer from issues like repetition or predictability. Furthermore, by always selecting the most probable word, greedy sampling can miss out on potentially interesting or more contextually appropriate choices that a stochastic sampling method might explore.\n",
    "\n",
    "Despite these limitations, greedy sampling is widely used in scenarios where the highest probability sequence is desired, and computational efficiency is paramount. It is particularly common in tasks where a single, clear output is preferred over a diverse set of plausible outputs.\n",
    "\n",
    "\n",
    "We can directly use argmax of probability distribution to implement greedy sampling. However, in this assignment we will re-use our top-k and top-p sampling to achieve greedy sampling.\n",
    "\n",
    "Note - The key thing that you get with greedy sampling is repeatability. You always will get the same output as the maximum probability token will be sampled always. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1. Greedy Sampling using top-k Sampling [Non-programming] (2 points)\n",
    "\n",
    "Describe how will you use your top-k sampling code for greedy sampling. In the following code cell, use the parameters of sample method and k (keep p=None) to implement greedy sampling. Show using an example that your output is repeatable.\n",
    "\n",
    "Also explain the logic behind your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.sample(src_sentences, max_decoding_time_step=100, k=1)\n",
    "for i in out[0]:\n",
    "    print(i.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " YOUR EXPLANATION HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2. Greedy Sampling using Top-p Sampling [Non-programming] (2 points)\n",
    "\n",
    "Describe how will you use your top-p sampling code for greedy sampling. In the following code cell, use the parameters of sample method and p (keep k=None) to implement greedy sampling. Show using an example that your output is repeatable.\n",
    "\n",
    "Also explain the logic behind your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR EXPLANATION HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Analysis - Distribution of 'k' in top-p Sampling [Non-programming] (6 points)\n",
    "\n",
    "Extend and reprogram the implementation of top-p sampling to identify the distribution of number of samples (k) when using \n",
    "1. p = 0.5\n",
    "2. p= 0.8\n",
    "3. p = 0.9\n",
    "\n",
    "Write your code below and print the mean and standard deviation of k in all the three cases. Use your existing trained model for this analysis.\n",
    "\n",
    "Note - Your results should execute from the code, and not just written in the form of markdown cells. You can add more code cells if you want.\n",
    "\n",
    "Also, in a markdown cell, provide intuitive reasoning why the output is coming that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def k_distribution_using_top_p(p=None):\n",
    "    mean = None\n",
    "    std = None\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # END YOUR CODE\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis cell should print the outputs\n",
    "mean_p1, std_p1 = k_distribution_using_top_p(0.5)\n",
    "mean_p2, std_p2 = k_distribution_using_top_p(0.8)\n",
    "mean_p3, std_p3 = k_distribution_using_top_p(0.9)\n",
    "\n",
    "print(f'Mean and Standard Deviation for p=0.5: {mean_p1}, {std_p1}')\n",
    "print(f'Mean and Standard Deviation for p=0.8: {mean_p2}, {std_p2}')\n",
    "print(f'Mean and Standard Deviation for p=0.9: {mean_p3}, {std_p3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR EXPLANATION HERE:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.7. Analysis - Distribution of 'p' in top-k Sampling [Non-programming] (6 points)\n",
    "\n",
    "Extend and reprogram the implementation of top-k sampling to identify the distribution of cumulative probability of the samples (p) filtered when using \n",
    "1. k = 1\n",
    "2. k = 3\n",
    "3. k = 5\n",
    "\n",
    "Write your code below and print the mean and standard deviation of p in all the three cases. Use your existing trained model for this analysis.\n",
    "\n",
    "Note - Your results should execute from the code, and not just written in the form of markdown cells. You can add more code cells if you want.\n",
    "\n",
    "Also, in a markdown cell, provide intuitive reasoning why the output is coming that way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def p_distribution_using_top_k(k=None):\n",
    "    mean = None\n",
    "    std = None\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # END YOUR CODE\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_k1, std_k1 = p_distribution_using_top_k(k=1)\n",
    "mean_k2, std_k2 = p_distribution_using_top_k(k=3)\n",
    "mean_k3, std_k3 = p_distribution_using_top_k(k=5)\n",
    "\n",
    "print(f'Mean and Standard Deviation for k=1: {mean_k1}, {std_k1}')\n",
    "print(f'Mean and Standard Deviation for k=3: {mean_k2}, {std_k2}')\n",
    "print(f'Mean and Standard Deviation for k=5: {mean_k3}, {std_k3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "YOUR EXPLANATION HERE:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bonus for all - Try Different Label Smoothing Techniques [Non-Programming] (15 points)\n",
    "\n",
    "Modify your label smoothing class to try different Label Smoothing techniques and improve your output on beam search. Show the following -\n",
    "\n",
    "- complete implementation of the Label Smoothing technique\n",
    "- final BLEU score after training the model on this label smoothing technique\n",
    "- A write-up describing your design, why you think it works, and how improved it is compared to the existing Label Smoothing technique used for previous sections\n",
    "\n",
    "For full credits, your technique should give an improved BLEU score over the current one.\n",
    "\n",
    "Note - You can add as many number of code or markdown cells as you want to provide your solution. Changing noise levels on existing implementation will not count towards any credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START CODING HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR EXPLANATION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Submission\n",
    "\n",
    "For programming, submit the following to the Programming Gradescope assignment -\n",
    "- nmt_model.py\n",
    "- utils.py\n",
    "- vocab.json\n",
    "- vocab.py\n",
    "- the weights of your best trained model (in Section 2)\n",
    "\n",
    "Please note, 100 MB is the limit of gradescope submission. Your submission should be less than that\n",
    "\n",
    "For non-programming, export this notebook as PDF and submit it in the Non-Programming Gradescope assignment. Please label all the pages of your questions correctly and remove any extra print statements that you added that make things redundant and is not required for the evaluation. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1AHxJ-svPipLZVxIypjIjVFsMOAySzoaz",
     "timestamp": 1698344413266
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
